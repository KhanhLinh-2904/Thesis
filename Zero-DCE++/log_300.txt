Total training examples: 3820
Loss at iteration 50 : 1.0624361038208008
Loss at iteration 100 : 1.0233582258224487
Loss at iteration 150 : 0.624635636806488
Loss at iteration 200 : 0.7581367492675781
Loss at iteration 250 : 0.9074001908302307
Loss at iteration 300 : 0.48886966705322266
Loss at iteration 350 : 0.652342677116394
Loss at iteration 400 : 0.7552201151847839
Loss at iteration 450 : 1.2849657535552979
Mean training loss eporch  0 :  0.7389913228765193
Loss at iteration 50 : 0.7248134613037109
Loss at iteration 100 : 0.9972364902496338
Loss at iteration 150 : 0.5753986835479736
Loss at iteration 200 : 0.5996105074882507
Loss at iteration 250 : 0.5518493056297302
Loss at iteration 300 : 0.6550832390785217
Loss at iteration 350 : 0.5008427500724792
Loss at iteration 400 : 0.9192911386489868
Loss at iteration 450 : 0.6347612142562866
Mean training loss eporch  1 :  0.6991699250672153
Loss at iteration 50 : 0.5460091829299927
Loss at iteration 100 : 0.652962863445282
Loss at iteration 150 : 0.7512432336807251
Loss at iteration 200 : 1.3495253324508667
Loss at iteration 250 : 1.0217459201812744
Loss at iteration 300 : 0.548758327960968
Loss at iteration 350 : 0.5320628881454468
Loss at iteration 400 : 0.5898098349571228
Loss at iteration 450 : 0.4064987301826477
Mean training loss eporch  2 :  0.6953136939009862
Loss at iteration 50 : 0.7040881514549255
Loss at iteration 100 : 0.8752272129058838
Loss at iteration 150 : 1.0397742986679077
Loss at iteration 200 : 0.37368887662887573
Loss at iteration 250 : 0.6262139081954956
Loss at iteration 300 : 0.7942876815795898
Loss at iteration 350 : 0.5321133136749268
Loss at iteration 400 : 0.7872662544250488
Loss at iteration 450 : 0.39812755584716797
Mean training loss eporch  3 :  0.6924902552653057
Loss at iteration 50 : 0.7176270484924316
Loss at iteration 100 : 0.7087532877922058
Loss at iteration 150 : 0.7521012425422668
Loss at iteration 200 : 0.5143337249755859
Loss at iteration 250 : 0.4222716689109802
Loss at iteration 300 : 0.6256616115570068
Loss at iteration 350 : 0.49332714080810547
Loss at iteration 400 : 0.5774446725845337
Loss at iteration 450 : 1.0008465051651
Mean training loss eporch  4 :  0.6900128775186618
Loss at iteration 50 : 0.7570784091949463
Loss at iteration 100 : 0.8276163935661316
Loss at iteration 150 : 0.6905571818351746
Loss at iteration 200 : 0.6415908932685852
Loss at iteration 250 : 0.8905870914459229
Loss at iteration 300 : 0.9072193503379822
Loss at iteration 350 : 0.7849153280258179
Loss at iteration 400 : 0.3858039975166321
Loss at iteration 450 : 0.5474569201469421
Mean training loss eporch  5 :  0.6876553887789718
Loss at iteration 50 : 0.5593752861022949
Loss at iteration 100 : 0.6194725632667542
Loss at iteration 150 : 0.6139153838157654
Loss at iteration 200 : 0.3023516535758972
Loss at iteration 250 : 0.9932531714439392
Loss at iteration 300 : 0.8387715220451355
Loss at iteration 350 : 0.5903885364532471
Loss at iteration 400 : 0.600273609161377
Loss at iteration 450 : 0.6213169097900391
Mean training loss eporch  6 :  0.6866564355385354
Loss at iteration 50 : 0.6302279233932495
Loss at iteration 100 : 0.6281623840332031
Loss at iteration 150 : 0.7584943771362305
Loss at iteration 200 : 0.6765083074569702
Loss at iteration 250 : 0.45144063234329224
Loss at iteration 300 : 0.6307619214057922
Loss at iteration 350 : 0.7442545890808105
Loss at iteration 400 : 0.4534133970737457
Loss at iteration 450 : 0.6033722162246704
Mean training loss eporch  7 :  0.6842153783123862
Loss at iteration 50 : 0.6203199625015259
Loss at iteration 100 : 0.3793812394142151
Loss at iteration 150 : 0.3725293278694153
Loss at iteration 200 : 0.93256014585495
Loss at iteration 250 : 0.423219233751297
Loss at iteration 300 : 0.6024402379989624
Loss at iteration 350 : 0.7574894428253174
Loss at iteration 400 : 1.077544093132019
Loss at iteration 450 : 0.2774448096752167
Mean training loss eporch  8 :  0.6819452797393919
Loss at iteration 50 : 0.7178888320922852
Loss at iteration 100 : 0.7514054179191589
Loss at iteration 150 : 0.6475722789764404
Loss at iteration 200 : 0.6250334978103638
Loss at iteration 250 : 0.9214931726455688
Loss at iteration 300 : 0.32747381925582886
Loss at iteration 350 : 0.6220105886459351
Loss at iteration 400 : 0.7008435726165771
Loss at iteration 450 : 0.5170001983642578
Mean training loss eporch  9 :  0.6799614388012487
Loss at iteration 50 : 0.876631498336792
Loss at iteration 100 : 0.9175098538398743
Loss at iteration 150 : 0.5494781732559204
Loss at iteration 200 : 0.6290569305419922
Loss at iteration 250 : 0.3391122817993164
Loss at iteration 300 : 0.7790651321411133
Loss at iteration 350 : 0.8661718368530273
Loss at iteration 400 : 1.2185192108154297
Loss at iteration 450 : 0.41597485542297363
Mean training loss eporch  10 :  0.6769486786680741
Loss at iteration 50 : 0.6853896379470825
Loss at iteration 100 : 0.48148608207702637
Loss at iteration 150 : 1.2013696432113647
Loss at iteration 200 : 0.37052029371261597
Loss at iteration 250 : 0.6483682990074158
Loss at iteration 300 : 0.5513424873352051
Loss at iteration 350 : 0.8437546491622925
Loss at iteration 400 : 0.48432332277297974
Loss at iteration 450 : 0.5868291854858398
Mean training loss eporch  11 :  0.6742432794249208
Loss at iteration 50 : 0.8397648334503174
Loss at iteration 100 : 0.9525619745254517
Loss at iteration 150 : 0.5670244693756104
Loss at iteration 200 : 0.5969442129135132
Loss at iteration 250 : 0.9835022687911987
Loss at iteration 300 : 0.6506531834602356
Loss at iteration 350 : 0.5172837972640991
Loss at iteration 400 : 0.6872080564498901
Loss at iteration 450 : 0.8777267336845398
Mean training loss eporch  12 :  0.6703950326001294
Loss at iteration 50 : 0.47295328974723816
Loss at iteration 100 : 0.9950239062309265
Loss at iteration 150 : 0.4043828845024109
Loss at iteration 200 : 0.6686497926712036
Loss at iteration 250 : 0.5391014814376831
Loss at iteration 300 : 0.5157639384269714
Loss at iteration 350 : 0.4167759418487549
Loss at iteration 400 : 1.034221887588501
Loss at iteration 450 : 0.6410678625106812
Mean training loss eporch  13 :  0.6666982082891664
Loss at iteration 50 : 0.717747151851654
Loss at iteration 100 : 0.45668911933898926
Loss at iteration 150 : 0.9896329641342163
Loss at iteration 200 : 0.33189958333969116
Loss at iteration 250 : 0.4223947525024414
Loss at iteration 300 : 0.5818161368370056
Loss at iteration 350 : 0.6607905626296997
Loss at iteration 400 : 1.1696699857711792
Loss at iteration 450 : 0.7157664895057678
Mean training loss eporch  14 :  0.663148355720931
Loss at iteration 50 : 0.5505855083465576
Loss at iteration 100 : 0.5846074819564819
Loss at iteration 150 : 0.6920944452285767
Loss at iteration 200 : 0.4678543210029602
Loss at iteration 250 : 0.5614384412765503
Loss at iteration 300 : 0.7543919086456299
Loss at iteration 350 : 0.712803065776825
Loss at iteration 400 : 0.7009737491607666
Loss at iteration 450 : 0.4451603293418884
Mean training loss eporch  15 :  0.6599387807077942
Loss at iteration 50 : 0.398354172706604
Loss at iteration 100 : 0.7733084559440613
Loss at iteration 150 : 0.45674312114715576
Loss at iteration 200 : 0.6160469651222229
Loss at iteration 250 : 0.7775788307189941
Loss at iteration 300 : 0.44111916422843933
Loss at iteration 350 : 0.8715193271636963
Loss at iteration 400 : 0.37584346532821655
Loss at iteration 450 : 0.3969117999076843
Mean training loss eporch  16 :  0.6574917783859384
Loss at iteration 50 : 0.5877660512924194
Loss at iteration 100 : 0.5674805045127869
Loss at iteration 150 : 0.5151014924049377
Loss at iteration 200 : 0.7675898671150208
Loss at iteration 250 : 0.6195987462997437
Loss at iteration 300 : 0.6274622678756714
Loss at iteration 350 : 0.671718955039978
Loss at iteration 400 : 0.6466569900512695
Loss at iteration 450 : 0.4016376733779907
Mean training loss eporch  17 :  0.6551209149500317
Loss at iteration 50 : 1.0163379907608032
Loss at iteration 100 : 0.565780520439148
Loss at iteration 150 : 0.3061894476413727
Loss at iteration 200 : 0.4438890814781189
Loss at iteration 250 : 0.6393581628799438
Loss at iteration 300 : 0.3812643885612488
Loss at iteration 350 : 0.8049499988555908
Loss at iteration 400 : 0.5124583840370178
Loss at iteration 450 : 0.7080779075622559
Mean training loss eporch  18 :  0.6525074740146992
Loss at iteration 50 : 0.51039719581604
Loss at iteration 100 : 0.3604080080986023
Loss at iteration 150 : 0.5138936638832092
Loss at iteration 200 : 0.9033815264701843
Loss at iteration 250 : 0.89913010597229
Loss at iteration 300 : 0.4166044294834137
Loss at iteration 350 : 0.8812286853790283
Loss at iteration 400 : 0.7240720987319946
Loss at iteration 450 : 1.0055782794952393
Mean training loss eporch  19 :  0.6517016823446401
Loss at iteration 50 : 0.5290022492408752
Loss at iteration 100 : 0.7796933650970459
Loss at iteration 150 : 0.6767349243164062
Loss at iteration 200 : 0.32997310161590576
Loss at iteration 250 : 0.3737083375453949
Loss at iteration 300 : 0.6476863622665405
Loss at iteration 350 : 0.5388664603233337
Loss at iteration 400 : 0.49154552817344666
Loss at iteration 450 : 0.7260536551475525
Mean training loss eporch  20 :  0.6508174739372781
Loss at iteration 50 : 0.5540058016777039
Loss at iteration 100 : 0.5104427337646484
Loss at iteration 150 : 0.3765469491481781
Loss at iteration 200 : 0.46877825260162354
Loss at iteration 250 : 0.41374671459198
Loss at iteration 300 : 0.8081293106079102
Loss at iteration 350 : 0.38636937737464905
Loss at iteration 400 : 0.6949648857116699
Loss at iteration 450 : 0.825977087020874
Mean training loss eporch  21 :  0.6503488171424826
Loss at iteration 50 : 0.3246992230415344
Loss at iteration 100 : 0.6590805053710938
Loss at iteration 150 : 1.0062594413757324
Loss at iteration 200 : 1.0634212493896484
Loss at iteration 250 : 0.7504580616950989
Loss at iteration 300 : 0.5517058372497559
Loss at iteration 350 : 1.0548211336135864
Loss at iteration 400 : 0.5596131086349487
Loss at iteration 450 : 0.8207316398620605
Mean training loss eporch  22 :  0.6486032455536113
Loss at iteration 50 : 0.4826379120349884
Loss at iteration 100 : 0.607208788394928
Loss at iteration 150 : 0.49099865555763245
Loss at iteration 200 : 0.49154406785964966
Loss at iteration 250 : 0.8786189556121826
Loss at iteration 300 : 0.6701706647872925
Loss at iteration 350 : 0.5927110910415649
Loss at iteration 400 : 1.1731361150741577
Loss at iteration 450 : 0.47608956694602966
Mean training loss eporch  23 :  0.6478879734811424
Loss at iteration 50 : 0.6185622811317444
Loss at iteration 100 : 0.8236472606658936
Loss at iteration 150 : 1.0033459663391113
Loss at iteration 200 : 0.52535480260849
Loss at iteration 250 : 0.5767685770988464
Loss at iteration 300 : 0.2836115062236786
Loss at iteration 350 : 0.3691801428794861
Loss at iteration 400 : 0.506521999835968
Loss at iteration 450 : 0.5268621444702148
Mean training loss eporch  24 :  0.6464320124442607
Loss at iteration 50 : 0.25202739238739014
Loss at iteration 100 : 0.5209898352622986
Loss at iteration 150 : 0.7587380409240723
Loss at iteration 200 : 0.30989259481430054
Loss at iteration 250 : 1.0414211750030518
Loss at iteration 300 : 0.6892962455749512
Loss at iteration 350 : 0.5834137201309204
Loss at iteration 400 : 0.45886939764022827
Loss at iteration 450 : 0.35389941930770874
Mean training loss eporch  25 :  0.6460025580794742
Loss at iteration 50 : 0.9981931447982788
Loss at iteration 100 : 0.785453200340271
Loss at iteration 150 : 0.3832700848579407
Loss at iteration 200 : 0.7200700044631958
Loss at iteration 250 : 0.7346785068511963
Loss at iteration 300 : 0.7578345537185669
Loss at iteration 350 : 0.5613367557525635
Loss at iteration 400 : 0.9746943712234497
Loss at iteration 450 : 0.6180983781814575
Mean training loss eporch  26 :  0.6451804482039049
Loss at iteration 50 : 1.0115187168121338
Loss at iteration 100 : 0.651344895362854
Loss at iteration 150 : 0.5030975341796875
Loss at iteration 200 : 0.7374448776245117
Loss at iteration 250 : 0.4160357415676117
Loss at iteration 300 : 0.7681831121444702
Loss at iteration 350 : 0.8007476925849915
Loss at iteration 400 : 0.7915767431259155
Loss at iteration 450 : 1.6646575927734375
Mean training loss eporch  27 :  0.6438384625328136
Loss at iteration 50 : 0.3886949121952057
Loss at iteration 100 : 0.4383520483970642
Loss at iteration 150 : 0.6541343927383423
Loss at iteration 200 : 0.3615538477897644
Loss at iteration 250 : 0.6055251359939575
Loss at iteration 300 : 0.8142983913421631
Loss at iteration 350 : 1.329856276512146
Loss at iteration 400 : 0.7561496496200562
Loss at iteration 450 : 0.842311680316925
Mean training loss eporch  28 :  0.6438879951148851
Loss at iteration 50 : 0.3982637822628021
Loss at iteration 100 : 0.3495844006538391
Loss at iteration 150 : 0.5582178235054016
Loss at iteration 200 : 0.6696711778640747
Loss at iteration 250 : 0.8352247476577759
Loss at iteration 300 : 0.8462827205657959
Loss at iteration 350 : 0.38442540168762207
Loss at iteration 400 : 0.423050194978714
Loss at iteration 450 : 0.282335489988327
Mean training loss eporch  29 :  0.642502489511438
Loss at iteration 50 : 0.7884951829910278
Loss at iteration 100 : 0.6713302135467529
Loss at iteration 150 : 0.41319596767425537
Loss at iteration 200 : 0.5757155418395996
Loss at iteration 250 : 0.8083860874176025
Loss at iteration 300 : 0.38494962453842163
Loss at iteration 350 : 1.05532705783844
Loss at iteration 400 : 0.9184894561767578
Loss at iteration 450 : 0.7423000335693359
Mean training loss eporch  30 :  0.6421239321513654
Loss at iteration 50 : 0.6128963232040405
Loss at iteration 100 : 0.6929672956466675
Loss at iteration 150 : 0.8562966585159302
Loss at iteration 200 : 0.765912652015686
Loss at iteration 250 : 0.4732780456542969
Loss at iteration 300 : 0.7725569009780884
Loss at iteration 350 : 0.5787769556045532
Loss at iteration 400 : 0.3414320945739746
Loss at iteration 450 : 0.8424291610717773
Mean training loss eporch  31 :  0.6412883643762337
Loss at iteration 50 : 0.9106948375701904
Loss at iteration 100 : 0.8920649290084839
Loss at iteration 150 : 0.5317496061325073
Loss at iteration 200 : 0.721723198890686
Loss at iteration 250 : 0.7746402621269226
Loss at iteration 300 : 0.976489245891571
Loss at iteration 350 : 0.4063851237297058
Loss at iteration 400 : 0.7880807518959045
Loss at iteration 450 : 1.0485116243362427
Mean training loss eporch  32 :  0.639930227623325
Loss at iteration 50 : 0.8478398323059082
Loss at iteration 100 : 0.43417686223983765
Loss at iteration 150 : 0.793380618095398
Loss at iteration 200 : 0.940228283405304
Loss at iteration 250 : 0.4516449272632599
Loss at iteration 300 : 0.4219452440738678
Loss at iteration 350 : 1.2214772701263428
Loss at iteration 400 : 0.9919716119766235
Loss at iteration 450 : 0.6570390462875366
Mean training loss eporch  33 :  0.6394665111557709
Loss at iteration 50 : 0.46525129675865173
Loss at iteration 100 : 0.7715069055557251
Loss at iteration 150 : 1.2206470966339111
Loss at iteration 200 : 0.8030577301979065
Loss at iteration 250 : 0.7787575721740723
Loss at iteration 300 : 0.44507476687431335
Loss at iteration 350 : 1.0519366264343262
Loss at iteration 400 : 0.5380606651306152
Loss at iteration 450 : 0.44224393367767334
Mean training loss eporch  34 :  0.6392745970432728
Loss at iteration 50 : 1.0128493309020996
Loss at iteration 100 : 0.678824782371521
Loss at iteration 150 : 0.3463934659957886
Loss at iteration 200 : 0.49414294958114624
Loss at iteration 250 : 0.9192121624946594
Loss at iteration 300 : 0.5076291561126709
Loss at iteration 350 : 0.9118284583091736
Loss at iteration 400 : 1.35209321975708
Loss at iteration 450 : 0.6309416890144348
Mean training loss eporch  35 :  0.6383902393749069
Loss at iteration 50 : 0.4425843060016632
Loss at iteration 100 : 0.3432183861732483
Loss at iteration 150 : 0.4484747648239136
Loss at iteration 200 : 0.7216565608978271
Loss at iteration 250 : 0.6667013764381409
Loss at iteration 300 : 0.512503981590271
Loss at iteration 350 : 0.6703349351882935
Loss at iteration 400 : 0.4121420383453369
Loss at iteration 450 : 0.5493475794792175
Mean training loss eporch  36 :  0.6379340409484369
Loss at iteration 50 : 0.5951617956161499
Loss at iteration 100 : 1.1732107400894165
Loss at iteration 150 : 0.41146743297576904
Loss at iteration 200 : 0.5485154390335083
Loss at iteration 250 : 1.0904607772827148
Loss at iteration 300 : 0.4993617832660675
Loss at iteration 350 : 0.895595908164978
Loss at iteration 400 : 0.4950750470161438
Loss at iteration 450 : 0.7038413882255554
Mean training loss eporch  37 :  0.6369588363557181
Loss at iteration 50 : 0.7459917664527893
Loss at iteration 100 : 1.2080085277557373
Loss at iteration 150 : 0.46747106313705444
Loss at iteration 200 : 0.6300550699234009
Loss at iteration 250 : 0.7188698053359985
Loss at iteration 300 : 1.0238837003707886
Loss at iteration 350 : 1.094051480293274
Loss at iteration 400 : 0.6937031745910645
Loss at iteration 450 : 0.9487866163253784
Mean training loss eporch  38 :  0.6358718289740415
Loss at iteration 50 : 0.9012303352355957
Loss at iteration 100 : 0.5070043206214905
Loss at iteration 150 : 0.6459929347038269
Loss at iteration 200 : 0.42229440808296204
Loss at iteration 250 : 0.4606362581253052
Loss at iteration 300 : 0.7246224284172058
Loss at iteration 350 : 0.48362404108047485
Loss at iteration 400 : 0.6366604566574097
Loss at iteration 450 : 1.0126638412475586
Mean training loss eporch  39 :  0.6354732086032504
Loss at iteration 50 : 0.5816079378128052
Loss at iteration 100 : 0.36530759930610657
Loss at iteration 150 : 0.9812874794006348
Loss at iteration 200 : 0.7169544696807861
Loss at iteration 250 : 0.4030212461948395
Loss at iteration 300 : 0.9204574823379517
Loss at iteration 350 : 0.4318578243255615
Loss at iteration 400 : 0.7578141689300537
Loss at iteration 450 : 0.36797451972961426
Mean training loss eporch  40 :  0.6353635256759292
Loss at iteration 50 : 1.1905431747436523
Loss at iteration 100 : 0.37348121404647827
Loss at iteration 150 : 0.4477527141571045
Loss at iteration 200 : 0.5033796429634094
Loss at iteration 250 : 0.7173265218734741
Loss at iteration 300 : 0.3476223349571228
Loss at iteration 350 : 0.6786576509475708
Loss at iteration 400 : 0.73561692237854
Loss at iteration 450 : 0.9670006632804871
Mean training loss eporch  41 :  0.635096376068173
Loss at iteration 50 : 0.5846410989761353
Loss at iteration 100 : 0.7649216651916504
Loss at iteration 150 : 0.5010873675346375
Loss at iteration 200 : 0.977708637714386
Loss at iteration 250 : 0.9110352993011475
Loss at iteration 300 : 0.9442986249923706
Loss at iteration 350 : 0.44611668586730957
Loss at iteration 400 : 0.5354703068733215
Loss at iteration 450 : 0.5567528009414673
Mean training loss eporch  42 :  0.6340494934298004
Loss at iteration 50 : 1.0719685554504395
Loss at iteration 100 : 0.49080324172973633
Loss at iteration 150 : 0.5020451545715332
Loss at iteration 200 : 0.4445725679397583
Loss at iteration 250 : 0.4287654459476471
Loss at iteration 300 : 1.1544476747512817
Loss at iteration 350 : 0.9836637377738953
Loss at iteration 400 : 0.4368927776813507
Loss at iteration 450 : 0.41668957471847534
Mean training loss eporch  43 :  0.6331097740820262
Loss at iteration 50 : 0.3821965456008911
Loss at iteration 100 : 0.7899489402770996
Loss at iteration 150 : 1.2584601640701294
Loss at iteration 200 : 0.40836629271507263
Loss at iteration 250 : 0.9517307877540588
Loss at iteration 300 : 0.2950262427330017
Loss at iteration 350 : 0.5637689232826233
Loss at iteration 400 : 0.34155622124671936
Loss at iteration 450 : 0.45050451159477234
Mean training loss eporch  44 :  0.6324516991504067
Loss at iteration 50 : 0.7810698747634888
Loss at iteration 100 : 0.6526308059692383
Loss at iteration 150 : 0.830452024936676
Loss at iteration 200 : 0.7010235786437988
Loss at iteration 250 : 0.943699836730957
Loss at iteration 300 : 1.1222803592681885
Loss at iteration 350 : 0.7602725028991699
Loss at iteration 400 : 0.70799720287323
Loss at iteration 450 : 0.3017481565475464
Mean training loss eporch  45 :  0.6318175690755186
Loss at iteration 50 : 0.6003267168998718
Loss at iteration 100 : 0.4974963068962097
Loss at iteration 150 : 1.056443691253662
Loss at iteration 200 : 0.46335774660110474
Loss at iteration 250 : 0.601117730140686
Loss at iteration 300 : 0.5492744445800781
Loss at iteration 350 : 0.6868647933006287
Loss at iteration 400 : 0.4460846185684204
Loss at iteration 450 : 0.350776344537735
Mean training loss eporch  46 :  0.6317223895294397
Loss at iteration 50 : 0.7185429334640503
Loss at iteration 100 : 0.6219379901885986
Loss at iteration 150 : 0.38496553897857666
Loss at iteration 200 : 0.38507336378097534
Loss at iteration 250 : 0.8079416751861572
Loss at iteration 300 : 1.0304789543151855
Loss at iteration 350 : 0.7014323472976685
Loss at iteration 400 : 0.3764823377132416
Loss at iteration 450 : 0.3712345361709595
Mean training loss eporch  47 :  0.6306999004660291
Loss at iteration 50 : 0.6602596044540405
Loss at iteration 100 : 0.5896849632263184
Loss at iteration 150 : 0.395236074924469
Loss at iteration 200 : 0.6445251703262329
Loss at iteration 250 : 0.7092732191085815
Loss at iteration 300 : 0.41949209570884705
Loss at iteration 350 : 0.17702388763427734
Loss at iteration 400 : 1.3693695068359375
Loss at iteration 450 : 0.8283370733261108
Mean training loss eporch  48 :  0.6309653910882304
Loss at iteration 50 : 0.68160080909729
Loss at iteration 100 : 0.6670026183128357
Loss at iteration 150 : 0.3811781704425812
Loss at iteration 200 : 0.6943461894989014
Loss at iteration 250 : 0.585667073726654
Loss at iteration 300 : 0.5284647941589355
Loss at iteration 350 : 0.6183929443359375
Loss at iteration 400 : 0.782293438911438
Loss at iteration 450 : 0.3359384536743164
Mean training loss eporch  49 :  0.6301629980675346
Loss at iteration 50 : 0.6107151508331299
Loss at iteration 100 : 0.44899457693099976
Loss at iteration 150 : 0.657004714012146
Loss at iteration 200 : 0.628000020980835
Loss at iteration 250 : 1.11615788936615
Loss at iteration 300 : 0.9247167110443115
Loss at iteration 350 : 0.31562310457229614
Loss at iteration 400 : 0.539211630821228
Loss at iteration 450 : 0.8497223258018494
Mean training loss eporch  50 :  0.6304653798498868
Loss at iteration 50 : 0.4508444666862488
Loss at iteration 100 : 0.5967106819152832
Loss at iteration 150 : 0.46517613530158997
Loss at iteration 200 : 0.6637736558914185
Loss at iteration 250 : 0.8371337652206421
Loss at iteration 300 : 0.3048563301563263
Loss at iteration 350 : 0.6882534027099609
Loss at iteration 400 : 0.8620228171348572
Loss at iteration 450 : 0.7365220189094543
Mean training loss eporch  51 :  0.6285503622939397
Loss at iteration 50 : 0.807167649269104
Loss at iteration 100 : 0.3209046423435211
Loss at iteration 150 : 0.48275479674339294
Loss at iteration 200 : 0.39641451835632324
Loss at iteration 250 : 0.7197967767715454
Loss at iteration 300 : 0.5119813680648804
Loss at iteration 350 : 0.7986737489700317
Loss at iteration 400 : 0.4285535216331482
Loss at iteration 450 : 0.6589983701705933
Mean training loss eporch  52 :  0.6281175188502008
Loss at iteration 50 : 0.660006582736969
Loss at iteration 100 : 0.3199995160102844
Loss at iteration 150 : 0.41174256801605225
Loss at iteration 200 : 0.9136199951171875
Loss at iteration 250 : 0.410910427570343
Loss at iteration 300 : 0.6314829587936401
Loss at iteration 350 : 0.4024439752101898
Loss at iteration 400 : 0.7970695495605469
Loss at iteration 450 : 0.7978261113166809
Mean training loss eporch  53 :  0.6273947431701995
Loss at iteration 50 : 0.7813596725463867
Loss at iteration 100 : 0.5873085260391235
Loss at iteration 150 : 0.48791608214378357
Loss at iteration 200 : 0.7852543592453003
Loss at iteration 250 : 0.41148242354393005
Loss at iteration 300 : 0.4284188449382782
Loss at iteration 350 : 0.34928229451179504
Loss at iteration 400 : 0.5317630171775818
Loss at iteration 450 : 0.6050901412963867
Mean training loss eporch  54 :  0.6269000607492036
Loss at iteration 50 : 0.7578064203262329
Loss at iteration 100 : 0.530419111251831
Loss at iteration 150 : 0.7462160587310791
Loss at iteration 200 : 0.3702383041381836
Loss at iteration 250 : 0.5280544757843018
Loss at iteration 300 : 0.8681692481040955
Loss at iteration 350 : 0.4383111596107483
Loss at iteration 400 : 0.6844397187232971
Loss at iteration 450 : 0.8377374410629272
Mean training loss eporch  55 :  0.6266479264617465
Loss at iteration 50 : 0.6059619188308716
Loss at iteration 100 : 0.7430347204208374
Loss at iteration 150 : 0.6172498464584351
Loss at iteration 200 : 0.8014926910400391
Loss at iteration 250 : 0.2684614658355713
Loss at iteration 300 : 0.804146409034729
Loss at iteration 350 : 0.5672215223312378
Loss at iteration 400 : 0.8484519124031067
Loss at iteration 450 : 0.8623407483100891
Mean training loss eporch  56 :  0.6261080568929097
Loss at iteration 50 : 0.6578596830368042
Loss at iteration 100 : 0.6916787624359131
Loss at iteration 150 : 0.540778398513794
Loss at iteration 200 : 0.3792729377746582
Loss at iteration 250 : 0.5091744661331177
Loss at iteration 300 : 0.6494241952896118
Loss at iteration 350 : 0.5889406204223633
Loss at iteration 400 : 0.6913125514984131
Loss at iteration 450 : 0.3755832612514496
Mean training loss eporch  57 :  0.6258360636907642
Loss at iteration 50 : 0.5962090492248535
Loss at iteration 100 : 0.5921300649642944
Loss at iteration 150 : 0.36935290694236755
Loss at iteration 200 : 0.36432328820228577
Loss at iteration 250 : 0.7230485677719116
Loss at iteration 300 : 0.44766998291015625
Loss at iteration 350 : 0.6964764595031738
Loss at iteration 400 : 0.9375424385070801
Loss at iteration 450 : 0.5184762477874756
Mean training loss eporch  58 :  0.6250751316547394
Loss at iteration 50 : 0.8259400129318237
Loss at iteration 100 : 0.3950524628162384
Loss at iteration 150 : 0.5135911107063293
Loss at iteration 200 : 0.5212645530700684
Loss at iteration 250 : 0.7271233797073364
Loss at iteration 300 : 0.3845495581626892
Loss at iteration 350 : 0.47241753339767456
Loss at iteration 400 : 0.7508601546287537
Loss at iteration 450 : 0.6511937975883484
Mean training loss eporch  59 :  0.6256238802269911
Loss at iteration 50 : 0.4305645823478699
Loss at iteration 100 : 0.6325785517692566
Loss at iteration 150 : 0.7462875247001648
Loss at iteration 200 : 0.7546842098236084
Loss at iteration 250 : 1.0685510635375977
Loss at iteration 300 : 0.5837804079055786
Loss at iteration 350 : 0.7919902801513672
Loss at iteration 400 : 0.4463018476963043
Loss at iteration 450 : 0.605558454990387
Mean training loss eporch  60 :  0.6245595032512892
Loss at iteration 50 : 0.6848533153533936
Loss at iteration 100 : 0.8675689697265625
Loss at iteration 150 : 0.7194567918777466
Loss at iteration 200 : 0.8679362535476685
Loss at iteration 250 : 0.5080669522285461
Loss at iteration 300 : 0.48290377855300903
Loss at iteration 350 : 0.43053701519966125
Loss at iteration 400 : 0.8295161724090576
Loss at iteration 450 : 0.8189733028411865
Mean training loss eporch  61 :  0.6238807278322874
Loss at iteration 50 : 0.6705044507980347
Loss at iteration 100 : 0.4909025728702545
Loss at iteration 150 : 0.4306929409503937
Loss at iteration 200 : 0.8451217412948608
Loss at iteration 250 : 0.23572024703025818
Loss at iteration 300 : 0.4185471534729004
Loss at iteration 350 : 0.5566487908363342
Loss at iteration 400 : 0.9476160407066345
Loss at iteration 450 : 0.5940803289413452
Mean training loss eporch  62 :  0.6253092687686118
Loss at iteration 50 : 0.4397977590560913
Loss at iteration 100 : 0.8271536827087402
Loss at iteration 150 : 0.8647001385688782
Loss at iteration 200 : 0.5415108799934387
Loss at iteration 250 : 0.36504167318344116
Loss at iteration 300 : 0.564813494682312
Loss at iteration 350 : 0.6685864925384521
Loss at iteration 400 : 0.5420889258384705
Loss at iteration 450 : 0.847858190536499
Mean training loss eporch  63 :  0.623304398332929
Loss at iteration 50 : 0.6818488836288452
Loss at iteration 100 : 0.49714887142181396
Loss at iteration 150 : 0.4406082034111023
Loss at iteration 200 : 0.6587013006210327
Loss at iteration 250 : 0.6470720171928406
Loss at iteration 300 : 0.5940116047859192
Loss at iteration 350 : 0.36144697666168213
Loss at iteration 400 : 0.706646740436554
Loss at iteration 450 : 0.3736328184604645
Mean training loss eporch  64 :  0.6232970833279597
Loss at iteration 50 : 0.4915454387664795
Loss at iteration 100 : 0.7912525534629822
Loss at iteration 150 : 0.4223877191543579
Loss at iteration 200 : 0.6368752717971802
Loss at iteration 250 : 0.5348508358001709
Loss at iteration 300 : 0.7654982805252075
Loss at iteration 350 : 0.4875470995903015
Loss at iteration 400 : 0.794742226600647
Loss at iteration 450 : 0.5512250661849976
Mean training loss eporch  65 :  0.6229761043664801
Loss at iteration 50 : 0.5873315334320068
Loss at iteration 100 : 0.5284044146537781
Loss at iteration 150 : 0.4163960814476013
Loss at iteration 200 : 0.3879750669002533
Loss at iteration 250 : 0.6020554304122925
Loss at iteration 300 : 0.5851375460624695
Loss at iteration 350 : 0.3673750162124634
Loss at iteration 400 : 0.7285804748535156
Loss at iteration 450 : 0.39740633964538574
Mean training loss eporch  66 :  0.6223130308434555
Loss at iteration 50 : 0.5097317695617676
Loss at iteration 100 : 0.6463806629180908
Loss at iteration 150 : 0.5800876617431641
Loss at iteration 200 : 0.6363490223884583
Loss at iteration 250 : 0.5082769393920898
Loss at iteration 300 : 0.5901074409484863
Loss at iteration 350 : 0.671796441078186
Loss at iteration 400 : 0.7792988419532776
Loss at iteration 450 : 0.5979923605918884
Mean training loss eporch  67 :  0.6224753796804899
Loss at iteration 50 : 0.901641845703125
Loss at iteration 100 : 0.575381875038147
Loss at iteration 150 : 0.6254732608795166
Loss at iteration 200 : 0.9438639283180237
Loss at iteration 250 : 0.43889522552490234
Loss at iteration 300 : 0.5817927122116089
Loss at iteration 350 : 0.46451300382614136
Loss at iteration 400 : 0.5295184850692749
Loss at iteration 450 : 0.3869490623474121
Mean training loss eporch  68 :  0.6220041804919682
Loss at iteration 50 : 0.5316793918609619
Loss at iteration 100 : 0.5774869918823242
Loss at iteration 150 : 0.5617067217826843
Loss at iteration 200 : 0.5192030072212219
Loss at iteration 250 : 1.2196786403656006
Loss at iteration 300 : 0.9508929252624512
Loss at iteration 350 : 0.8818758726119995
Loss at iteration 400 : 0.8078958988189697
Loss at iteration 450 : 0.35377824306488037
Mean training loss eporch  69 :  0.6214578590009003
Loss at iteration 50 : 0.26409971714019775
Loss at iteration 100 : 0.9714460372924805
Loss at iteration 150 : 0.8165662288665771
Loss at iteration 200 : 0.6860291361808777
Loss at iteration 250 : 0.6515483856201172
Loss at iteration 300 : 0.3873831629753113
Loss at iteration 350 : 0.5050287246704102
Loss at iteration 400 : 0.520520031452179
Loss at iteration 450 : 0.44403985142707825
Mean training loss eporch  70 :  0.621422465843885
Loss at iteration 50 : 0.6885597705841064
Loss at iteration 100 : 0.7140793800354004
Loss at iteration 150 : 0.7232656478881836
Loss at iteration 200 : 0.6556190252304077
Loss at iteration 250 : 0.4108216166496277
Loss at iteration 300 : 0.794527530670166
Loss at iteration 350 : 0.6657909750938416
Loss at iteration 400 : 0.3198801875114441
Loss at iteration 450 : 0.5416771173477173
Mean training loss eporch  71 :  0.6210621721949039
Loss at iteration 50 : 0.477554589509964
Loss at iteration 100 : 1.0124094486236572
Loss at iteration 150 : 0.8668832778930664
Loss at iteration 200 : 1.0557405948638916
Loss at iteration 250 : 0.5061817169189453
Loss at iteration 300 : 0.43998104333877563
Loss at iteration 350 : 0.6324767470359802
Loss at iteration 400 : 0.38255834579467773
Loss at iteration 450 : 0.6567266583442688
Mean training loss eporch  72 :  0.6212467603915406
Loss at iteration 50 : 0.5598424077033997
Loss at iteration 100 : 0.5091108679771423
Loss at iteration 150 : 0.4038654565811157
Loss at iteration 200 : 1.0293018817901611
Loss at iteration 250 : 1.5047667026519775
Loss at iteration 300 : 0.3748443126678467
Loss at iteration 350 : 0.2898441255092621
Loss at iteration 400 : 0.6137176156044006
Loss at iteration 450 : 0.18514007329940796
Mean training loss eporch  73 :  0.6204112830521172
Loss at iteration 50 : 0.6163464188575745
Loss at iteration 100 : 0.49099504947662354
Loss at iteration 150 : 1.0780271291732788
Loss at iteration 200 : 0.8258490562438965
Loss at iteration 250 : 0.378164678812027
Loss at iteration 300 : 0.771087646484375
Loss at iteration 350 : 0.43641024827957153
Loss at iteration 400 : 0.40143290162086487
Loss at iteration 450 : 0.3232528567314148
Mean training loss eporch  74 :  0.6197650013296674
Loss at iteration 50 : 0.9845069646835327
Loss at iteration 100 : 0.5348606109619141
Loss at iteration 150 : 0.6930770874023438
Loss at iteration 200 : 0.5221869945526123
Loss at iteration 250 : 0.47623634338378906
Loss at iteration 300 : 0.608444333076477
Loss at iteration 350 : 0.5600621700286865
Loss at iteration 400 : 0.894197940826416
Loss at iteration 450 : 1.153638243675232
Mean training loss eporch  75 :  0.6198762044248222
Loss at iteration 50 : 0.37444204092025757
Loss at iteration 100 : 0.8535258769989014
Loss at iteration 150 : 0.3857879638671875
Loss at iteration 200 : 0.5949593782424927
Loss at iteration 250 : 0.8149436712265015
Loss at iteration 300 : 1.0045727491378784
Loss at iteration 350 : 1.154582142829895
Loss at iteration 400 : 0.9711118936538696
Loss at iteration 450 : 0.355286180973053
Mean training loss eporch  76 :  0.6190590506817008
Loss at iteration 50 : 0.5119613409042358
Loss at iteration 100 : 0.6191651821136475
Loss at iteration 150 : 0.9957693219184875
Loss at iteration 200 : 0.450429767370224
Loss at iteration 250 : 0.5815585851669312
Loss at iteration 300 : 0.5785697102546692
Loss at iteration 350 : 0.6936012506484985
Loss at iteration 400 : 0.5025404691696167
Loss at iteration 450 : 0.6843193173408508
Mean training loss eporch  77 :  0.6195982518181142
Loss at iteration 50 : 0.5729662179946899
Loss at iteration 100 : 0.7799745798110962
Loss at iteration 150 : 0.32506245374679565
Loss at iteration 200 : 0.8454651832580566
Loss at iteration 250 : 0.4243144690990448
Loss at iteration 300 : 0.42037123441696167
Loss at iteration 350 : 0.6436665058135986
Loss at iteration 400 : 0.4582898020744324
Loss at iteration 450 : 0.7823982238769531
Mean training loss eporch  78 :  0.6195811294175092
Loss at iteration 50 : 0.5533249974250793
Loss at iteration 100 : 0.5742588043212891
Loss at iteration 150 : 0.6334595680236816
Loss at iteration 200 : 0.3528384268283844
Loss at iteration 250 : 0.6734176874160767
Loss at iteration 300 : 0.6287761926651001
Loss at iteration 350 : 0.5632879734039307
Loss at iteration 400 : 0.7941210269927979
Loss at iteration 450 : 0.4524630606174469
Mean training loss eporch  79 :  0.6193496732033447
Loss at iteration 50 : 0.7580262422561646
Loss at iteration 100 : 0.7933980226516724
Loss at iteration 150 : 0.7944644093513489
Loss at iteration 200 : 0.9870201945304871
Loss at iteration 250 : 0.38805443048477173
Loss at iteration 300 : 0.9286479949951172
Loss at iteration 350 : 0.7619888186454773
Loss at iteration 400 : 0.9078879356384277
Loss at iteration 450 : 0.5600241422653198
Mean training loss eporch  80 :  0.6183655444049436
Loss at iteration 50 : 0.9002720713615417
Loss at iteration 100 : 0.9897207021713257
Loss at iteration 150 : 0.5578568577766418
Loss at iteration 200 : 0.7946285009384155
Loss at iteration 250 : 0.7286176681518555
Loss at iteration 300 : 0.7076926827430725
Loss at iteration 350 : 0.37328726053237915
Loss at iteration 400 : 0.3674083650112152
Loss at iteration 450 : 1.1848012208938599
Mean training loss eporch  81 :  0.6185142462098948
Loss at iteration 50 : 0.9387856721878052
Loss at iteration 100 : 0.911358118057251
Loss at iteration 150 : 0.6797100305557251
Loss at iteration 200 : 0.547119677066803
Loss at iteration 250 : 0.5733785629272461
Loss at iteration 300 : 0.6559959650039673
Loss at iteration 350 : 0.7869537472724915
Loss at iteration 400 : 0.4244173765182495
Loss at iteration 450 : 1.090010643005371
Mean training loss eporch  82 :  0.6179439243100677
Loss at iteration 50 : 0.5916630029678345
Loss at iteration 100 : 0.5742853879928589
Loss at iteration 150 : 0.9002395272254944
Loss at iteration 200 : 0.5864623785018921
Loss at iteration 250 : 0.4886077642440796
Loss at iteration 300 : 0.6474766731262207
Loss at iteration 350 : 0.4731466770172119
Loss at iteration 400 : 0.582381010055542
Loss at iteration 450 : 0.3752062916755676
Mean training loss eporch  83 :  0.6173139733874149
Loss at iteration 50 : 0.811430811882019
Loss at iteration 100 : 0.666327178478241
Loss at iteration 150 : 0.8319956064224243
Loss at iteration 200 : 0.35543179512023926
Loss at iteration 250 : 0.49201056361198425
Loss at iteration 300 : 0.4166070222854614
Loss at iteration 350 : 0.7939441204071045
Loss at iteration 400 : 0.4716995656490326
Loss at iteration 450 : 0.7832674384117126
Mean training loss eporch  84 :  0.6175326185308739
Loss at iteration 50 : 0.7479042410850525
Loss at iteration 100 : 0.43070071935653687
Loss at iteration 150 : 0.3997449278831482
Loss at iteration 200 : 0.8174401521682739
Loss at iteration 250 : 0.8602863550186157
Loss at iteration 300 : 0.3676920235157013
Loss at iteration 350 : 0.32810133695602417
Loss at iteration 400 : 1.0042387247085571
Loss at iteration 450 : 0.6323485374450684
Mean training loss eporch  85 :  0.6172832213809799
Loss at iteration 50 : 0.8186284899711609
Loss at iteration 100 : 0.44178077578544617
Loss at iteration 150 : 0.7380409240722656
Loss at iteration 200 : 0.4437476396560669
Loss at iteration 250 : 0.8614760637283325
Loss at iteration 300 : 0.36659950017929077
Loss at iteration 350 : 0.26846131682395935
Loss at iteration 400 : 0.6591291427612305
Loss at iteration 450 : 0.9334992170333862
Mean training loss eporch  86 :  0.6169309871092002
Loss at iteration 50 : 0.9642395377159119
Loss at iteration 100 : 0.6527756452560425
Loss at iteration 150 : 0.5208762288093567
Loss at iteration 200 : 0.7718566656112671
Loss at iteration 250 : 0.3861018121242523
Loss at iteration 300 : 0.5030705332756042
Loss at iteration 350 : 0.5442745685577393
Loss at iteration 400 : 0.38236141204833984
Loss at iteration 450 : 0.6396145820617676
Mean training loss eporch  87 :  0.6174895727547143
Loss at iteration 50 : 0.43844592571258545
Loss at iteration 100 : 0.6399534940719604
Loss at iteration 150 : 0.4748566150665283
Loss at iteration 200 : 0.5236672163009644
Loss at iteration 250 : 1.0604612827301025
Loss at iteration 300 : 0.38440126180648804
Loss at iteration 350 : 0.5724121332168579
Loss at iteration 400 : 0.5446758270263672
Loss at iteration 450 : 0.5568758249282837
Mean training loss eporch  88 :  0.6165894586359107
Loss at iteration 50 : 0.2779582738876343
Loss at iteration 100 : 0.38645464181900024
Loss at iteration 150 : 0.8984785079956055
Loss at iteration 200 : 0.850263774394989
Loss at iteration 250 : 0.37702682614326477
Loss at iteration 300 : 0.4725697636604309
Loss at iteration 350 : 0.8253742456436157
Loss at iteration 400 : 0.7154884934425354
Loss at iteration 450 : 0.5290611386299133
Mean training loss eporch  89 :  0.6173436299402844
Loss at iteration 50 : 0.4807785451412201
Loss at iteration 100 : 0.5576164722442627
Loss at iteration 150 : 0.6099247932434082
Loss at iteration 200 : 0.5408338904380798
Loss at iteration 250 : 0.4390561282634735
Loss at iteration 300 : 0.7288535237312317
Loss at iteration 350 : 0.8441307544708252
Loss at iteration 400 : 0.6942422389984131
Loss at iteration 450 : 0.5959935188293457
Mean training loss eporch  90 :  0.6164813817919049
Loss at iteration 50 : 0.5535944700241089
Loss at iteration 100 : 0.33397024869918823
Loss at iteration 150 : 0.5904653072357178
Loss at iteration 200 : 0.7930880784988403
Loss at iteration 250 : 0.3048679530620575
Loss at iteration 300 : 0.4852588176727295
Loss at iteration 350 : 0.6374919414520264
Loss at iteration 400 : 1.1571478843688965
Loss at iteration 450 : 0.3654034733772278
Mean training loss eporch  91 :  0.6198806169766262
Loss at iteration 50 : 0.4211738705635071
Loss at iteration 100 : 0.48089954257011414
Loss at iteration 150 : 0.9275871515274048
Loss at iteration 200 : 0.43565183877944946
Loss at iteration 250 : 0.7216960787773132
Loss at iteration 300 : 0.6356116533279419
Loss at iteration 350 : 0.5509268045425415
Loss at iteration 400 : 0.31298524141311646
Loss at iteration 450 : 0.6825708150863647
Mean training loss eporch  92 :  0.615805920197874
Loss at iteration 50 : 0.524563729763031
Loss at iteration 100 : 1.1197450160980225
Loss at iteration 150 : 0.46321412920951843
Loss at iteration 200 : 0.7619154453277588
Loss at iteration 250 : 0.3904053568840027
Loss at iteration 300 : 0.30315977334976196
Loss at iteration 350 : 0.39393150806427
Loss at iteration 400 : 0.8237287402153015
Loss at iteration 450 : 0.44139647483825684
Mean training loss eporch  93 :  0.6157598863024591
Loss at iteration 50 : 0.7522059679031372
Loss at iteration 100 : 0.8804143667221069
Loss at iteration 150 : 0.9432066082954407
Loss at iteration 200 : 0.4881632626056671
Loss at iteration 250 : 0.5896744728088379
Loss at iteration 300 : 0.6186346411705017
Loss at iteration 350 : 0.4394162893295288
Loss at iteration 400 : 0.6838057637214661
Loss at iteration 450 : 0.7598284482955933
Mean training loss eporch  94 :  0.615916563096655
Loss at iteration 50 : 0.4315793514251709
Loss at iteration 100 : 0.7312679290771484
Loss at iteration 150 : 0.47421789169311523
Loss at iteration 200 : 1.005601406097412
Loss at iteration 250 : 0.33831697702407837
Loss at iteration 300 : 0.591984212398529
Loss at iteration 350 : 0.6553465723991394
Loss at iteration 400 : 0.3646126091480255
Loss at iteration 450 : 0.4086294174194336
Mean training loss eporch  95 :  0.6153190630063352
Loss at iteration 50 : 0.3440207242965698
Loss at iteration 100 : 0.6203105449676514
Loss at iteration 150 : 1.0242934226989746
Loss at iteration 200 : 0.3061686158180237
Loss at iteration 250 : 0.6894970536231995
Loss at iteration 300 : 0.555475115776062
Loss at iteration 350 : 0.7913945913314819
Loss at iteration 400 : 0.5466873645782471
Loss at iteration 450 : 0.4879254996776581
Mean training loss eporch  96 :  0.6151213282758222
Loss at iteration 50 : 0.4030578136444092
Loss at iteration 100 : 0.46361568570137024
Loss at iteration 150 : 0.5285780429840088
Loss at iteration 200 : 0.795886754989624
Loss at iteration 250 : 0.5644247531890869
Loss at iteration 300 : 0.7790535688400269
Loss at iteration 350 : 0.6147332191467285
Loss at iteration 400 : 0.628887414932251
Loss at iteration 450 : 0.9723755121231079
Mean training loss eporch  97 :  0.6158861808135918
Loss at iteration 50 : 0.6642588376998901
Loss at iteration 100 : 0.7348783016204834
Loss at iteration 150 : 0.46377843618392944
Loss at iteration 200 : 0.6280063390731812
Loss at iteration 250 : 0.9334474205970764
Loss at iteration 300 : 0.9204753637313843
Loss at iteration 350 : 0.8609013557434082
Loss at iteration 400 : 0.5393007397651672
Loss at iteration 450 : 0.40376242995262146
Mean training loss eporch  98 :  0.614371704194835
Loss at iteration 50 : 0.4946194589138031
Loss at iteration 100 : 0.45200657844543457
Loss at iteration 150 : 0.7278811931610107
Loss at iteration 200 : 0.8492704033851624
Loss at iteration 250 : 0.9614444375038147
Loss at iteration 300 : 0.536191463470459
Loss at iteration 350 : 0.834028959274292
Loss at iteration 400 : 1.1053894758224487
Loss at iteration 450 : 0.9535273313522339
Mean training loss eporch  99 :  0.6145785609610909
Loss at iteration 50 : 0.47570234537124634
Loss at iteration 100 : 0.5564053058624268
Loss at iteration 150 : 0.6225312948226929
Loss at iteration 200 : 0.45089972019195557
Loss at iteration 250 : 0.45570245385169983
Loss at iteration 300 : 0.7113955020904541
Loss at iteration 350 : 0.5018414258956909
Loss at iteration 400 : 0.666753888130188
Loss at iteration 450 : 0.262012779712677
Mean training loss eporch  100 :  0.614214328698284
Loss at iteration 50 : 0.3635517656803131
Loss at iteration 100 : 0.6853489875793457
Loss at iteration 150 : 0.5379583835601807
Loss at iteration 200 : 0.5912066698074341
Loss at iteration 250 : 0.7040650844573975
Loss at iteration 300 : 0.591026782989502
Loss at iteration 350 : 0.5243037939071655
Loss at iteration 400 : 1.0059139728546143
Loss at iteration 450 : 0.71990966796875
Mean training loss eporch  101 :  0.6143273824677806
Loss at iteration 50 : 0.7821888327598572
Loss at iteration 100 : 0.5953365564346313
Loss at iteration 150 : 0.5103857517242432
Loss at iteration 200 : 0.5642659664154053
Loss at iteration 250 : 0.5303481221199036
Loss at iteration 300 : 0.33494606614112854
Loss at iteration 350 : 0.34319132566452026
Loss at iteration 400 : 0.5171524882316589
Loss at iteration 450 : 0.48679119348526
Mean training loss eporch  102 :  0.6137204814904903
Loss at iteration 50 : 0.7656040787696838
Loss at iteration 100 : 0.7394325733184814
Loss at iteration 150 : 0.5623948574066162
Loss at iteration 200 : 1.3059896230697632
Loss at iteration 250 : 0.6702749729156494
Loss at iteration 300 : 0.41776275634765625
Loss at iteration 350 : 0.8164998292922974
Loss at iteration 400 : 0.7914037108421326
Loss at iteration 450 : 0.8996305465698242
Mean training loss eporch  103 :  0.6138433931886401
Loss at iteration 50 : 0.7157233357429504
Loss at iteration 100 : 2.051449775695801
Loss at iteration 150 : 0.39500686526298523
Loss at iteration 200 : 0.37733858823776245
Loss at iteration 250 : 0.8069530725479126
Loss at iteration 300 : 0.3650286793708801
Loss at iteration 350 : 0.890252947807312
Loss at iteration 400 : 0.5243462324142456
Loss at iteration 450 : 0.8064126968383789
Mean training loss eporch  104 :  0.6134767772076519
Loss at iteration 50 : 0.7407916188240051
Loss at iteration 100 : 0.6958531141281128
Loss at iteration 150 : 0.5300883054733276
Loss at iteration 200 : 0.5694131255149841
Loss at iteration 250 : 0.4349275231361389
Loss at iteration 300 : 0.3474428653717041
Loss at iteration 350 : 0.6373186111450195
Loss at iteration 400 : 0.5591151714324951
Loss at iteration 450 : 0.5224642753601074
Mean training loss eporch  105 :  0.6139925957411901
Loss at iteration 50 : 0.6904469132423401
Loss at iteration 100 : 0.4064721167087555
Loss at iteration 150 : 0.8107494711875916
Loss at iteration 200 : 0.5348629951477051
Loss at iteration 250 : 0.7018316388130188
Loss at iteration 300 : 0.8325612545013428
Loss at iteration 350 : 0.5743162631988525
Loss at iteration 400 : 0.6431435346603394
Loss at iteration 450 : 1.0567971467971802
Mean training loss eporch  106 :  0.6140920585605889
Loss at iteration 50 : 0.29043981432914734
Loss at iteration 100 : 0.5335495471954346
Loss at iteration 150 : 0.264844685792923
Loss at iteration 200 : 0.4997538626194
Loss at iteration 250 : 0.9631922841072083
Loss at iteration 300 : 0.4971340000629425
Loss at iteration 350 : 0.7633216977119446
Loss at iteration 400 : 0.6594231128692627
Loss at iteration 450 : 0.5281451940536499
Mean training loss eporch  107 :  0.6141996784564342
Loss at iteration 50 : 0.670933187007904
Loss at iteration 100 : 0.777030885219574
Loss at iteration 150 : 0.7767555713653564
Loss at iteration 200 : 0.6468093395233154
Loss at iteration 250 : 0.6236550807952881
Loss at iteration 300 : 0.6297209858894348
Loss at iteration 350 : 0.8982448577880859
Loss at iteration 400 : 0.3349989652633667
Loss at iteration 450 : 0.6921851634979248
Mean training loss eporch  108 :  0.6132288613516417
Loss at iteration 50 : 0.4112316966056824
Loss at iteration 100 : 0.8091071248054504
Loss at iteration 150 : 0.5585396885871887
Loss at iteration 200 : 1.1886389255523682
Loss at iteration 250 : 0.9925135374069214
Loss at iteration 300 : 0.23150679469108582
Loss at iteration 350 : 0.6691344976425171
Loss at iteration 400 : 0.44520777463912964
Loss at iteration 450 : 0.5889774560928345
Mean training loss eporch  109 :  0.6124040169259495
Loss at iteration 50 : 0.5514337420463562
Loss at iteration 100 : 0.5665667057037354
Loss at iteration 150 : 1.0430086851119995
Loss at iteration 200 : 0.7530872821807861
Loss at iteration 250 : 0.8567582368850708
Loss at iteration 300 : 0.34625864028930664
Loss at iteration 350 : 0.382487416267395
Loss at iteration 400 : 0.6636707782745361
Loss at iteration 450 : 0.5817680954933167
Mean training loss eporch  110 :  0.6121059811389596
Loss at iteration 50 : 0.3607448935508728
Loss at iteration 100 : 0.49874189496040344
Loss at iteration 150 : 0.6048547029495239
Loss at iteration 200 : 0.7805090546607971
Loss at iteration 250 : 0.6617064476013184
Loss at iteration 300 : 0.7990953922271729
Loss at iteration 350 : 0.5715852975845337
Loss at iteration 400 : 0.4630745053291321
Loss at iteration 450 : 0.7775990962982178
Mean training loss eporch  111 :  0.612390305270211
Loss at iteration 50 : 0.8281686902046204
Loss at iteration 100 : 0.47661092877388
Loss at iteration 150 : 0.9138696193695068
Loss at iteration 200 : 0.46828314661979675
Loss at iteration 250 : 0.3857578635215759
Loss at iteration 300 : 0.2576264441013336
Loss at iteration 350 : 0.6199051737785339
Loss at iteration 400 : 0.5566190481185913
Loss at iteration 450 : 0.45593300461769104
Mean training loss eporch  112 :  0.6124617633522804
Loss at iteration 50 : 0.6663267016410828
Loss at iteration 100 : 0.347791850566864
Loss at iteration 150 : 0.8642759919166565
Loss at iteration 200 : 0.6919811964035034
Loss at iteration 250 : 0.8108761310577393
Loss at iteration 300 : 0.8878421783447266
Loss at iteration 350 : 0.5927324295043945
Loss at iteration 400 : 0.8955886960029602
Loss at iteration 450 : 0.9162473678588867
Mean training loss eporch  113 :  0.6123238802267916
Loss at iteration 50 : 0.44935017824172974
Loss at iteration 100 : 0.7127672433853149
Loss at iteration 150 : 0.6322555541992188
Loss at iteration 200 : 0.3603324890136719
Loss at iteration 250 : 0.5152231454849243
Loss at iteration 300 : 0.37127062678337097
Loss at iteration 350 : 0.8212397694587708
Loss at iteration 400 : 0.9642294645309448
Loss at iteration 450 : 0.3886885643005371
Mean training loss eporch  114 :  0.6115508530304522
Loss at iteration 50 : 0.48400184512138367
Loss at iteration 100 : 0.5995711088180542
Loss at iteration 150 : 0.803960919380188
Loss at iteration 200 : 0.6930170655250549
Loss at iteration 250 : 0.4276790916919708
Loss at iteration 300 : 0.6429865956306458
Loss at iteration 350 : 0.8860394954681396
Loss at iteration 400 : 0.4546079933643341
Loss at iteration 450 : 0.3095327615737915
Mean training loss eporch  115 :  0.6119357702560005
Loss at iteration 50 : 0.6996023058891296
Loss at iteration 100 : 0.38111740350723267
Loss at iteration 150 : 0.40918615460395813
Loss at iteration 200 : 0.6318730711936951
Loss at iteration 250 : 0.39953938126564026
Loss at iteration 300 : 0.88160240650177
Loss at iteration 350 : 0.7142698168754578
Loss at iteration 400 : 0.6387437582015991
Loss at iteration 450 : 0.910371720790863
Mean training loss eporch  116 :  0.6114178852618489
Loss at iteration 50 : 0.7671360373497009
Loss at iteration 100 : 0.24550381302833557
Loss at iteration 150 : 0.7429978251457214
Loss at iteration 200 : 0.6161088943481445
Loss at iteration 250 : 0.4370579123497009
Loss at iteration 300 : 0.471549928188324
Loss at iteration 350 : 0.5402769446372986
Loss at iteration 400 : 0.905375599861145
Loss at iteration 450 : 0.5913289785385132
Mean training loss eporch  117 :  0.611421907608978
Loss at iteration 50 : 0.40460294485092163
Loss at iteration 100 : 0.468718945980072
Loss at iteration 150 : 0.6364239454269409
Loss at iteration 200 : 0.5766586065292358
Loss at iteration 250 : 1.027334213256836
Loss at iteration 300 : 0.5295157432556152
Loss at iteration 350 : 0.8767145276069641
Loss at iteration 400 : 0.39477500319480896
Loss at iteration 450 : 0.44003012776374817
Mean training loss eporch  118 :  0.6112787164903585
Loss at iteration 50 : 0.8353455066680908
Loss at iteration 100 : 0.6777619123458862
Loss at iteration 150 : 0.7614142894744873
Loss at iteration 200 : 0.9461452960968018
Loss at iteration 250 : 0.3719099462032318
Loss at iteration 300 : 0.8953301310539246
Loss at iteration 350 : 0.4313472509384155
Loss at iteration 400 : 0.4144272208213806
Loss at iteration 450 : 0.3790395259857178
Mean training loss eporch  119 :  0.6113511409033792
Loss at iteration 50 : 0.41532033681869507
Loss at iteration 100 : 0.5602037906646729
Loss at iteration 150 : 0.5474932193756104
Loss at iteration 200 : 1.0302724838256836
Loss at iteration 250 : 0.7797882556915283
Loss at iteration 300 : 0.38775384426116943
Loss at iteration 350 : 0.766191840171814
Loss at iteration 400 : 0.4925227165222168
Loss at iteration 450 : 0.6582556962966919
Mean training loss eporch  120 :  0.6104944322460865
Loss at iteration 50 : 0.4658905863761902
Loss at iteration 100 : 0.8181334733963013
Loss at iteration 150 : 0.9832103252410889
Loss at iteration 200 : 0.5383175611495972
Loss at iteration 250 : 0.7605890035629272
Loss at iteration 300 : 0.7407100796699524
Loss at iteration 350 : 0.8260712027549744
Loss at iteration 400 : 0.4858345091342926
Loss at iteration 450 : 0.38231635093688965
Mean training loss eporch  121 :  0.6100919968351161
Loss at iteration 50 : 0.9964103102684021
Loss at iteration 100 : 0.4201663136482239
Loss at iteration 150 : 0.778705358505249
Loss at iteration 200 : 0.623668909072876
Loss at iteration 250 : 0.4949159622192383
Loss at iteration 300 : 0.6502800583839417
Loss at iteration 350 : 0.6966110467910767
Loss at iteration 400 : 0.9307854175567627
Loss at iteration 450 : 0.5636763572692871
Mean training loss eporch  122 :  0.6105350198294328
Loss at iteration 50 : 0.7407273054122925
Loss at iteration 100 : 0.6000458002090454
Loss at iteration 150 : 0.7008508443832397
Loss at iteration 200 : 1.1244587898254395
Loss at iteration 250 : 0.3577831983566284
Loss at iteration 300 : 0.6519562602043152
Loss at iteration 350 : 0.7139369249343872
Loss at iteration 400 : 0.6039562225341797
Loss at iteration 450 : 0.4334293603897095
Mean training loss eporch  123 :  0.6097964112915754
Loss at iteration 50 : 0.7327274084091187
Loss at iteration 100 : 0.2817378044128418
Loss at iteration 150 : 0.5013762712478638
Loss at iteration 200 : 0.585421085357666
Loss at iteration 250 : 0.6105794906616211
Loss at iteration 300 : 0.463910311460495
Loss at iteration 350 : 0.3656463623046875
Loss at iteration 400 : 0.5036160945892334
Loss at iteration 450 : 0.551059365272522
Mean training loss eporch  124 :  0.6100233496482402
Loss at iteration 50 : 0.49476975202560425
Loss at iteration 100 : 0.4652348756790161
Loss at iteration 150 : 0.9132773876190186
Loss at iteration 200 : 0.6326937675476074
Loss at iteration 250 : 0.21811875700950623
Loss at iteration 300 : 0.6850264072418213
Loss at iteration 350 : 0.3483424782752991
Loss at iteration 400 : 1.1064023971557617
Loss at iteration 450 : 0.9659556150436401
Mean training loss eporch  125 :  0.6094947717927989
Loss at iteration 50 : 0.48381537199020386
Loss at iteration 100 : 0.40335243940353394
Loss at iteration 150 : 0.3672974407672882
Loss at iteration 200 : 0.49125969409942627
Loss at iteration 250 : 1.3945430517196655
Loss at iteration 300 : 0.9098016023635864
Loss at iteration 350 : 0.7098281979560852
Loss at iteration 400 : 0.6979182958602905
Loss at iteration 450 : 0.4167514741420746
Mean training loss eporch  126 :  0.6100637159275211
Loss at iteration 50 : 0.7399695515632629
Loss at iteration 100 : 0.4440688192844391
Loss at iteration 150 : 0.5399830937385559
Loss at iteration 200 : 0.4277188181877136
Loss at iteration 250 : 0.7016980648040771
Loss at iteration 300 : 0.9781152009963989
Loss at iteration 350 : 0.5708792209625244
Loss at iteration 400 : 1.0420444011688232
Loss at iteration 450 : 0.2734173536300659
Mean training loss eporch  127 :  0.609862765538892
Loss at iteration 50 : 0.577678918838501
Loss at iteration 100 : 0.48097920417785645
Loss at iteration 150 : 0.8612902760505676
Loss at iteration 200 : 0.7948325872421265
Loss at iteration 250 : 0.4910621643066406
Loss at iteration 300 : 0.5185543298721313
Loss at iteration 350 : 0.6346952319145203
Loss at iteration 400 : 0.5496034622192383
Loss at iteration 450 : 0.3918091654777527
Mean training loss eporch  128 :  0.6087390328114501
Loss at iteration 50 : 0.49191033840179443
Loss at iteration 100 : 0.8251230716705322
Loss at iteration 150 : 0.7734702825546265
Loss at iteration 200 : 0.8133373856544495
Loss at iteration 250 : 0.5835452675819397
Loss at iteration 300 : 1.1178542375564575
Loss at iteration 350 : 0.9194092154502869
Loss at iteration 400 : 0.7416383028030396
Loss at iteration 450 : 0.7023486495018005
Mean training loss eporch  129 :  0.6090634359226067
Loss at iteration 50 : 0.5696969628334045
Loss at iteration 100 : 0.8427374362945557
Loss at iteration 150 : 0.8946664929389954
Loss at iteration 200 : 0.3348565697669983
Loss at iteration 250 : 0.42404618859291077
Loss at iteration 300 : 0.8208538293838501
Loss at iteration 350 : 0.4078192412853241
Loss at iteration 400 : 0.8652675151824951
Loss at iteration 450 : 0.5307710766792297
Mean training loss eporch  130 :  0.608832233526966
Loss at iteration 50 : 0.6005736589431763
Loss at iteration 100 : 0.8110001683235168
Loss at iteration 150 : 0.8128100633621216
Loss at iteration 200 : 1.7718437910079956
Loss at iteration 250 : 0.4428046941757202
Loss at iteration 300 : 0.42556262016296387
Loss at iteration 350 : 0.7026420831680298
Loss at iteration 400 : 0.5188003182411194
Loss at iteration 450 : 0.2908713221549988
Mean training loss eporch  131 :  0.6086454728470188
Loss at iteration 50 : 0.7919476628303528
Loss at iteration 100 : 1.3801870346069336
Loss at iteration 150 : 0.5199634432792664
Loss at iteration 200 : 0.8218759894371033
Loss at iteration 250 : 0.46523189544677734
Loss at iteration 300 : 0.48926523327827454
Loss at iteration 350 : 0.5906029343605042
Loss at iteration 400 : 0.7549698352813721
Loss at iteration 450 : 0.48787131905555725
Mean training loss eporch  132 :  0.6084988147456776
Loss at iteration 50 : 0.7079490423202515
Loss at iteration 100 : 0.8742200136184692
Loss at iteration 150 : 0.3930271863937378
Loss at iteration 200 : 0.35429394245147705
Loss at iteration 250 : 0.6002033948898315
Loss at iteration 300 : 0.4901577830314636
Loss at iteration 350 : 0.48375359177589417
Loss at iteration 400 : 0.6631913781166077
Loss at iteration 450 : 0.3799077272415161
Mean training loss eporch  133 :  0.6084102415265399
Loss at iteration 50 : 0.9371501803398132
Loss at iteration 100 : 0.8478411436080933
Loss at iteration 150 : 0.2811594307422638
Loss at iteration 200 : 0.2976014018058777
Loss at iteration 250 : 0.7280640602111816
Loss at iteration 300 : 0.3902781009674072
Loss at iteration 350 : 0.5674946308135986
Loss at iteration 400 : 0.7564607262611389
Loss at iteration 450 : 0.8846467733383179
Mean training loss eporch  134 :  0.6078756984681764
Loss at iteration 50 : 0.6681630611419678
Loss at iteration 100 : 0.4870546758174896
Loss at iteration 150 : 0.5201737880706787
Loss at iteration 200 : 0.8879663348197937
Loss at iteration 250 : 1.0986958742141724
Loss at iteration 300 : 0.6051616668701172
Loss at iteration 350 : 0.5819375514984131
Loss at iteration 400 : 0.6335686445236206
Loss at iteration 450 : 0.5225051641464233
Mean training loss eporch  135 :  0.6077397897153719
Loss at iteration 50 : 0.477095365524292
Loss at iteration 100 : 0.5431000590324402
Loss at iteration 150 : 0.5958404541015625
Loss at iteration 200 : 0.5694798827171326
Loss at iteration 250 : 0.8947485685348511
Loss at iteration 300 : 0.3102840781211853
Loss at iteration 350 : 0.7423613667488098
Loss at iteration 400 : 0.6633208394050598
Loss at iteration 450 : 0.28021734952926636
Mean training loss eporch  136 :  0.6073066147675574
Loss at iteration 50 : 0.8440971970558167
Loss at iteration 100 : 0.5087195634841919
Loss at iteration 150 : 0.6825747489929199
Loss at iteration 200 : 0.7669219374656677
Loss at iteration 250 : 0.7596516013145447
Loss at iteration 300 : 0.6708250641822815
Loss at iteration 350 : 0.9454972147941589
Loss at iteration 400 : 0.4386376142501831
Loss at iteration 450 : 0.3877006769180298
Mean training loss eporch  137 :  0.6068159130764307
Loss at iteration 50 : 0.48596253991127014
Loss at iteration 100 : 0.4427494406700134
Loss at iteration 150 : 0.5937262177467346
Loss at iteration 200 : 0.6051571369171143
Loss at iteration 250 : 0.6285873651504517
Loss at iteration 300 : 0.6604456901550293
Loss at iteration 350 : 1.2740291357040405
Loss at iteration 400 : 0.5915861129760742
Loss at iteration 450 : 0.7728896737098694
Mean training loss eporch  138 :  0.6064265456720875
Loss at iteration 50 : 1.2035834789276123
Loss at iteration 100 : 0.8195109367370605
Loss at iteration 150 : 0.8973135948181152
Loss at iteration 200 : 0.7487866282463074
Loss at iteration 250 : 0.30022215843200684
Loss at iteration 300 : 0.5492383241653442
Loss at iteration 350 : 0.5161080360412598
Loss at iteration 400 : 0.6385498046875
Loss at iteration 450 : 0.3990585207939148
Mean training loss eporch  139 :  0.6068068808529168
Loss at iteration 50 : 0.6974578499794006
Loss at iteration 100 : 0.8762273788452148
Loss at iteration 150 : 1.4970792531967163
Loss at iteration 200 : 0.31789565086364746
Loss at iteration 250 : 0.328622967004776
Loss at iteration 300 : 0.30425363779067993
Loss at iteration 350 : 0.33017146587371826
Loss at iteration 400 : 0.7440528869628906
Loss at iteration 450 : 1.0000007152557373
Mean training loss eporch  140 :  0.6059797540244697
Loss at iteration 50 : 0.7553353309631348
Loss at iteration 100 : 0.5236841440200806
Loss at iteration 150 : 0.6151165962219238
Loss at iteration 200 : 0.4305359125137329
Loss at iteration 250 : 0.20179486274719238
Loss at iteration 300 : 0.4693017303943634
Loss at iteration 350 : 0.510741114616394
Loss at iteration 400 : 0.7338961958885193
Loss at iteration 450 : 0.5389785766601562
Mean training loss eporch  141 :  0.6055209536931505
Loss at iteration 50 : 0.5306072235107422
Loss at iteration 100 : 0.3890257179737091
Loss at iteration 150 : 0.5632715225219727
Loss at iteration 200 : 0.5171798467636108
Loss at iteration 250 : 0.6181835532188416
Loss at iteration 300 : 0.6732005476951599
Loss at iteration 350 : 0.8102328777313232
Loss at iteration 400 : 0.5656263828277588
Loss at iteration 450 : 0.7243359684944153
Mean training loss eporch  142 :  0.605214967731402
Loss at iteration 50 : 0.6195288300514221
Loss at iteration 100 : 0.33339083194732666
Loss at iteration 150 : 0.984134316444397
Loss at iteration 200 : 0.8809051513671875
Loss at iteration 250 : 0.42110475897789
Loss at iteration 300 : 0.6648417711257935
Loss at iteration 350 : 0.7211458086967468
Loss at iteration 400 : 0.7577357292175293
Loss at iteration 450 : 0.6488267183303833
Mean training loss eporch  143 :  0.6054807724002514
Loss at iteration 50 : 0.25125083327293396
Loss at iteration 100 : 0.4080340564250946
Loss at iteration 150 : 1.3062126636505127
Loss at iteration 200 : 0.7504595518112183
Loss at iteration 250 : 0.537933349609375
Loss at iteration 300 : 0.3076689541339874
Loss at iteration 350 : 0.5280523300170898
Loss at iteration 400 : 0.687336802482605
Loss at iteration 450 : 1.4433026313781738
Mean training loss eporch  144 :  0.6059429889007093
Loss at iteration 50 : 0.9210118055343628
Loss at iteration 100 : 0.9934923648834229
Loss at iteration 150 : 0.39451009035110474
Loss at iteration 200 : 0.7545803785324097
Loss at iteration 250 : 0.5189865827560425
Loss at iteration 300 : 0.832557201385498
Loss at iteration 350 : 1.345597505569458
Loss at iteration 400 : 0.28216224908828735
Loss at iteration 450 : 0.5029743909835815
Mean training loss eporch  145 :  0.604839577179823
Loss at iteration 50 : 0.40840214490890503
Loss at iteration 100 : 0.46474868059158325
Loss at iteration 150 : 1.1794666051864624
Loss at iteration 200 : 0.6132251620292664
Loss at iteration 250 : 0.8944221138954163
Loss at iteration 300 : 0.6502973437309265
Loss at iteration 350 : 0.7615082263946533
Loss at iteration 400 : 0.3117593228816986
Loss at iteration 450 : 0.38887667655944824
Mean training loss eporch  146 :  0.6044438853236422
Loss at iteration 50 : 0.4199943542480469
Loss at iteration 100 : 0.4372592568397522
Loss at iteration 150 : 0.52804034948349
Loss at iteration 200 : 0.6041470766067505
Loss at iteration 250 : 0.475862592458725
Loss at iteration 300 : 0.6013656854629517
Loss at iteration 350 : 1.1042776107788086
Loss at iteration 400 : 0.7600563168525696
Loss at iteration 450 : 0.4194096326828003
Mean training loss eporch  147 :  0.6037090512997935
Loss at iteration 50 : 0.47101518511772156
Loss at iteration 100 : 0.8837000727653503
Loss at iteration 150 : 0.34575098752975464
Loss at iteration 200 : 0.2551057040691376
Loss at iteration 250 : 0.9480736255645752
Loss at iteration 300 : 0.2860558331012726
Loss at iteration 350 : 0.6536160707473755
Loss at iteration 400 : 0.8350971341133118
Loss at iteration 450 : 0.42165499925613403
Mean training loss eporch  148 :  0.6044499211667971
Loss at iteration 50 : 0.23550894856452942
Loss at iteration 100 : 0.37478938698768616
Loss at iteration 150 : 0.6888811588287354
Loss at iteration 200 : 0.5804311037063599
Loss at iteration 250 : 0.8977026343345642
Loss at iteration 300 : 0.7504305839538574
Loss at iteration 350 : 0.7253156900405884
Loss at iteration 400 : 0.29740238189697266
Loss at iteration 450 : 0.5749901533126831
Mean training loss eporch  149 :  0.6032810974320607
Loss at iteration 50 : 1.127474308013916
Loss at iteration 100 : 0.6054816842079163
Loss at iteration 150 : 0.8473160266876221
Loss at iteration 200 : 0.5468789339065552
Loss at iteration 250 : 0.7503409385681152
Loss at iteration 300 : 0.4587860703468323
Loss at iteration 350 : 0.729621171951294
Loss at iteration 400 : 0.7613446712493896
Loss at iteration 450 : 0.3369664251804352
Mean training loss eporch  150 :  0.6040027996049765
Loss at iteration 50 : 0.5646647214889526
Loss at iteration 100 : 0.6781151294708252
Loss at iteration 150 : 0.6268042325973511
Loss at iteration 200 : 0.6560530662536621
Loss at iteration 250 : 0.4650875926017761
Loss at iteration 300 : 0.9194985628128052
Loss at iteration 350 : 1.0989437103271484
Loss at iteration 400 : 0.3698002099990845
Loss at iteration 450 : 0.5727678537368774
Mean training loss eporch  151 :  0.6033374101359974
Loss at iteration 50 : 0.8250206708908081
Loss at iteration 100 : 0.40280091762542725
Loss at iteration 150 : 0.8685941100120544
Loss at iteration 200 : 0.7918637990951538
Loss at iteration 250 : 0.4606826901435852
Loss at iteration 300 : 0.7400440573692322
Loss at iteration 350 : 0.28727057576179504
Loss at iteration 400 : 0.5779644250869751
Loss at iteration 450 : 1.0513298511505127
Mean training loss eporch  152 :  0.6031578888017762
Loss at iteration 50 : 0.649235725402832
Loss at iteration 100 : 0.5390055775642395
Loss at iteration 150 : 0.3572627604007721
Loss at iteration 200 : 0.32045820355415344
Loss at iteration 250 : 0.565906286239624
Loss at iteration 300 : 0.4335419237613678
Loss at iteration 350 : 0.6114537119865417
Loss at iteration 400 : 0.5660699605941772
Loss at iteration 450 : 0.5656186938285828
Mean training loss eporch  153 :  0.6026723030097315
Loss at iteration 50 : 0.5580387115478516
Loss at iteration 100 : 0.4866250157356262
Loss at iteration 150 : 1.0232388973236084
Loss at iteration 200 : 0.6388940811157227
Loss at iteration 250 : 0.6527932286262512
Loss at iteration 300 : 0.555748462677002
Loss at iteration 350 : 1.1490378379821777
Loss at iteration 400 : 0.4794922471046448
Loss at iteration 450 : 0.4994761645793915
Mean training loss eporch  154 :  0.6025860919675567
Loss at iteration 50 : 0.6278950572013855
Loss at iteration 100 : 0.4471472203731537
Loss at iteration 150 : 0.41528087854385376
Loss at iteration 200 : 0.7626587748527527
Loss at iteration 250 : 0.4466380774974823
Loss at iteration 300 : 0.30959582328796387
Loss at iteration 350 : 0.8705880641937256
Loss at iteration 400 : 0.7160571217536926
Loss at iteration 450 : 0.418002724647522
Mean training loss eporch  155 :  0.6024164596424442
Loss at iteration 50 : 0.27075740694999695
Loss at iteration 100 : 0.7926464080810547
Loss at iteration 150 : 0.6288285851478577
Loss at iteration 200 : 0.4048992693424225
Loss at iteration 250 : 0.7751601338386536
Loss at iteration 300 : 0.8567372560501099
Loss at iteration 350 : 0.4155736565589905
Loss at iteration 400 : 0.6330076456069946
Loss at iteration 450 : 0.5023152232170105
Mean training loss eporch  156 :  0.6025677417486781
Loss at iteration 50 : 0.6632537841796875
Loss at iteration 100 : 0.8214651942253113
Loss at iteration 150 : 0.5246206521987915
Loss at iteration 200 : 0.3406493663787842
Loss at iteration 250 : 0.7310397624969482
Loss at iteration 300 : 0.4230274260044098
Loss at iteration 350 : 0.6811233758926392
Loss at iteration 400 : 0.6235203742980957
Loss at iteration 450 : 0.5971471071243286
Mean training loss eporch  157 :  0.6025927482040357
Loss at iteration 50 : 0.8482736349105835
Loss at iteration 100 : 0.728098452091217
Loss at iteration 150 : 0.5111945271492004
Loss at iteration 200 : 0.2440197765827179
Loss at iteration 250 : 0.7260110378265381
Loss at iteration 300 : 0.23387585580348969
Loss at iteration 350 : 0.49170249700546265
Loss at iteration 400 : 0.6529158353805542
Loss at iteration 450 : 0.4330189824104309
Mean training loss eporch  158 :  0.601535582922742
Loss at iteration 50 : 0.4145965576171875
Loss at iteration 100 : 0.7004799842834473
Loss at iteration 150 : 0.6039044857025146
Loss at iteration 200 : 0.24602779746055603
Loss at iteration 250 : 1.0501823425292969
Loss at iteration 300 : 0.5443370342254639
Loss at iteration 350 : 0.9254964590072632
Loss at iteration 400 : 0.4637961685657501
Loss at iteration 450 : 0.7615026235580444
Mean training loss eporch  159 :  0.6013239066284072
Loss at iteration 50 : 0.9253644943237305
Loss at iteration 100 : 1.0655250549316406
Loss at iteration 150 : 0.8245591521263123
Loss at iteration 200 : 0.5669610500335693
Loss at iteration 250 : 0.684224009513855
Loss at iteration 300 : 0.7654343247413635
Loss at iteration 350 : 0.7239502668380737
Loss at iteration 400 : 0.3687918186187744
Loss at iteration 450 : 0.5782521963119507
Mean training loss eporch  160 :  0.6007295238423048
Loss at iteration 50 : 1.3476513624191284
Loss at iteration 100 : 1.0471351146697998
Loss at iteration 150 : 1.0294950008392334
Loss at iteration 200 : 0.6212373971939087
Loss at iteration 250 : 1.161171793937683
Loss at iteration 300 : 1.209848403930664
Loss at iteration 350 : 0.33433228731155396
Loss at iteration 400 : 0.7066397666931152
Loss at iteration 450 : 0.5458046793937683
Mean training loss eporch  161 :  0.6009406902862393
Loss at iteration 50 : 0.489828884601593
Loss at iteration 100 : 0.7779802083969116
Loss at iteration 150 : 0.5197879076004028
Loss at iteration 200 : 0.43561476469039917
Loss at iteration 250 : 0.5178393125534058
Loss at iteration 300 : 0.6028487086296082
Loss at iteration 350 : 0.6736408472061157
Loss at iteration 400 : 0.4913199841976166
Loss at iteration 450 : 0.5595501661300659
Mean training loss eporch  162 :  0.6004986452819413
Loss at iteration 50 : 0.7662520408630371
Loss at iteration 100 : 0.5484747886657715
Loss at iteration 150 : 0.6783959865570068
Loss at iteration 200 : 0.6200550198554993
Loss at iteration 250 : 0.4021996855735779
Loss at iteration 300 : 0.7481719255447388
Loss at iteration 350 : 0.6391568183898926
Loss at iteration 400 : 0.2786942720413208
Loss at iteration 450 : 0.621813178062439
Mean training loss eporch  163 :  0.6001886637826844
Loss at iteration 50 : 0.46242618560791016
Loss at iteration 100 : 1.4199742078781128
Loss at iteration 150 : 0.43740415573120117
Loss at iteration 200 : 0.494934618473053
Loss at iteration 250 : 0.6462769508361816
Loss at iteration 300 : 0.4895995855331421
Loss at iteration 350 : 0.5391241312026978
Loss at iteration 400 : 0.45670145750045776
Loss at iteration 450 : 0.5017049908638
Mean training loss eporch  164 :  0.6001211063757103
Loss at iteration 50 : 0.7730082273483276
Loss at iteration 100 : 0.5402503609657288
Loss at iteration 150 : 0.8078972101211548
Loss at iteration 200 : 0.37632784247398376
Loss at iteration 250 : 0.9145897030830383
Loss at iteration 300 : 0.9299122095108032
Loss at iteration 350 : 0.8234412670135498
Loss at iteration 400 : 0.3515056371688843
Loss at iteration 450 : 0.4714337885379791
Mean training loss eporch  165 :  0.6000904227026337
Loss at iteration 50 : 0.4099242091178894
Loss at iteration 100 : 0.8803699612617493
Loss at iteration 150 : 0.28460708260536194
Loss at iteration 200 : 0.4970207214355469
Loss at iteration 250 : 1.1071441173553467
Loss at iteration 300 : 0.32901373505592346
Loss at iteration 350 : 0.43499964475631714
Loss at iteration 400 : 0.6079845428466797
Loss at iteration 450 : 0.43633630871772766
Mean training loss eporch  166 :  0.5994545286170608
Loss at iteration 50 : 0.5276650190353394
Loss at iteration 100 : 0.3957709074020386
Loss at iteration 150 : 0.521237850189209
Loss at iteration 200 : 0.8439419269561768
Loss at iteration 250 : 0.3926749527454376
Loss at iteration 300 : 0.5226084589958191
Loss at iteration 350 : 1.1057392358779907
Loss at iteration 400 : 0.37258774042129517
Loss at iteration 450 : 0.3964305520057678
Mean training loss eporch  167 :  0.5995211556055057
Loss at iteration 50 : 0.3091241121292114
Loss at iteration 100 : 0.4868900775909424
Loss at iteration 150 : 0.37700116634368896
Loss at iteration 200 : 0.7461724281311035
Loss at iteration 250 : 0.3508850634098053
Loss at iteration 300 : 0.37124764919281006
Loss at iteration 350 : 0.8247161507606506
Loss at iteration 400 : 0.4339757561683655
Loss at iteration 450 : 0.716400146484375
Mean training loss eporch  168 :  0.6002415252467579
Loss at iteration 50 : 0.6663801074028015
Loss at iteration 100 : 0.6907280683517456
Loss at iteration 150 : 0.6930567622184753
Loss at iteration 200 : 0.38872629404067993
Loss at iteration 250 : 1.2989170551300049
Loss at iteration 300 : 0.4447101652622223
Loss at iteration 350 : 0.45552659034729004
Loss at iteration 400 : 0.4079686403274536
Loss at iteration 450 : 0.3841051459312439
Mean training loss eporch  169 :  0.5993786251569891
Loss at iteration 50 : 0.5996723175048828
Loss at iteration 100 : 0.6144813299179077
Loss at iteration 150 : 0.5463927984237671
Loss at iteration 200 : 0.410797119140625
Loss at iteration 250 : 0.6220359802246094
Loss at iteration 300 : 0.41958752274513245
Loss at iteration 350 : 0.20430363714694977
Loss at iteration 400 : 0.49497437477111816
Loss at iteration 450 : 0.6635801196098328
Mean training loss eporch  170 :  0.5995762863169155
Loss at iteration 50 : 0.31702107191085815
Loss at iteration 100 : 0.5396744608879089
Loss at iteration 150 : 0.6096954941749573
Loss at iteration 200 : 0.7089160084724426
Loss at iteration 250 : 0.4501529037952423
Loss at iteration 300 : 0.5434569120407104
Loss at iteration 350 : 0.5898526906967163
Loss at iteration 400 : 0.6239234209060669
Loss at iteration 450 : 0.3935495913028717
Mean training loss eporch  171 :  0.5998087831184455
Loss at iteration 50 : 0.6067770719528198
Loss at iteration 100 : 0.6233633756637573
Loss at iteration 150 : 0.6057665348052979
Loss at iteration 200 : 0.5318289995193481
Loss at iteration 250 : 0.31262773275375366
Loss at iteration 300 : 0.3590090274810791
Loss at iteration 350 : 0.612415611743927
Loss at iteration 400 : 0.6507893800735474
Loss at iteration 450 : 0.458467960357666
Mean training loss eporch  172 :  0.598960323157919
Loss at iteration 50 : 0.6901265382766724
Loss at iteration 100 : 0.2764870822429657
Loss at iteration 150 : 0.44185179471969604
Loss at iteration 200 : 0.8189610242843628
Loss at iteration 250 : 0.46372970938682556
Loss at iteration 300 : 0.6098495721817017
Loss at iteration 350 : 0.3815837800502777
Loss at iteration 400 : 0.9828054904937744
Loss at iteration 450 : 0.3883989155292511
Mean training loss eporch  173 :  0.5985877605225252
Loss at iteration 50 : 0.6861741542816162
Loss at iteration 100 : 0.4237445592880249
Loss at iteration 150 : 0.8271880149841309
Loss at iteration 200 : 0.33493733406066895
Loss at iteration 250 : 0.42592522501945496
Loss at iteration 300 : 0.9668017625808716
Loss at iteration 350 : 0.4439317584037781
Loss at iteration 400 : 1.0679314136505127
Loss at iteration 450 : 0.5694047808647156
Mean training loss eporch  174 :  0.5983747843045071
Loss at iteration 50 : 0.7574584484100342
Loss at iteration 100 : 0.28043264150619507
Loss at iteration 150 : 0.3818221092224121
Loss at iteration 200 : 0.39485716819763184
Loss at iteration 250 : 0.5477614402770996
Loss at iteration 300 : 0.730973482131958
Loss at iteration 350 : 0.5900278687477112
Loss at iteration 400 : 0.26348915696144104
Loss at iteration 450 : 0.6156569719314575
Mean training loss eporch  175 :  0.5990551943310135
Loss at iteration 50 : 0.6591904163360596
Loss at iteration 100 : 0.41876137256622314
Loss at iteration 150 : 0.534658670425415
Loss at iteration 200 : 0.5435037612915039
Loss at iteration 250 : 0.8394760489463806
Loss at iteration 300 : 0.21349066495895386
Loss at iteration 350 : 0.4183691740036011
Loss at iteration 400 : 0.402921199798584
Loss at iteration 450 : 0.9329038858413696
Mean training loss eporch  176 :  0.5981051201102126
Loss at iteration 50 : 0.9590795636177063
Loss at iteration 100 : 0.3772673010826111
Loss at iteration 150 : 0.42893606424331665
Loss at iteration 200 : 0.5861831903457642
Loss at iteration 250 : 0.6086462736129761
Loss at iteration 300 : 0.4846217930316925
Loss at iteration 350 : 0.5158849358558655
Loss at iteration 400 : 0.704549252986908
Loss at iteration 450 : 0.5669089555740356
Mean training loss eporch  177 :  0.597978628205455
Loss at iteration 50 : 0.484305739402771
Loss at iteration 100 : 0.6310257911682129
Loss at iteration 150 : 0.30522361397743225
Loss at iteration 200 : 1.1364952325820923
Loss at iteration 250 : 0.837378978729248
Loss at iteration 300 : 0.5116134881973267
Loss at iteration 350 : 0.6793175339698792
Loss at iteration 400 : 0.7842705249786377
Loss at iteration 450 : 0.6593500375747681
Mean training loss eporch  178 :  0.5976706486678522
Loss at iteration 50 : 0.7470503449440002
Loss at iteration 100 : 0.6515405178070068
Loss at iteration 150 : 0.2412998080253601
Loss at iteration 200 : 0.5038913488388062
Loss at iteration 250 : 0.9541594386100769
Loss at iteration 300 : 0.4604608416557312
Loss at iteration 350 : 0.6002308130264282
Loss at iteration 400 : 0.5448841452598572
Loss at iteration 450 : 0.6418182253837585
Mean training loss eporch  179 :  0.5981027396029508
Loss at iteration 50 : 0.7028056979179382
Loss at iteration 100 : 0.4386185109615326
Loss at iteration 150 : 0.5644654631614685
Loss at iteration 200 : 0.6821083426475525
Loss at iteration 250 : 0.6945517659187317
Loss at iteration 300 : 0.32277432084083557
Loss at iteration 350 : 0.47887706756591797
Loss at iteration 400 : 0.8380378484725952
Loss at iteration 450 : 0.2510518431663513
Mean training loss eporch  180 :  0.5983417644847387
Loss at iteration 50 : 1.125025987625122
Loss at iteration 100 : 0.48878923058509827
Loss at iteration 150 : 0.5625040531158447
Loss at iteration 200 : 0.48543646931648254
Loss at iteration 250 : 0.48018181324005127
Loss at iteration 300 : 0.6513893008232117
Loss at iteration 350 : 1.0028083324432373
Loss at iteration 400 : 0.3982468843460083
Loss at iteration 450 : 0.9243225455284119
Mean training loss eporch  181 :  0.5974278933832337
Loss at iteration 50 : 0.7548015117645264
Loss at iteration 100 : 0.5848485231399536
Loss at iteration 150 : 0.40344369411468506
Loss at iteration 200 : 1.0722088813781738
Loss at iteration 250 : 0.5592835545539856
Loss at iteration 300 : 0.8857945203781128
Loss at iteration 350 : 0.36387208104133606
Loss at iteration 400 : 0.741096019744873
Loss at iteration 450 : 0.923287570476532
Mean training loss eporch  182 :  0.5981791047496277
Loss at iteration 50 : 0.8970744609832764
Loss at iteration 100 : 0.6358400583267212
Loss at iteration 150 : 0.6545097827911377
Loss at iteration 200 : 0.7984892129898071
Loss at iteration 250 : 0.4480797052383423
Loss at iteration 300 : 0.5503365397453308
Loss at iteration 350 : 0.4867694079875946
Loss at iteration 400 : 0.4296378493309021
Loss at iteration 450 : 0.5661741495132446
Mean training loss eporch  183 :  0.5972541902354571
Loss at iteration 50 : 0.3112754225730896
Loss at iteration 100 : 0.24558044970035553
Loss at iteration 150 : 0.7812435626983643
Loss at iteration 200 : 0.5821212530136108
Loss at iteration 250 : 0.6781646609306335
Loss at iteration 300 : 0.4980657696723938
Loss at iteration 350 : 0.2678769826889038
Loss at iteration 400 : 0.4368012547492981
Loss at iteration 450 : 0.37279170751571655
Mean training loss eporch  184 :  0.5982797173900085
Loss at iteration 50 : 0.3651866614818573
Loss at iteration 100 : 0.4477454423904419
Loss at iteration 150 : 1.0371209383010864
Loss at iteration 200 : 0.6729565858840942
Loss at iteration 250 : 1.0498228073120117
Loss at iteration 300 : 1.1296601295471191
Loss at iteration 350 : 0.549945056438446
Loss at iteration 400 : 0.7993322610855103
Loss at iteration 450 : 0.3646564185619354
Mean training loss eporch  185 :  0.597142495433157
Loss at iteration 50 : 0.3941417932510376
Loss at iteration 100 : 0.7949626445770264
Loss at iteration 150 : 0.4437090754508972
Loss at iteration 200 : 0.4297190010547638
Loss at iteration 250 : 0.5621013045310974
Loss at iteration 300 : 0.39789438247680664
Loss at iteration 350 : 0.44126516580581665
Loss at iteration 400 : 0.6620665192604065
Loss at iteration 450 : 0.8954718112945557
Mean training loss eporch  186 :  0.5975996972876114
Loss at iteration 50 : 0.7154738903045654
Loss at iteration 100 : 0.36361366510391235
Loss at iteration 150 : 0.49415528774261475
Loss at iteration 200 : 1.0560553073883057
Loss at iteration 250 : 0.5038769245147705
Loss at iteration 300 : 0.5412483811378479
Loss at iteration 350 : 0.39364689588546753
Loss at iteration 400 : 0.5156534910202026
Loss at iteration 450 : 1.0801632404327393
Mean training loss eporch  187 :  0.5967073992331158
Loss at iteration 50 : 0.8698157072067261
Loss at iteration 100 : 0.5079554319381714
Loss at iteration 150 : 0.6029460430145264
Loss at iteration 200 : 0.7037826180458069
Loss at iteration 250 : 0.2861707806587219
Loss at iteration 300 : 0.6817470192909241
Loss at iteration 350 : 0.5236845016479492
Loss at iteration 400 : 0.5815721750259399
Loss at iteration 450 : 0.8558394908905029
Mean training loss eporch  188 :  0.5966914796804285
Loss at iteration 50 : 0.3134153485298157
Loss at iteration 100 : 0.740311861038208
Loss at iteration 150 : 0.5591996908187866
Loss at iteration 200 : 0.7875732183456421
Loss at iteration 250 : 0.4584687054157257
Loss at iteration 300 : 0.5950145721435547
Loss at iteration 350 : 0.7104300260543823
Loss at iteration 400 : 0.7327296137809753
Loss at iteration 450 : 0.6309723854064941
Mean training loss eporch  189 :  0.596838926046962
Loss at iteration 50 : 1.0147488117218018
Loss at iteration 100 : 0.9381507635116577
Loss at iteration 150 : 0.7354767322540283
Loss at iteration 200 : 0.5293833017349243
Loss at iteration 250 : 0.3980564773082733
Loss at iteration 300 : 0.8066434860229492
Loss at iteration 350 : 0.7870600819587708
Loss at iteration 400 : 0.7270203828811646
Loss at iteration 450 : 0.346981406211853
Mean training loss eporch  190 :  0.5971187082544032
Loss at iteration 50 : 0.854433000087738
Loss at iteration 100 : 0.860426127910614
Loss at iteration 150 : 0.9448527097702026
Loss at iteration 200 : 0.5771255493164062
Loss at iteration 250 : 0.9698107242584229
Loss at iteration 300 : 0.7308653593063354
Loss at iteration 350 : 0.4426990747451782
Loss at iteration 400 : 0.5109415054321289
Loss at iteration 450 : 0.46446114778518677
Mean training loss eporch  191 :  0.5962680199156246
Loss at iteration 50 : 0.9014259576797485
Loss at iteration 100 : 0.20591604709625244
Loss at iteration 150 : 0.7604097127914429
Loss at iteration 200 : 0.4079861044883728
Loss at iteration 250 : 0.44853562116622925
Loss at iteration 300 : 0.3829312324523926
Loss at iteration 350 : 0.5071276426315308
Loss at iteration 400 : 1.22420072555542
Loss at iteration 450 : 0.5255092978477478
Mean training loss eporch  192 :  0.5962684387691849
Loss at iteration 50 : 0.391379714012146
Loss at iteration 100 : 0.8537681102752686
Loss at iteration 150 : 0.4476112425327301
Loss at iteration 200 : 0.4831787347793579
Loss at iteration 250 : 0.5501745939254761
Loss at iteration 300 : 0.4788293242454529
Loss at iteration 350 : 0.3831157386302948
Loss at iteration 400 : 0.7586244344711304
Loss at iteration 450 : 1.1196086406707764
Mean training loss eporch  193 :  0.5966200310440741
Loss at iteration 50 : 0.6867942214012146
Loss at iteration 100 : 0.45037949085235596
Loss at iteration 150 : 0.3693560063838959
Loss at iteration 200 : 0.502021312713623
Loss at iteration 250 : 0.3751990497112274
Loss at iteration 300 : 0.48441335558891296
Loss at iteration 350 : 0.5621140599250793
Loss at iteration 400 : 0.5592763423919678
Loss at iteration 450 : 0.4061436951160431
Mean training loss eporch  194 :  0.5963953273316308
Loss at iteration 50 : 0.8086389899253845
Loss at iteration 100 : 0.8068585395812988
Loss at iteration 150 : 0.46217167377471924
Loss at iteration 200 : 1.1324069499969482
Loss at iteration 250 : 0.5652551651000977
Loss at iteration 300 : 1.412440538406372
Loss at iteration 350 : 1.0225541591644287
Loss at iteration 400 : 0.5639170408248901
Loss at iteration 450 : 0.7413958311080933
Mean training loss eporch  195 :  0.5968131576684229
Loss at iteration 50 : 0.43947070837020874
Loss at iteration 100 : 0.7237123847007751
Loss at iteration 150 : 0.9324789047241211
Loss at iteration 200 : 0.5823017954826355
Loss at iteration 250 : 0.45089057087898254
Loss at iteration 300 : 1.0205806493759155
Loss at iteration 350 : 0.4135114550590515
Loss at iteration 400 : 0.7868176698684692
Loss at iteration 450 : 0.571090817451477
Mean training loss eporch  196 :  0.5961366962002411
Loss at iteration 50 : 0.9138154983520508
Loss at iteration 100 : 0.33920592069625854
Loss at iteration 150 : 0.9311593174934387
Loss at iteration 200 : 0.7197690606117249
Loss at iteration 250 : 0.4327382445335388
Loss at iteration 300 : 0.5234758853912354
Loss at iteration 350 : 1.0233502388000488
Loss at iteration 400 : 0.9646196961402893
Loss at iteration 450 : 0.6329953670501709
Mean training loss eporch  197 :  0.5961341179876148
Loss at iteration 50 : 0.3393864929676056
Loss at iteration 100 : 0.8399375677108765
Loss at iteration 150 : 0.33741840720176697
Loss at iteration 200 : 0.4348216652870178
Loss at iteration 250 : 0.687899112701416
Loss at iteration 300 : 0.4942430853843689
Loss at iteration 350 : 0.6704054474830627
Loss at iteration 400 : 0.6183319091796875
Loss at iteration 450 : 0.6855382323265076
Mean training loss eporch  198 :  0.5960428964144017
Loss at iteration 50 : 0.7668521404266357
Loss at iteration 100 : 0.7579598426818848
Loss at iteration 150 : 0.41666001081466675
Loss at iteration 200 : 0.5332356691360474
Loss at iteration 250 : 0.5423054695129395
Loss at iteration 300 : 0.6088204383850098
Loss at iteration 350 : 0.9130192399024963
Loss at iteration 400 : 0.5548456311225891
Loss at iteration 450 : 0.7636313438415527
Mean training loss eporch  199 :  0.5953121346783937
Loss at iteration 50 : 0.44679173827171326
Loss at iteration 100 : 0.4605666399002075
Loss at iteration 150 : 1.278911828994751
Loss at iteration 200 : 0.6990041136741638
Loss at iteration 250 : 0.6683940887451172
Loss at iteration 300 : 0.39094099402427673
Loss at iteration 350 : 0.7479252815246582
Loss at iteration 400 : 0.6899462938308716
Loss at iteration 450 : 0.5545487999916077
Mean training loss eporch  200 :  0.5954887275603525
Loss at iteration 50 : 0.6975674629211426
Loss at iteration 100 : 0.4168180525302887
Loss at iteration 150 : 0.5091150999069214
Loss at iteration 200 : 0.7748743891716003
Loss at iteration 250 : 0.6024697422981262
Loss at iteration 300 : 0.6126914620399475
Loss at iteration 350 : 0.4174978733062744
Loss at iteration 400 : 0.6026643514633179
Loss at iteration 450 : 0.6418114900588989
Mean training loss eporch  201 :  0.5954290272114666
Loss at iteration 50 : 0.6676813364028931
Loss at iteration 100 : 0.6912667155265808
Loss at iteration 150 : 0.8069709539413452
Loss at iteration 200 : 0.3952079117298126
Loss at iteration 250 : 0.6214079260826111
Loss at iteration 300 : 0.550787627696991
Loss at iteration 350 : 0.3040216565132141
Loss at iteration 400 : 0.3345550000667572
Loss at iteration 450 : 0.8241314888000488
Mean training loss eporch  202 :  0.5953229023260551
Loss at iteration 50 : 0.4976382255554199
Loss at iteration 100 : 0.5872008204460144
Loss at iteration 150 : 0.9041881561279297
Loss at iteration 200 : 0.5443906784057617
Loss at iteration 250 : 0.4254748225212097
Loss at iteration 300 : 0.5530452728271484
Loss at iteration 350 : 0.47506219148635864
Loss at iteration 400 : 0.36295706033706665
Loss at iteration 450 : 0.5195605754852295
Mean training loss eporch  203 :  0.5958928602696961
Loss at iteration 50 : 0.6111659407615662
Loss at iteration 100 : 0.6343990564346313
Loss at iteration 150 : 0.5903505682945251
Loss at iteration 200 : 0.5289520621299744
Loss at iteration 250 : 0.29205790162086487
Loss at iteration 300 : 0.3308318853378296
Loss at iteration 350 : 0.4877256751060486
Loss at iteration 400 : 0.3409371078014374
Loss at iteration 450 : 0.5094048976898193
Mean training loss eporch  204 :  0.5957127464116867
Loss at iteration 50 : 0.4771408438682556
Loss at iteration 100 : 0.5353596210479736
Loss at iteration 150 : 0.5431227684020996
Loss at iteration 200 : 0.8535398840904236
Loss at iteration 250 : 0.4261132478713989
Loss at iteration 300 : 0.52835613489151
Loss at iteration 350 : 0.5036089420318604
Loss at iteration 400 : 0.5042679905891418
Loss at iteration 450 : 0.467873752117157
Mean training loss eporch  205 :  0.5948843837220799
Loss at iteration 50 : 0.4148162603378296
Loss at iteration 100 : 0.33688071370124817
Loss at iteration 150 : 1.030200481414795
Loss at iteration 200 : 0.6963411569595337
Loss at iteration 250 : 0.5186497569084167
Loss at iteration 300 : 0.20265068113803864
Loss at iteration 350 : 0.40096408128738403
Loss at iteration 400 : 0.6776543855667114
Loss at iteration 450 : 0.5932339429855347
Mean training loss eporch  206 :  0.5945155325980864
Loss at iteration 50 : 0.7317951917648315
Loss at iteration 100 : 1.1566014289855957
Loss at iteration 150 : 0.7517762184143066
Loss at iteration 200 : 0.9150583744049072
Loss at iteration 250 : 0.8968039751052856
Loss at iteration 300 : 0.7707746028900146
Loss at iteration 350 : 0.9775087833404541
Loss at iteration 400 : 0.3952363431453705
Loss at iteration 450 : 0.47628283500671387
Mean training loss eporch  207 :  0.5948285292444369
Loss at iteration 50 : 0.4963821768760681
Loss at iteration 100 : 0.5125675201416016
Loss at iteration 150 : 0.922335147857666
Loss at iteration 200 : 0.40155917406082153
Loss at iteration 250 : 0.28766384720802307
Loss at iteration 300 : 0.43132591247558594
Loss at iteration 350 : 0.6273754835128784
Loss at iteration 400 : 0.5955692529678345
Loss at iteration 450 : 0.787243127822876
Mean training loss eporch  208 :  0.5944401999958888
Loss at iteration 50 : 0.731108546257019
Loss at iteration 100 : 0.5804394483566284
Loss at iteration 150 : 0.399188756942749
Loss at iteration 200 : 0.24887162446975708
Loss at iteration 250 : 0.5612972378730774
Loss at iteration 300 : 1.1600512266159058
Loss at iteration 350 : 0.4304271340370178
Loss at iteration 400 : 0.6185699105262756
Loss at iteration 450 : 0.6197901964187622
Mean training loss eporch  209 :  0.5947396346215923
Loss at iteration 50 : 0.6584238409996033
Loss at iteration 100 : 0.35568374395370483
Loss at iteration 150 : 0.3337980806827545
Loss at iteration 200 : 0.5286537408828735
Loss at iteration 250 : 0.37023213505744934
Loss at iteration 300 : 0.5907413959503174
Loss at iteration 350 : 0.5300544500350952
Loss at iteration 400 : 0.49408748745918274
Loss at iteration 450 : 0.4474598467350006
Mean training loss eporch  210 :  0.5943667211230829
Loss at iteration 50 : 0.9149696826934814
Loss at iteration 100 : 0.8582754731178284
Loss at iteration 150 : 0.5592377185821533
Loss at iteration 200 : 0.5393441915512085
Loss at iteration 250 : 0.5306813716888428
Loss at iteration 300 : 0.38191962242126465
Loss at iteration 350 : 0.7322602868080139
Loss at iteration 400 : 0.5650684833526611
Loss at iteration 450 : 0.43098482489585876
Mean training loss eporch  211 :  0.5946131024712299
Loss at iteration 50 : 0.9908181428909302
Loss at iteration 100 : 0.42226487398147583
Loss at iteration 150 : 0.9611641764640808
Loss at iteration 200 : 0.5921928882598877
Loss at iteration 250 : 0.588294267654419
Loss at iteration 300 : 0.8951776027679443
Loss at iteration 350 : 0.7579361796379089
Loss at iteration 400 : 0.6953883767127991
Loss at iteration 450 : 0.38790300488471985
Mean training loss eporch  212 :  0.5943117493366098
Loss at iteration 50 : 0.5567847490310669
Loss at iteration 100 : 0.36074793338775635
Loss at iteration 150 : 0.6308140754699707
Loss at iteration 200 : 0.2730468809604645
Loss at iteration 250 : 0.8160884976387024
Loss at iteration 300 : 0.35430753231048584
Loss at iteration 350 : 0.4978901743888855
Loss at iteration 400 : 0.28852254152297974
Loss at iteration 450 : 0.690854549407959
Mean training loss eporch  213 :  0.5939204764540724
Loss at iteration 50 : 0.5158523321151733
Loss at iteration 100 : 0.36915019154548645
Loss at iteration 150 : 0.8210471868515015
Loss at iteration 200 : 0.21174269914627075
Loss at iteration 250 : 0.4244306683540344
Loss at iteration 300 : 0.623155951499939
Loss at iteration 350 : 0.3707011938095093
Loss at iteration 400 : 0.3967711329460144
Loss at iteration 450 : 0.5683354139328003
Mean training loss eporch  214 :  0.5940452831559601
Loss at iteration 50 : 0.8561626672744751
Loss at iteration 100 : 0.7899425029754639
Loss at iteration 150 : 0.7890492081642151
Loss at iteration 200 : 0.5025335550308228
Loss at iteration 250 : 0.6942905783653259
Loss at iteration 300 : 0.3897111117839813
Loss at iteration 350 : 0.7821685075759888
Loss at iteration 400 : 0.8967591524124146
Loss at iteration 450 : 0.7805635929107666
Mean training loss eporch  215 :  0.594432859646476
Loss at iteration 50 : 0.44075658917427063
Loss at iteration 100 : 0.46801406145095825
Loss at iteration 150 : 0.3179425299167633
Loss at iteration 200 : 0.36099594831466675
Loss at iteration 250 : 0.5980902314186096
Loss at iteration 300 : 0.4559735357761383
Loss at iteration 350 : 0.7285326719284058
Loss at iteration 400 : 0.5861055850982666
Loss at iteration 450 : 0.7929746508598328
Mean training loss eporch  216 :  0.5934530044262878
Loss at iteration 50 : 0.4830875098705292
Loss at iteration 100 : 0.21522007882595062
Loss at iteration 150 : 0.590216875076294
Loss at iteration 200 : 0.434465229511261
Loss at iteration 250 : 0.671504020690918
Loss at iteration 300 : 0.6125640869140625
Loss at iteration 350 : 0.5316897034645081
Loss at iteration 400 : 0.4860818386077881
Loss at iteration 450 : 0.3406405448913574
Mean training loss eporch  217 :  0.5936163477132012
Loss at iteration 50 : 1.02011239528656
Loss at iteration 100 : 0.38266438245773315
Loss at iteration 150 : 0.2800193428993225
Loss at iteration 200 : 0.6803175210952759
Loss at iteration 250 : 0.9760152101516724
Loss at iteration 300 : 0.5348278880119324
Loss at iteration 350 : 0.5163443088531494
Loss at iteration 400 : 0.5911850929260254
Loss at iteration 450 : 0.43925192952156067
Mean training loss eporch  218 :  0.5931754803058992
Loss at iteration 50 : 0.3483622074127197
Loss at iteration 100 : 1.0629847049713135
Loss at iteration 150 : 0.9265823364257812
Loss at iteration 200 : 0.6084187030792236
Loss at iteration 250 : 0.6233735084533691
Loss at iteration 300 : 0.4081270694732666
Loss at iteration 350 : 0.5630529522895813
Loss at iteration 400 : 0.6400405168533325
Loss at iteration 450 : 0.8286089897155762
Mean training loss eporch  219 :  0.5935834539609973
Loss at iteration 50 : 0.5388261079788208
Loss at iteration 100 : 0.3085968494415283
Loss at iteration 150 : 0.39587193727493286
Loss at iteration 200 : 0.4085589647293091
Loss at iteration 250 : 0.4064652919769287
Loss at iteration 300 : 0.33190491795539856
Loss at iteration 350 : 0.3091977834701538
Loss at iteration 400 : 0.31322431564331055
Loss at iteration 450 : 0.5098233222961426
Mean training loss eporch  220 :  0.5935628205600144
Loss at iteration 50 : 0.541841983795166
Loss at iteration 100 : 0.6709400415420532
Loss at iteration 150 : 0.6493328809738159
Loss at iteration 200 : 0.7794501781463623
Loss at iteration 250 : 0.4127103388309479
Loss at iteration 300 : 0.57044517993927
Loss at iteration 350 : 0.7214641571044922
Loss at iteration 400 : 0.9492467641830444
Loss at iteration 450 : 0.2174292951822281
Mean training loss eporch  221 :  0.5929831348079019
Loss at iteration 50 : 0.6138802170753479
Loss at iteration 100 : 0.49332618713378906
Loss at iteration 150 : 0.6741307973861694
Loss at iteration 200 : 0.3546256721019745
Loss at iteration 250 : 0.5581955313682556
Loss at iteration 300 : 0.789177656173706
Loss at iteration 350 : 1.4611765146255493
Loss at iteration 400 : 0.5340184569358826
Loss at iteration 450 : 0.3927896022796631
Mean training loss eporch  222 :  0.593673954854201
Loss at iteration 50 : 0.29062074422836304
Loss at iteration 100 : 0.44504284858703613
Loss at iteration 150 : 0.21635067462921143
Loss at iteration 200 : 0.4718002378940582
Loss at iteration 250 : 0.21729904413223267
Loss at iteration 300 : 0.38478559255599976
Loss at iteration 350 : 0.7597278356552124
Loss at iteration 400 : 0.2974337339401245
Loss at iteration 450 : 0.7855431437492371
Mean training loss eporch  223 :  0.5920485175491377
Loss at iteration 50 : 0.34783798456192017
Loss at iteration 100 : 0.7570385932922363
Loss at iteration 150 : 0.5178176164627075
Loss at iteration 200 : 0.8135820627212524
Loss at iteration 250 : 0.8007041215896606
Loss at iteration 300 : 0.8590831756591797
Loss at iteration 350 : 0.5643196105957031
Loss at iteration 400 : 0.428545206785202
Loss at iteration 450 : 0.7389166355133057
Mean training loss eporch  224 :  0.5927785420517543
Loss at iteration 50 : 0.6173750758171082
Loss at iteration 100 : 0.6737767457962036
Loss at iteration 150 : 0.7569960951805115
Loss at iteration 200 : 0.4337642192840576
Loss at iteration 250 : 0.7564113736152649
Loss at iteration 300 : 0.846580445766449
Loss at iteration 350 : 0.40578609704971313
Loss at iteration 400 : 0.5756072998046875
Loss at iteration 450 : 0.6517661809921265
Mean training loss eporch  225 :  0.5924271485109708
Loss at iteration 50 : 0.4627174437046051
Loss at iteration 100 : 0.47398841381073
Loss at iteration 150 : 0.39434120059013367
Loss at iteration 200 : 0.4748818278312683
Loss at iteration 250 : 0.6466900110244751
Loss at iteration 300 : 0.8185446262359619
Loss at iteration 350 : 0.30129197239875793
Loss at iteration 400 : 0.5876403450965881
Loss at iteration 450 : 0.25529563426971436
Mean training loss eporch  226 :  0.5919255428172056
Loss at iteration 50 : 0.3874915838241577
Loss at iteration 100 : 0.437735915184021
Loss at iteration 150 : 0.7033878564834595
Loss at iteration 200 : 0.7495867013931274
Loss at iteration 250 : 0.6568174958229065
Loss at iteration 300 : 0.5926127433776855
Loss at iteration 350 : 0.715968668460846
Loss at iteration 400 : 0.6183892488479614
Loss at iteration 450 : 0.9809392094612122
Mean training loss eporch  227 :  0.5920035836586892
Loss at iteration 50 : 0.7905662059783936
Loss at iteration 100 : 0.7179193496704102
Loss at iteration 150 : 0.7793645858764648
Loss at iteration 200 : 1.0228910446166992
Loss at iteration 250 : 0.364682137966156
Loss at iteration 300 : 1.136973261833191
Loss at iteration 350 : 0.3964063823223114
Loss at iteration 400 : 0.34484487771987915
Loss at iteration 450 : 0.9369913339614868
Mean training loss eporch  228 :  0.5919764814080055
Loss at iteration 50 : 1.0150105953216553
Loss at iteration 100 : 0.3895263075828552
Loss at iteration 150 : 0.3923899531364441
Loss at iteration 200 : 0.4676849842071533
Loss at iteration 250 : 0.5907500982284546
Loss at iteration 300 : 0.4846498370170593
Loss at iteration 350 : 0.6607469320297241
Loss at iteration 400 : 0.4037131369113922
Loss at iteration 450 : 0.4401590824127197
Mean training loss eporch  229 :  0.5916880547501552
Loss at iteration 50 : 0.34139370918273926
Loss at iteration 100 : 0.6352055668830872
Loss at iteration 150 : 0.9381994009017944
Loss at iteration 200 : 0.3867698311805725
Loss at iteration 250 : 0.5710093975067139
Loss at iteration 300 : 0.641492486000061
Loss at iteration 350 : 0.7908662557601929
Loss at iteration 400 : 0.3567432165145874
Loss at iteration 450 : 0.4654295742511749
Mean training loss eporch  230 :  0.5916274905080077
Loss at iteration 50 : 0.6885497570037842
Loss at iteration 100 : 1.1235312223434448
Loss at iteration 150 : 0.6066576838493347
Loss at iteration 200 : 0.7433568835258484
Loss at iteration 250 : 0.35087403655052185
Loss at iteration 300 : 0.7225351929664612
Loss at iteration 350 : 0.6441220641136169
Loss at iteration 400 : 0.4506910443305969
Loss at iteration 450 : 0.6992440223693848
Mean training loss eporch  231 :  0.5919887484616315
Loss at iteration 50 : 0.512434720993042
Loss at iteration 100 : 0.22088688611984253
Loss at iteration 150 : 0.2880561351776123
Loss at iteration 200 : 0.8656207323074341
Loss at iteration 250 : 0.7943053245544434
Loss at iteration 300 : 0.5875148773193359
Loss at iteration 350 : 0.5689026117324829
Loss at iteration 400 : 0.5134660005569458
Loss at iteration 450 : 0.38953226804733276
Mean training loss eporch  232 :  0.5909789640097938
Loss at iteration 50 : 0.317782461643219
Loss at iteration 100 : 0.5205612182617188
Loss at iteration 150 : 0.47734999656677246
Loss at iteration 200 : 0.4192769527435303
Loss at iteration 250 : 0.5308427214622498
Loss at iteration 300 : 0.41051942110061646
Loss at iteration 350 : 0.657177746295929
Loss at iteration 400 : 0.7437888383865356
Loss at iteration 450 : 0.7434341311454773
Mean training loss eporch  233 :  0.5914955456822986
Loss at iteration 50 : 0.4708713889122009
Loss at iteration 100 : 0.5415692925453186
Loss at iteration 150 : 0.4186021685600281
Loss at iteration 200 : 0.7866837978363037
Loss at iteration 250 : 0.3998279869556427
Loss at iteration 300 : 0.21320629119873047
Loss at iteration 350 : 0.3418564200401306
Loss at iteration 400 : 0.8993785381317139
Loss at iteration 450 : 0.6839580535888672
Mean training loss eporch  234 :  0.591026941509177
Loss at iteration 50 : 0.36631035804748535
Loss at iteration 100 : 0.4508923292160034
Loss at iteration 150 : 0.7764890789985657
Loss at iteration 200 : 0.34183114767074585
Loss at iteration 250 : 0.39248931407928467
Loss at iteration 300 : 0.5250104665756226
Loss at iteration 350 : 0.6075845956802368
Loss at iteration 400 : 0.5198420286178589
Loss at iteration 450 : 0.4447953701019287
Mean training loss eporch  235 :  0.5913046234634132
Loss at iteration 50 : 0.3466496169567108
Loss at iteration 100 : 0.5086370706558228
Loss at iteration 150 : 0.323314905166626
Loss at iteration 200 : 0.37202590703964233
Loss at iteration 250 : 0.6057444214820862
Loss at iteration 300 : 0.7858107089996338
Loss at iteration 350 : 0.45727473497390747
Loss at iteration 400 : 0.576735258102417
Loss at iteration 450 : 0.3937991261482239
Mean training loss eporch  236 :  0.5916953342791381
Loss at iteration 50 : 0.4797091782093048
Loss at iteration 100 : 0.5157352089881897
Loss at iteration 150 : 0.528369128704071
Loss at iteration 200 : 1.413956642150879
Loss at iteration 250 : 0.33990970253944397
Loss at iteration 300 : 0.5204913020133972
Loss at iteration 350 : 0.3726034164428711
Loss at iteration 400 : 0.42994317412376404
Loss at iteration 450 : 0.4309203326702118
Mean training loss eporch  237 :  0.5913022281485123
Loss at iteration 50 : 0.5436025857925415
Loss at iteration 100 : 0.5361064672470093
Loss at iteration 150 : 0.44959840178489685
Loss at iteration 200 : 0.7774474620819092
Loss at iteration 250 : 0.8818063139915466
Loss at iteration 300 : 0.8437963724136353
Loss at iteration 350 : 0.43690308928489685
Loss at iteration 400 : 0.45163610577583313
Loss at iteration 450 : 0.2172967493534088
Mean training loss eporch  238 :  0.5911326068901617
Loss at iteration 50 : 0.6428417563438416
Loss at iteration 100 : 0.2498997151851654
Loss at iteration 150 : 0.38042423129081726
Loss at iteration 200 : 0.593191921710968
Loss at iteration 250 : 0.49943822622299194
Loss at iteration 300 : 0.2261517345905304
Loss at iteration 350 : 0.39869797229766846
Loss at iteration 400 : 0.7001339197158813
Loss at iteration 450 : 0.6609139442443848
Mean training loss eporch  239 :  0.5910561682975941
Loss at iteration 50 : 1.0375239849090576
Loss at iteration 100 : 0.7035151720046997
Loss at iteration 150 : 0.584099292755127
Loss at iteration 200 : 0.33519238233566284
Loss at iteration 250 : 0.5921774506568909
Loss at iteration 300 : 0.26017826795578003
Loss at iteration 350 : 0.5698813199996948
Loss at iteration 400 : 0.6771407723426819
Loss at iteration 450 : 0.6406459808349609
Mean training loss eporch  240 :  0.5912397883240149
Loss at iteration 50 : 0.3762526512145996
Loss at iteration 100 : 0.6267390251159668
Loss at iteration 150 : 0.617206335067749
Loss at iteration 200 : 0.4092002809047699
Loss at iteration 250 : 0.6431008577346802
Loss at iteration 300 : 0.3987140655517578
Loss at iteration 350 : 0.4857558310031891
Loss at iteration 400 : 0.3850184381008148
Loss at iteration 450 : 0.9052960872650146
Mean training loss eporch  241 :  0.5905973314927213
Loss at iteration 50 : 0.4104936718940735
Loss at iteration 100 : 0.5614739656448364
Loss at iteration 150 : 1.1770964860916138
Loss at iteration 200 : 0.25202175974845886
Loss at iteration 250 : 0.5866495966911316
Loss at iteration 300 : 0.5122036933898926
Loss at iteration 350 : 0.41661307215690613
Loss at iteration 400 : 0.503338634967804
Loss at iteration 450 : 0.7210996150970459
Mean training loss eporch  242 :  0.5907910796638313
Loss at iteration 50 : 0.2921450138092041
Loss at iteration 100 : 0.2598739564418793
Loss at iteration 150 : 0.6468324661254883
Loss at iteration 200 : 1.1827671527862549
Loss at iteration 250 : 0.5689239501953125
Loss at iteration 300 : 0.6819924116134644
Loss at iteration 350 : 0.5255716443061829
Loss at iteration 400 : 0.7218177914619446
Loss at iteration 450 : 0.5561395287513733
Mean training loss eporch  243 :  0.5906726445800091
Loss at iteration 50 : 0.7714434266090393
Loss at iteration 100 : 0.5264058113098145
Loss at iteration 150 : 0.5108039975166321
Loss at iteration 200 : 0.40784284472465515
Loss at iteration 250 : 0.7608186602592468
Loss at iteration 300 : 0.5008944272994995
Loss at iteration 350 : 0.4203915596008301
Loss at iteration 400 : 0.4068197011947632
Loss at iteration 450 : 0.5312409400939941
Mean training loss eporch  244 :  0.5900102288279074
Loss at iteration 50 : 0.7905615568161011
Loss at iteration 100 : 0.6630429625511169
Loss at iteration 150 : 0.9241726398468018
Loss at iteration 200 : 0.4917384088039398
Loss at iteration 250 : 0.8338385820388794
Loss at iteration 300 : 0.6933574080467224
Loss at iteration 350 : 0.7046406269073486
Loss at iteration 400 : 0.5124490261077881
Loss at iteration 450 : 0.43406397104263306
Mean training loss eporch  245 :  0.5905372851126862
Loss at iteration 50 : 0.4297889471054077
Loss at iteration 100 : 0.7487732172012329
Loss at iteration 150 : 0.3504585921764374
Loss at iteration 200 : 0.6821146011352539
Loss at iteration 250 : 0.4777738153934479
Loss at iteration 300 : 0.6128503680229187
Loss at iteration 350 : 0.34997060894966125
Loss at iteration 400 : 0.5409661531448364
Loss at iteration 450 : 0.5371896028518677
Mean training loss eporch  246 :  0.5900633212288054
Loss at iteration 50 : 0.5388675928115845
Loss at iteration 100 : 0.596089243888855
Loss at iteration 150 : 0.46384137868881226
Loss at iteration 200 : 0.420909583568573
Loss at iteration 250 : 0.7875776290893555
Loss at iteration 300 : 0.6796782612800598
Loss at iteration 350 : 0.598914623260498
Loss at iteration 400 : 0.6562409996986389
Loss at iteration 450 : 0.8585780262947083
Mean training loss eporch  247 :  0.5901010175816185
Loss at iteration 50 : 0.4005504846572876
Loss at iteration 100 : 1.2607204914093018
Loss at iteration 150 : 0.47934770584106445
Loss at iteration 200 : 0.40856409072875977
Loss at iteration 250 : 0.450011670589447
Loss at iteration 300 : 0.4389289319515228
Loss at iteration 350 : 0.6105717420578003
Loss at iteration 400 : 0.39733898639678955
Loss at iteration 450 : 0.49084800481796265
Mean training loss eporch  248 :  0.5902423860824757
Loss at iteration 50 : 0.31622904539108276
Loss at iteration 100 : 0.35618382692337036
Loss at iteration 150 : 0.4612513780593872
Loss at iteration 200 : 0.8939985036849976
Loss at iteration 250 : 0.7198755145072937
Loss at iteration 300 : 0.4272902309894562
Loss at iteration 350 : 0.35401129722595215
Loss at iteration 400 : 1.1363048553466797
Loss at iteration 450 : 0.20640891790390015
Mean training loss eporch  249 :  0.590468981774781
Loss at iteration 50 : 0.4005663990974426
Loss at iteration 100 : 0.4759487509727478
Loss at iteration 150 : 0.451961874961853
Loss at iteration 200 : 0.6285122036933899
Loss at iteration 250 : 0.49947696924209595
Loss at iteration 300 : 0.31312358379364014
Loss at iteration 350 : 0.4070051610469818
Loss at iteration 400 : 0.5589827299118042
Loss at iteration 450 : 0.7376882433891296
Mean training loss eporch  250 :  0.589960980271944
Loss at iteration 50 : 0.48435527086257935
Loss at iteration 100 : 0.8445945978164673
Loss at iteration 150 : 0.35966575145721436
Loss at iteration 200 : 0.6391482949256897
Loss at iteration 250 : 0.7093522548675537
Loss at iteration 300 : 0.6990333795547485
Loss at iteration 350 : 0.32561397552490234
Loss at iteration 400 : 0.6009208559989929
Loss at iteration 450 : 0.7479450106620789
Mean training loss eporch  251 :  0.5905774470465452
Loss at iteration 50 : 0.5888166427612305
Loss at iteration 100 : 0.9856082201004028
Loss at iteration 150 : 0.8751336336135864
Loss at iteration 200 : 0.4654603600502014
Loss at iteration 250 : 0.3768388032913208
Loss at iteration 300 : 1.3238036632537842
Loss at iteration 350 : 0.4812600612640381
Loss at iteration 400 : 0.7408179640769958
Loss at iteration 450 : 0.6925745606422424
Mean training loss eporch  252 :  0.5897791114748772
Loss at iteration 50 : 0.4254406988620758
Loss at iteration 100 : 0.8132961392402649
Loss at iteration 150 : 0.9664643406867981
Loss at iteration 200 : 1.0697439908981323
Loss at iteration 250 : 0.8367995023727417
Loss at iteration 300 : 0.8429456949234009
Loss at iteration 350 : 1.0211848020553589
Loss at iteration 400 : 0.544032096862793
Loss at iteration 450 : 0.6572701930999756
Mean training loss eporch  253 :  0.5891284101864783
Loss at iteration 50 : 0.5978017449378967
Loss at iteration 100 : 0.42795565724372864
Loss at iteration 150 : 0.4562835991382599
Loss at iteration 200 : 0.7004320621490479
Loss at iteration 250 : 0.48558393120765686
Loss at iteration 300 : 0.8712517023086548
Loss at iteration 350 : 0.2777175009250641
Loss at iteration 400 : 0.7663933634757996
Loss at iteration 450 : 0.6718509197235107
Mean training loss eporch  254 :  0.5896299646925727
Loss at iteration 50 : 0.8029167652130127
Loss at iteration 100 : 0.23859184980392456
Loss at iteration 150 : 0.6228665113449097
Loss at iteration 200 : 0.8306923508644104
Loss at iteration 250 : 0.5867630243301392
Loss at iteration 300 : 0.33978694677352905
Loss at iteration 350 : 0.5154823064804077
Loss at iteration 400 : 0.6812242269515991
Loss at iteration 450 : 0.5389184355735779
Mean training loss eporch  255 :  0.5896278031201542
Loss at iteration 50 : 0.5527085661888123
Loss at iteration 100 : 0.30741602182388306
Loss at iteration 150 : 0.4270380437374115
Loss at iteration 200 : 0.5240594744682312
Loss at iteration 250 : 0.48499637842178345
Loss at iteration 300 : 0.7833266854286194
Loss at iteration 350 : 0.5716617107391357
Loss at iteration 400 : 0.381075918674469
Loss at iteration 450 : 0.448486328125
Mean training loss eporch  256 :  0.5887908264059402
Loss at iteration 50 : 1.0578240156173706
Loss at iteration 100 : 0.6077487468719482
Loss at iteration 150 : 0.23752237856388092
Loss at iteration 200 : 0.32052111625671387
Loss at iteration 250 : 0.7594397068023682
Loss at iteration 300 : 0.9584096670150757
Loss at iteration 350 : 0.7455633878707886
Loss at iteration 400 : 0.29055076837539673
Loss at iteration 450 : 0.5399346351623535
Mean training loss eporch  257 :  0.5893717815624121
Loss at iteration 50 : 0.9217449426651001
Loss at iteration 100 : 0.4772036373615265
Loss at iteration 150 : 0.9111617803573608
Loss at iteration 200 : 0.6099736094474792
Loss at iteration 250 : 0.47721314430236816
Loss at iteration 300 : 0.3429199457168579
Loss at iteration 350 : 0.5841718912124634
Loss at iteration 400 : 0.3599122166633606
Loss at iteration 450 : 0.5123053789138794
Mean training loss eporch  258 :  0.5885547641742679
Loss at iteration 50 : 0.8568578958511353
Loss at iteration 100 : 0.5546647906303406
Loss at iteration 150 : 0.4303678870201111
Loss at iteration 200 : 0.46664685010910034
Loss at iteration 250 : 0.7061319351196289
Loss at iteration 300 : 0.8644384145736694
Loss at iteration 350 : 0.22577492892742157
Loss at iteration 400 : 1.0035226345062256
Loss at iteration 450 : 0.4841904938220978
Mean training loss eporch  259 :  0.5891394677209555
Loss at iteration 50 : 0.33410245180130005
Loss at iteration 100 : 0.5584539771080017
Loss at iteration 150 : 0.78330397605896
Loss at iteration 200 : 0.7424106597900391
Loss at iteration 250 : 0.7331961393356323
Loss at iteration 300 : 0.5341098308563232
Loss at iteration 350 : 0.712770938873291
Loss at iteration 400 : 0.4189596474170685
Loss at iteration 450 : 0.4766629934310913
Mean training loss eporch  260 :  0.588421143466708
Loss at iteration 50 : 0.7228946089744568
Loss at iteration 100 : 0.39072781801223755
Loss at iteration 150 : 0.935274600982666
Loss at iteration 200 : 0.801965594291687
Loss at iteration 250 : 0.5230425596237183
Loss at iteration 300 : 0.9345632195472717
Loss at iteration 350 : 0.4812812805175781
Loss at iteration 400 : 0.4814276695251465
Loss at iteration 450 : 0.36383700370788574
Mean training loss eporch  261 :  0.588090483457723
Loss at iteration 50 : 0.5204243063926697
Loss at iteration 100 : 0.2680339813232422
Loss at iteration 150 : 0.2529889941215515
Loss at iteration 200 : 0.44046473503112793
Loss at iteration 250 : 0.531389594078064
Loss at iteration 300 : 0.8187037706375122
Loss at iteration 350 : 0.36366355419158936
Loss at iteration 400 : 0.36665791273117065
Loss at iteration 450 : 0.4885658025741577
Mean training loss eporch  262 :  0.5880896977487967
Loss at iteration 50 : 0.6878676414489746
Loss at iteration 100 : 0.5921306014060974
Loss at iteration 150 : 1.149274468421936
Loss at iteration 200 : 0.9068504571914673
Loss at iteration 250 : 0.7545194029808044
Loss at iteration 300 : 0.4297674894332886
Loss at iteration 350 : 0.48538878560066223
Loss at iteration 400 : 0.38715600967407227
Loss at iteration 450 : 0.2883005142211914
Mean training loss eporch  263 :  0.5878935630850214
Loss at iteration 50 : 0.24786587059497833
Loss at iteration 100 : 0.47279277443885803
Loss at iteration 150 : 0.21786771714687347
Loss at iteration 200 : 0.7438082695007324
Loss at iteration 250 : 0.21227037906646729
Loss at iteration 300 : 0.9443929195404053
Loss at iteration 350 : 0.3201911449432373
Loss at iteration 400 : 0.5120776891708374
Loss at iteration 450 : 0.9343883395195007
Mean training loss eporch  264 :  0.588452549191449
Loss at iteration 50 : 0.7416170239448547
Loss at iteration 100 : 0.7374765872955322
Loss at iteration 150 : 0.8075259327888489
Loss at iteration 200 : 0.28630995750427246
Loss at iteration 250 : 0.29352903366088867
Loss at iteration 300 : 0.5319693088531494
Loss at iteration 350 : 0.45860108733177185
Loss at iteration 400 : 0.6139888167381287
Loss at iteration 450 : 0.45872342586517334
Mean training loss eporch  265 :  0.5878445246104915
Loss at iteration 50 : 0.7370138764381409
Loss at iteration 100 : 0.5329614877700806
Loss at iteration 150 : 0.33510592579841614
Loss at iteration 200 : 0.3599330186843872
Loss at iteration 250 : 0.4528499245643616
Loss at iteration 300 : 0.411892294883728
Loss at iteration 350 : 0.7740685343742371
Loss at iteration 400 : 0.8681752681732178
Loss at iteration 450 : 0.7494204044342041
Mean training loss eporch  266 :  0.5880200132478232
Loss at iteration 50 : 0.5714691281318665
Loss at iteration 100 : 0.3558174967765808
Loss at iteration 150 : 0.9161009788513184
Loss at iteration 200 : 0.4415605664253235
Loss at iteration 250 : 0.3395532965660095
Loss at iteration 300 : 0.301688015460968
Loss at iteration 350 : 0.4826473593711853
Loss at iteration 400 : 0.41816478967666626
Loss at iteration 450 : 0.2480381578207016
Mean training loss eporch  267 :  0.5881221990206251
Loss at iteration 50 : 0.5162447690963745
Loss at iteration 100 : 0.664027214050293
Loss at iteration 150 : 0.8147038221359253
Loss at iteration 200 : 0.8525147438049316
Loss at iteration 250 : 0.7231262922286987
Loss at iteration 300 : 0.3049764037132263
Loss at iteration 350 : 0.6882556080818176
Loss at iteration 400 : 0.6220003366470337
Loss at iteration 450 : 0.24888768792152405
Mean training loss eporch  268 :  0.5876221783365665
Loss at iteration 50 : 0.6952100992202759
Loss at iteration 100 : 0.2827781140804291
Loss at iteration 150 : 0.6601440906524658
Loss at iteration 200 : 0.28054019808769226
Loss at iteration 250 : 0.7591190338134766
Loss at iteration 300 : 0.39429956674575806
Loss at iteration 350 : 0.7058926820755005
Loss at iteration 400 : 0.7027901411056519
Loss at iteration 450 : 0.481342077255249
Mean training loss eporch  269 :  0.5882231559464124
Loss at iteration 50 : 0.6432090997695923
Loss at iteration 100 : 0.9586610794067383
Loss at iteration 150 : 0.5164796113967896
Loss at iteration 200 : 0.477876216173172
Loss at iteration 250 : 0.9481006860733032
Loss at iteration 300 : 0.6520483493804932
Loss at iteration 350 : 0.5643868446350098
Loss at iteration 400 : 0.37341925501823425
Loss at iteration 450 : 0.7487342953681946
Mean training loss eporch  270 :  0.5877776839556056
Loss at iteration 50 : 0.5997766256332397
Loss at iteration 100 : 0.4481005072593689
Loss at iteration 150 : 0.6821157932281494
Loss at iteration 200 : 0.6347079873085022
Loss at iteration 250 : 0.9191751480102539
Loss at iteration 300 : 0.7773687839508057
Loss at iteration 350 : 0.6665270328521729
Loss at iteration 400 : 0.6895459890365601
Loss at iteration 450 : 0.43004292249679565
Mean training loss eporch  271 :  0.5870149049237682
Loss at iteration 50 : 0.7705936431884766
Loss at iteration 100 : 0.3971511721611023
Loss at iteration 150 : 0.7798119783401489
Loss at iteration 200 : 0.6516897082328796
Loss at iteration 250 : 0.7436090111732483
Loss at iteration 300 : 0.8975279331207275
Loss at iteration 350 : 0.4801221489906311
Loss at iteration 400 : 0.682693362236023
Loss at iteration 450 : 0.31995856761932373
Mean training loss eporch  272 :  0.5875324438555969
Loss at iteration 50 : 0.6147552728652954
Loss at iteration 100 : 1.4568653106689453
Loss at iteration 150 : 0.7082897424697876
Loss at iteration 200 : 0.39083132147789
Loss at iteration 250 : 0.5279940962791443
Loss at iteration 300 : 0.6288707256317139
Loss at iteration 350 : 0.862185001373291
Loss at iteration 400 : 0.4697839617729187
Loss at iteration 450 : 0.6876643300056458
Mean training loss eporch  273 :  0.5867125360162687
Loss at iteration 50 : 0.5348305702209473
Loss at iteration 100 : 0.5653386116027832
Loss at iteration 150 : 0.4921852946281433
Loss at iteration 200 : 0.5618748664855957
Loss at iteration 250 : 0.7289648056030273
Loss at iteration 300 : 0.6792252659797668
Loss at iteration 350 : 0.6098423004150391
Loss at iteration 400 : 0.6122984886169434
Loss at iteration 450 : 0.5414254665374756
Mean training loss eporch  274 :  0.5872420395829687
Loss at iteration 50 : 0.5292352437973022
Loss at iteration 100 : 0.4678204655647278
Loss at iteration 150 : 0.413010835647583
Loss at iteration 200 : 0.9752779006958008
Loss at iteration 250 : 0.8530880212783813
Loss at iteration 300 : 0.4039963185787201
Loss at iteration 350 : 0.5311774015426636
Loss at iteration 400 : 1.0441269874572754
Loss at iteration 450 : 0.5833162069320679
Mean training loss eporch  275 :  0.5870408891821007
Loss at iteration 50 : 0.5400056838989258
Loss at iteration 100 : 0.7475975751876831
Loss at iteration 150 : 0.6663993000984192
Loss at iteration 200 : 0.7948892116546631
Loss at iteration 250 : 0.6210688352584839
Loss at iteration 300 : 0.32131409645080566
Loss at iteration 350 : 0.6588083505630493
Loss at iteration 400 : 0.9007774591445923
Loss at iteration 450 : 0.4812889099121094
Mean training loss eporch  276 :  0.586681161714897
Loss at iteration 50 : 1.015924096107483
Loss at iteration 100 : 0.5032281875610352
Loss at iteration 150 : 0.5292108058929443
Loss at iteration 200 : 1.1494665145874023
Loss at iteration 250 : 0.8789814710617065
Loss at iteration 300 : 0.38154733180999756
Loss at iteration 350 : 0.7287989258766174
Loss at iteration 400 : 0.8345608711242676
Loss at iteration 450 : 0.5545308589935303
Mean training loss eporch  277 :  0.5876322360280667
Loss at iteration 50 : 0.31632229685783386
Loss at iteration 100 : 0.8046365976333618
Loss at iteration 150 : 0.5855064392089844
Loss at iteration 200 : 0.33670997619628906
Loss at iteration 250 : 0.8106032013893127
Loss at iteration 300 : 0.3953895568847656
Loss at iteration 350 : 0.8442807197570801
Loss at iteration 400 : 0.2729422450065613
Loss at iteration 450 : 0.5155593752861023
Mean training loss eporch  278 :  0.5863256324160548
Loss at iteration 50 : 0.34657496213912964
Loss at iteration 100 : 0.5720905065536499
Loss at iteration 150 : 0.3418123722076416
Loss at iteration 200 : 0.2820981740951538
Loss at iteration 250 : 0.7999045848846436
Loss at iteration 300 : 0.7030718326568604
Loss at iteration 350 : 0.7130314111709595
Loss at iteration 400 : 1.0374410152435303
Loss at iteration 450 : 0.3418331444263458
Mean training loss eporch  279 :  0.5862308686996603
Loss at iteration 50 : 0.49416112899780273
Loss at iteration 100 : 0.42229509353637695
Loss at iteration 150 : 1.021148920059204
Loss at iteration 200 : 0.776482880115509
Loss at iteration 250 : 0.8747953176498413
Loss at iteration 300 : 0.27394282817840576
Loss at iteration 350 : 0.4715142250061035
Loss at iteration 400 : 0.9401240348815918
Loss at iteration 450 : 0.23558521270751953
Mean training loss eporch  280 :  0.5870536432356016
Loss at iteration 50 : 0.5855804681777954
Loss at iteration 100 : 0.7907541990280151
Loss at iteration 150 : 0.6128034591674805
Loss at iteration 200 : 0.5392026901245117
Loss at iteration 250 : 0.6633983850479126
Loss at iteration 300 : 0.5753320455551147
Loss at iteration 350 : 0.7019623517990112
Loss at iteration 400 : 0.5049090385437012
Loss at iteration 450 : 0.7294086217880249
Mean training loss eporch  281 :  0.5863445260285334
Loss at iteration 50 : 0.8839025497436523
Loss at iteration 100 : 1.0097267627716064
Loss at iteration 150 : 0.9757401943206787
Loss at iteration 200 : 0.8683719635009766
Loss at iteration 250 : 0.6421071887016296
Loss at iteration 300 : 0.4610868990421295
Loss at iteration 350 : 0.8599555492401123
Loss at iteration 400 : 0.8686527013778687
Loss at iteration 450 : 0.8081915378570557
Mean training loss eporch  282 :  0.5862142834266858
Loss at iteration 50 : 0.8016473650932312
Loss at iteration 100 : 0.6734552383422852
Loss at iteration 150 : 0.3650816082954407
Loss at iteration 200 : 0.6091416478157043
Loss at iteration 250 : 0.542119026184082
Loss at iteration 300 : 1.1566652059555054
Loss at iteration 350 : 0.4566535949707031
Loss at iteration 400 : 0.6756240129470825
Loss at iteration 450 : 0.6810775995254517
Mean training loss eporch  283 :  0.5864609062172877
Loss at iteration 50 : 0.38808611035346985
Loss at iteration 100 : 0.43788349628448486
Loss at iteration 150 : 0.5962888598442078
Loss at iteration 200 : 0.8185313940048218
Loss at iteration 250 : 0.5256046056747437
Loss at iteration 300 : 0.7942620515823364
Loss at iteration 350 : 0.41280341148376465
Loss at iteration 400 : 0.4947270452976227
Loss at iteration 450 : 0.6916497945785522
Mean training loss eporch  284 :  0.5859279925292007
Loss at iteration 50 : 0.510771632194519
Loss at iteration 100 : 0.5635114908218384
Loss at iteration 150 : 0.6341613531112671
Loss at iteration 200 : 0.8038251996040344
Loss at iteration 250 : 0.3769192695617676
Loss at iteration 300 : 0.29766911268234253
Loss at iteration 350 : 0.9805465936660767
Loss at iteration 400 : 0.48720619082450867
Loss at iteration 450 : 0.5706000328063965
Mean training loss eporch  285 :  0.5861890308216027
Loss at iteration 50 : 0.4008285403251648
Loss at iteration 100 : 0.7215676307678223
Loss at iteration 150 : 0.741867184638977
Loss at iteration 200 : 0.6622772812843323
Loss at iteration 250 : 0.7851129770278931
Loss at iteration 300 : 0.32741880416870117
Loss at iteration 350 : 0.6509474515914917
Loss at iteration 400 : 0.23448839783668518
Loss at iteration 450 : 0.5952351093292236
Mean training loss eporch  286 :  0.5856372391391997
Loss at iteration 50 : 0.7966355085372925
Loss at iteration 100 : 0.5119102001190186
Loss at iteration 150 : 0.789704442024231
Loss at iteration 200 : 0.7741315364837646
Loss at iteration 250 : 0.5768126249313354
Loss at iteration 300 : 0.30351150035858154
Loss at iteration 350 : 0.24899083375930786
Loss at iteration 400 : 0.30905717611312866
Loss at iteration 450 : 0.9288421869277954
Mean training loss eporch  287 :  0.5863614388776623
Loss at iteration 50 : 0.29598167538642883
Loss at iteration 100 : 0.7916727662086487
Loss at iteration 150 : 0.6020081043243408
Loss at iteration 200 : 0.8681427240371704
Loss at iteration 250 : 0.5490530729293823
Loss at iteration 300 : 0.5525627732276917
Loss at iteration 350 : 0.45411255955696106
Loss at iteration 400 : 0.4626256227493286
Loss at iteration 450 : 0.669600248336792
Mean training loss eporch  288 :  0.5862825297785603
Loss at iteration 50 : 0.3174331784248352
Loss at iteration 100 : 0.9210873246192932
Loss at iteration 150 : 0.7854689359664917
Loss at iteration 200 : 0.7950378656387329
Loss at iteration 250 : 0.49582189321517944
Loss at iteration 300 : 1.0042728185653687
Loss at iteration 350 : 0.603541374206543
Loss at iteration 400 : 0.559444010257721
Loss at iteration 450 : 0.9377736449241638
Mean training loss eporch  289 :  0.5857811064213888
Loss at iteration 50 : 0.4083089828491211
Loss at iteration 100 : 0.394898384809494
Loss at iteration 150 : 0.48146647214889526
Loss at iteration 200 : 1.1525373458862305
Loss at iteration 250 : 0.7180273532867432
Loss at iteration 300 : 0.9071863889694214
Loss at iteration 350 : 0.6042824387550354
Loss at iteration 400 : 0.5904621481895447
Loss at iteration 450 : 1.2746633291244507
Mean training loss eporch  290 :  0.5863400979520885
Loss at iteration 50 : 0.24888359010219574
Loss at iteration 100 : 0.25071030855178833
Loss at iteration 150 : 0.40582558512687683
Loss at iteration 200 : 0.6400336623191833
Loss at iteration 250 : 0.6046508550643921
Loss at iteration 300 : 0.42651671171188354
Loss at iteration 350 : 0.8102144598960876
Loss at iteration 400 : 0.5524608492851257
Loss at iteration 450 : 0.5588980913162231
Mean training loss eporch  291 :  0.5850206865426387
Loss at iteration 50 : 0.1799478381872177
Loss at iteration 100 : 0.6265541315078735
Loss at iteration 150 : 0.6956795454025269
Loss at iteration 200 : 0.6397919654846191
Loss at iteration 250 : 0.6126794219017029
Loss at iteration 300 : 0.4520810544490814
Loss at iteration 350 : 1.1284940242767334
Loss at iteration 400 : 0.9701410531997681
Loss at iteration 450 : 0.24582067131996155
Mean training loss eporch  292 :  0.586789239837036
Loss at iteration 50 : 0.31662294268608093
Loss at iteration 100 : 0.8034927845001221
Loss at iteration 150 : 0.32460182905197144
Loss at iteration 200 : 0.3619650900363922
Loss at iteration 250 : 0.5639675259590149
Loss at iteration 300 : 0.8982934951782227
Loss at iteration 350 : 0.7741491794586182
Loss at iteration 400 : 0.5346997976303101
Loss at iteration 450 : 0.8704460859298706
Mean training loss eporch  293 :  0.5855651841689852
Loss at iteration 50 : 0.3567713499069214
Loss at iteration 100 : 0.7764233946800232
Loss at iteration 150 : 0.2735724449157715
Loss at iteration 200 : 0.727436363697052
Loss at iteration 250 : 0.5485302209854126
Loss at iteration 300 : 0.35244789719581604
Loss at iteration 350 : 0.5423492193222046
Loss at iteration 400 : 0.9323465824127197
Loss at iteration 450 : 0.8384081721305847
Mean training loss eporch  294 :  0.5854578252616787
Loss at iteration 50 : 0.4036203622817993
Loss at iteration 100 : 0.47410523891448975
Loss at iteration 150 : 0.4717331826686859
Loss at iteration 200 : 0.3203345537185669
Loss at iteration 250 : 0.8991470336914062
Loss at iteration 300 : 0.6793218851089478
Loss at iteration 350 : 0.3657376766204834
Loss at iteration 400 : 0.5171968936920166
Loss at iteration 450 : 0.5942486524581909
Mean training loss eporch  295 :  0.5863424836654044
Loss at iteration 50 : 0.4692167043685913
Loss at iteration 100 : 0.47263142466545105
Loss at iteration 150 : 0.75039142370224
Loss at iteration 200 : 0.7590515613555908
Loss at iteration 250 : 0.6914761066436768
Loss at iteration 300 : 0.4203352928161621
Loss at iteration 350 : 0.41463959217071533
Loss at iteration 400 : 0.377235472202301
Loss at iteration 450 : 0.9461931586265564
Mean training loss eporch  296 :  0.5855844586962935
Loss at iteration 50 : 0.7300417423248291
Loss at iteration 100 : 0.7858995795249939
Loss at iteration 150 : 0.4424307346343994
Loss at iteration 200 : 0.5145871043205261
Loss at iteration 250 : 0.4875262975692749
Loss at iteration 300 : 0.4968211054801941
Loss at iteration 350 : 0.454019159078598
Loss at iteration 400 : 0.5274322032928467
Loss at iteration 450 : 0.4092717170715332
Mean training loss eporch  297 :  0.5852090790244326
Loss at iteration 50 : 0.7040886878967285
Loss at iteration 100 : 0.401298463344574
Loss at iteration 150 : 0.707008421421051
Loss at iteration 200 : 0.5080733299255371
Loss at iteration 250 : 0.6953214406967163
Loss at iteration 300 : 0.37238264083862305
Loss at iteration 350 : 0.47334134578704834
Loss at iteration 400 : 0.43645381927490234
Loss at iteration 450 : 0.6943016648292542
Mean training loss eporch  298 :  0.585179036568897
Loss at iteration 50 : 0.5293992757797241
Loss at iteration 100 : 0.6321893334388733
Loss at iteration 150 : 0.6769726276397705
Loss at iteration 200 : 0.4384969472885132
Loss at iteration 250 : 0.6463571786880493
Loss at iteration 300 : 0.7512689232826233
Loss at iteration 350 : 0.3219391107559204
Loss at iteration 400 : 0.31250160932540894
Loss at iteration 450 : 0.2349695861339569
Mean training loss eporch  299 :  0.5856481164257397
Min training loss at epoch  292 :  0.5850206865426387
