Total training examples: 3561
Loss at iteration 50 : 1.296865701675415
Loss at iteration 100 : 0.9789953827857971
Loss at iteration 150 : 0.32319575548171997
Loss at iteration 200 : 0.5672298073768616
Loss at iteration 250 : 0.5676242113113403
Loss at iteration 300 : 1.4485970735549927
Loss at iteration 350 : 0.6846802234649658
Loss at iteration 400 : 0.6081736087799072
Mean training loss eporch  0 :  0.7036213150473454
Loss at iteration 50 : 0.8689693212509155
Loss at iteration 100 : 0.3409314751625061
Loss at iteration 150 : 0.9762783050537109
Loss at iteration 200 : 0.9535062313079834
Loss at iteration 250 : 0.7610936164855957
Loss at iteration 300 : 0.7181305885314941
Loss at iteration 350 : 0.5344444513320923
Loss at iteration 400 : 1.2339051961898804
Mean training loss eporch  1 :  0.6693641808669129
Loss at iteration 50 : 0.4650474190711975
Loss at iteration 100 : 0.6596905589103699
Loss at iteration 150 : 0.6563149094581604
Loss at iteration 200 : 0.5294964909553528
Loss at iteration 250 : 0.6320812702178955
Loss at iteration 300 : 0.419251412153244
Loss at iteration 350 : 1.0884207487106323
Loss at iteration 400 : 0.42240840196609497
Mean training loss eporch  2 :  0.6641132211137245
Loss at iteration 50 : 0.9307589530944824
Loss at iteration 100 : 0.45668429136276245
Loss at iteration 150 : 0.3113636374473572
Loss at iteration 200 : 0.8752163648605347
Loss at iteration 250 : 0.3278861939907074
Loss at iteration 300 : 0.45717281103134155
Loss at iteration 350 : 0.41410380601882935
Loss at iteration 400 : 0.4556580185890198
Mean training loss eporch  3 :  0.6633196081548528
Loss at iteration 50 : 0.6133259534835815
Loss at iteration 100 : 0.9551330804824829
Loss at iteration 150 : 0.6393624544143677
Loss at iteration 200 : 1.3395788669586182
Loss at iteration 250 : 0.9511192440986633
Loss at iteration 300 : 0.8735256791114807
Loss at iteration 350 : 0.7959998846054077
Loss at iteration 400 : 0.5253262519836426
Mean training loss eporch  4 :  0.6588069533954287
Loss at iteration 50 : 0.7811533212661743
Loss at iteration 100 : 0.4576435089111328
Loss at iteration 150 : 1.1121536493301392
Loss at iteration 200 : 0.5002797245979309
Loss at iteration 250 : 0.7723556160926819
Loss at iteration 300 : 0.34297674894332886
Loss at iteration 350 : 0.8401889801025391
Loss at iteration 400 : 0.5375258922576904
Mean training loss eporch  5 :  0.6567937537960942
Loss at iteration 50 : 0.8483210802078247
Loss at iteration 100 : 0.7371711730957031
Loss at iteration 150 : 1.24918794631958
Loss at iteration 200 : 0.7592582702636719
Loss at iteration 250 : 1.8443301916122437
Loss at iteration 300 : 0.8294544219970703
Loss at iteration 350 : 0.5647773742675781
Loss at iteration 400 : 0.36334705352783203
Mean training loss eporch  6 :  0.6556540423578211
Loss at iteration 50 : 0.5850048065185547
Loss at iteration 100 : 0.5013924241065979
Loss at iteration 150 : 0.4279671609401703
Loss at iteration 200 : 0.8896427154541016
Loss at iteration 250 : 0.693731427192688
Loss at iteration 300 : 0.6934718489646912
Loss at iteration 350 : 0.6346877813339233
Loss at iteration 400 : 0.6441024541854858
Mean training loss eporch  7 :  0.6566508638885523
Loss at iteration 50 : 0.4280511438846588
Loss at iteration 100 : 0.6740842461585999
Loss at iteration 150 : 1.4029994010925293
Loss at iteration 200 : 0.8442918658256531
Loss at iteration 250 : 0.9475401639938354
Loss at iteration 300 : 0.3138293921947479
Loss at iteration 350 : 0.5134859085083008
Loss at iteration 400 : 0.46300333738327026
Mean training loss eporch  8 :  0.6533982508519305
Loss at iteration 50 : 1.041254997253418
Loss at iteration 100 : 0.6107037663459778
Loss at iteration 150 : 0.7032892107963562
Loss at iteration 200 : 0.5540624260902405
Loss at iteration 250 : 0.6578244566917419
Loss at iteration 300 : 0.5992283225059509
Loss at iteration 350 : 0.6184411644935608
Loss at iteration 400 : 0.5601823329925537
Mean training loss eporch  9 :  0.65588882498677
Loss at iteration 50 : 1.0296800136566162
Loss at iteration 100 : 0.32837045192718506
Loss at iteration 150 : 0.30199897289276123
Loss at iteration 200 : 1.054349660873413
Loss at iteration 250 : 0.7685190439224243
Loss at iteration 300 : 0.36555391550064087
Loss at iteration 350 : 0.3798328638076782
Loss at iteration 400 : 0.9025924205780029
Mean training loss eporch  10 :  0.6493073859556908
Loss at iteration 50 : 0.7007606029510498
Loss at iteration 100 : 0.2804632782936096
Loss at iteration 150 : 0.5077659487724304
Loss at iteration 200 : 0.6584419012069702
Loss at iteration 250 : 0.514660120010376
Loss at iteration 300 : 0.30609720945358276
Loss at iteration 350 : 0.4119885563850403
Loss at iteration 400 : 0.8314120173454285
Mean training loss eporch  11 :  0.6514320203808925
Loss at iteration 50 : 1.405185580253601
Loss at iteration 100 : 1.0453681945800781
Loss at iteration 150 : 0.4945773482322693
Loss at iteration 200 : 0.4976131319999695
Loss at iteration 250 : 0.547670304775238
Loss at iteration 300 : 1.0099749565124512
Loss at iteration 350 : 0.7850636839866638
Loss at iteration 400 : 0.5572869777679443
Mean training loss eporch  12 :  0.6474144983318354
Loss at iteration 50 : 0.6000888347625732
Loss at iteration 100 : 0.7735011577606201
Loss at iteration 150 : 0.35579901933670044
Loss at iteration 200 : 0.3076360523700714
Loss at iteration 250 : 0.4643577039241791
Loss at iteration 300 : 0.5229496359825134
Loss at iteration 350 : 0.3429960310459137
Loss at iteration 400 : 0.7054177522659302
Mean training loss eporch  13 :  0.6470335982572872
Loss at iteration 50 : 0.5205074548721313
Loss at iteration 100 : 0.7622063159942627
Loss at iteration 150 : 0.9938048720359802
Loss at iteration 200 : 0.6245329976081848
Loss at iteration 250 : 0.5688650012016296
Loss at iteration 300 : 1.037424921989441
Loss at iteration 350 : 0.2908228635787964
Loss at iteration 400 : 0.3707738220691681
Mean training loss eporch  14 :  0.6451917471105207
Loss at iteration 50 : 0.6594972610473633
Loss at iteration 100 : 1.1422237157821655
Loss at iteration 150 : 0.3719370365142822
Loss at iteration 200 : 0.6351136565208435
Loss at iteration 250 : 0.5932527780532837
Loss at iteration 300 : 0.33863258361816406
Loss at iteration 350 : 0.9179610013961792
Loss at iteration 400 : 0.9579842686653137
Mean training loss eporch  15 :  0.6448025391374468
Loss at iteration 50 : 0.6709163784980774
Loss at iteration 100 : 1.2614383697509766
Loss at iteration 150 : 0.6497505903244019
Loss at iteration 200 : 0.8661243319511414
Loss at iteration 250 : 0.6648072004318237
Loss at iteration 300 : 0.7199419736862183
Loss at iteration 350 : 0.7225580215454102
Loss at iteration 400 : 0.5074366927146912
Mean training loss eporch  16 :  0.6431203715402983
Loss at iteration 50 : 1.3487768173217773
Loss at iteration 100 : 0.5161380171775818
Loss at iteration 150 : 0.24890254437923431
Loss at iteration 200 : 0.5937206149101257
Loss at iteration 250 : 0.4637472629547119
Loss at iteration 300 : 0.596646785736084
Loss at iteration 350 : 0.9791754484176636
Loss at iteration 400 : 0.704831600189209
Mean training loss eporch  17 :  0.6441934652325818
Loss at iteration 50 : 0.8080719113349915
Loss at iteration 100 : 0.46777546405792236
Loss at iteration 150 : 0.19419655203819275
Loss at iteration 200 : 0.4919910132884979
Loss at iteration 250 : 1.0903260707855225
Loss at iteration 300 : 1.2595741748809814
Loss at iteration 350 : 0.6888068914413452
Loss at iteration 400 : 0.8279922604560852
Mean training loss eporch  18 :  0.6434875944537433
Loss at iteration 50 : 0.4679773449897766
Loss at iteration 100 : 0.36586132645606995
Loss at iteration 150 : 0.31235942244529724
Loss at iteration 200 : 0.7613035440444946
Loss at iteration 250 : 0.3337564468383789
Loss at iteration 300 : 0.41100940108299255
Loss at iteration 350 : 0.4582834839820862
Loss at iteration 400 : 0.7014243006706238
Mean training loss eporch  19 :  0.6413176606655655
Loss at iteration 50 : 0.915984570980072
Loss at iteration 100 : 0.5449872016906738
Loss at iteration 150 : 0.37918853759765625
Loss at iteration 200 : 0.4226731061935425
Loss at iteration 250 : 0.7099913358688354
Loss at iteration 300 : 0.7366366386413574
Loss at iteration 350 : 0.49970969557762146
Loss at iteration 400 : 0.8242138028144836
Mean training loss eporch  20 :  0.6408460830158716
Loss at iteration 50 : 0.5373073816299438
Loss at iteration 100 : 0.3629630506038666
Loss at iteration 150 : 0.24041947722434998
Loss at iteration 200 : 0.7349527478218079
Loss at iteration 250 : 0.9553617835044861
Loss at iteration 300 : 1.0272620916366577
Loss at iteration 350 : 0.6250002384185791
Loss at iteration 400 : 0.6867232918739319
Mean training loss eporch  21 :  0.6396590970757297
Loss at iteration 50 : 1.1201519966125488
Loss at iteration 100 : 0.5719082355499268
Loss at iteration 150 : 0.2673427164554596
Loss at iteration 200 : 0.5476711988449097
Loss at iteration 250 : 1.2406582832336426
Loss at iteration 300 : 0.4866586923599243
Loss at iteration 350 : 0.24982020258903503
Loss at iteration 400 : 0.8972165584564209
Mean training loss eporch  22 :  0.6383020687397285
Loss at iteration 50 : 1.1570035219192505
Loss at iteration 100 : 0.6133601069450378
Loss at iteration 150 : 0.4761495590209961
Loss at iteration 200 : 0.5437566041946411
Loss at iteration 250 : 0.2676183581352234
Loss at iteration 300 : 0.4315195381641388
Loss at iteration 350 : 0.6157134771347046
Loss at iteration 400 : 0.8100062012672424
Mean training loss eporch  23 :  0.6399177485250037
Loss at iteration 50 : 0.2853478193283081
Loss at iteration 100 : 0.406747043132782
Loss at iteration 150 : 0.6249181032180786
Loss at iteration 200 : 0.40898576378822327
Loss at iteration 250 : 0.3069019317626953
Loss at iteration 300 : 0.2834338843822479
Loss at iteration 350 : 0.4626622200012207
Loss at iteration 400 : 0.5534231066703796
Mean training loss eporch  24 :  0.6370191646597845
Loss at iteration 50 : 0.32320863008499146
Loss at iteration 100 : 0.9219020009040833
Loss at iteration 150 : 0.3871619701385498
Loss at iteration 200 : 0.381863534450531
Loss at iteration 250 : 0.38308051228523254
Loss at iteration 300 : 1.0499931573867798
Loss at iteration 350 : 0.522067666053772
Loss at iteration 400 : 0.5460119247436523
Mean training loss eporch  25 :  0.6366860103446806
Loss at iteration 50 : 0.42492520809173584
Loss at iteration 100 : 0.3552956283092499
Loss at iteration 150 : 0.6309647560119629
Loss at iteration 200 : 0.6078382730484009
Loss at iteration 250 : 0.2844742238521576
Loss at iteration 300 : 0.7246308922767639
Loss at iteration 350 : 0.4185000956058502
Loss at iteration 400 : 0.9229663610458374
Mean training loss eporch  26 :  0.6353458306447273
Loss at iteration 50 : 0.3772332966327667
Loss at iteration 100 : 0.37282276153564453
Loss at iteration 150 : 0.5109073519706726
Loss at iteration 200 : 0.5027496218681335
Loss at iteration 250 : 0.4138321578502655
Loss at iteration 300 : 0.8671227693557739
Loss at iteration 350 : 0.3440254330635071
Loss at iteration 400 : 0.35047805309295654
Mean training loss eporch  27 :  0.6353553869331364
Loss at iteration 50 : 0.5327365398406982
Loss at iteration 100 : 0.2641918659210205
Loss at iteration 150 : 0.6825936436653137
Loss at iteration 200 : 0.8238198757171631
Loss at iteration 250 : 0.5632697343826294
Loss at iteration 300 : 0.7080174088478088
Loss at iteration 350 : 0.5905561447143555
Loss at iteration 400 : 0.6932390928268433
Mean training loss eporch  28 :  0.6412806339862636
Loss at iteration 50 : 0.28449010848999023
Loss at iteration 100 : 0.607666015625
Loss at iteration 150 : 0.6602805852890015
Loss at iteration 200 : 0.7078699469566345
Loss at iteration 250 : 0.36953073740005493
Loss at iteration 300 : 0.6443099975585938
Loss at iteration 350 : 0.4879617989063263
Loss at iteration 400 : 0.44898858666419983
Mean training loss eporch  29 :  0.6336245093444538
Loss at iteration 50 : 1.0282039642333984
Loss at iteration 100 : 0.586250901222229
Loss at iteration 150 : 0.7311015129089355
Loss at iteration 200 : 0.287325918674469
Loss at iteration 250 : 0.2602129876613617
Loss at iteration 300 : 0.6686603426933289
Loss at iteration 350 : 0.5103003978729248
Loss at iteration 400 : 0.7371357083320618
Mean training loss eporch  30 :  0.6343017866993699
Loss at iteration 50 : 0.6468203663825989
Loss at iteration 100 : 0.35840457677841187
Loss at iteration 150 : 0.39399784803390503
Loss at iteration 200 : 0.6174665093421936
Loss at iteration 250 : 0.6149673461914062
Loss at iteration 300 : 0.7028179168701172
Loss at iteration 350 : 0.5908707976341248
Loss at iteration 400 : 0.955633282661438
Mean training loss eporch  31 :  0.6337907678142791
Loss at iteration 50 : 0.9262050986289978
Loss at iteration 100 : 0.4830021858215332
Loss at iteration 150 : 0.2617749869823456
Loss at iteration 200 : 0.4219769835472107
Loss at iteration 250 : 0.4721130132675171
Loss at iteration 300 : 0.9331972599029541
Loss at iteration 350 : 0.6948664784431458
Loss at iteration 400 : 0.4847138524055481
Mean training loss eporch  32 :  0.6324027711233215
Loss at iteration 50 : 0.4987102150917053
Loss at iteration 100 : 1.1019508838653564
Loss at iteration 150 : 0.5414090752601624
Loss at iteration 200 : 1.1999180316925049
Loss at iteration 250 : 0.457014262676239
Loss at iteration 300 : 0.4153045415878296
Loss at iteration 350 : 0.4267168939113617
Loss at iteration 400 : 0.8403844833374023
Mean training loss eporch  33 :  0.6334004015933238
Loss at iteration 50 : 0.7635285258293152
Loss at iteration 100 : 0.6415213346481323
Loss at iteration 150 : 0.9245056509971619
Loss at iteration 200 : 0.34532397985458374
Loss at iteration 250 : 0.32266736030578613
Loss at iteration 300 : 0.3990073800086975
Loss at iteration 350 : 0.6537104249000549
Loss at iteration 400 : 0.8102017641067505
Mean training loss eporch  34 :  0.6323027308746304
Loss at iteration 50 : 0.30745038390159607
Loss at iteration 100 : 0.7494081258773804
Loss at iteration 150 : 0.4761219322681427
Loss at iteration 200 : 0.8266539573669434
Loss at iteration 250 : 0.34255608916282654
Loss at iteration 300 : 0.48069125413894653
Loss at iteration 350 : 0.7231168150901794
Loss at iteration 400 : 0.3389820158481598
Mean training loss eporch  35 :  0.632260549963857
Loss at iteration 50 : 0.7070362567901611
Loss at iteration 100 : 0.753909170627594
Loss at iteration 150 : 0.5229781866073608
Loss at iteration 200 : 0.9776207804679871
Loss at iteration 250 : 1.0614326000213623
Loss at iteration 300 : 0.22096474468708038
Loss at iteration 350 : 0.8133912086486816
Loss at iteration 400 : 0.8735557794570923
Mean training loss eporch  36 :  0.6319147180014127
Loss at iteration 50 : 0.9236634373664856
Loss at iteration 100 : 0.6325099468231201
Loss at iteration 150 : 0.4914315938949585
Loss at iteration 200 : 1.2682485580444336
Loss at iteration 250 : 1.251705288887024
Loss at iteration 300 : 0.7489426136016846
Loss at iteration 350 : 0.8551712036132812
Loss at iteration 400 : 0.44026586413383484
Mean training loss eporch  37 :  0.6312583065153238
Loss at iteration 50 : 0.9227022528648376
Loss at iteration 100 : 0.38465428352355957
Loss at iteration 150 : 0.5068047642707825
Loss at iteration 200 : 0.988153338432312
Loss at iteration 250 : 0.3008362650871277
Loss at iteration 300 : 0.8307728171348572
Loss at iteration 350 : 0.44136476516723633
Loss at iteration 400 : 0.5688872337341309
Mean training loss eporch  38 :  0.6317271778000845
Loss at iteration 50 : 0.6869421005249023
Loss at iteration 100 : 0.6230860948562622
Loss at iteration 150 : 0.7234044075012207
Loss at iteration 200 : 0.5561510324478149
Loss at iteration 250 : 0.5913878083229065
Loss at iteration 300 : 1.0439510345458984
Loss at iteration 350 : 0.8674579858779907
Loss at iteration 400 : 0.6961443424224854
Mean training loss eporch  39 :  0.6303896409513704
Loss at iteration 50 : 0.906708836555481
Loss at iteration 100 : 0.6086580753326416
Loss at iteration 150 : 0.3215048909187317
Loss at iteration 200 : 0.5719021558761597
Loss at iteration 250 : 0.41118669509887695
Loss at iteration 300 : 0.2383137047290802
Loss at iteration 350 : 0.6339843273162842
Loss at iteration 400 : 0.6934613585472107
Mean training loss eporch  40 :  0.6298501507249649
Loss at iteration 50 : 0.42284420132637024
Loss at iteration 100 : 0.7815278172492981
Loss at iteration 150 : 0.8729711771011353
Loss at iteration 200 : 1.100657343864441
Loss at iteration 250 : 1.0627007484436035
Loss at iteration 300 : 0.41411080956459045
Loss at iteration 350 : 0.581322431564331
Loss at iteration 400 : 0.73773592710495
Mean training loss eporch  41 :  0.6357802393524636
Loss at iteration 50 : 0.9509786367416382
Loss at iteration 100 : 0.8452297449111938
Loss at iteration 150 : 0.4456005096435547
Loss at iteration 200 : 0.7370437979698181
Loss at iteration 250 : 0.8933176398277283
Loss at iteration 300 : 0.4179258346557617
Loss at iteration 350 : 0.5282506942749023
Loss at iteration 400 : 0.9560807347297668
Mean training loss eporch  42 :  0.6295306332776899
Loss at iteration 50 : 0.5719483494758606
Loss at iteration 100 : 0.6149768829345703
Loss at iteration 150 : 0.9093804955482483
Loss at iteration 200 : 0.4358937740325928
Loss at iteration 250 : 0.37129130959510803
Loss at iteration 300 : 0.6373324394226074
Loss at iteration 350 : 0.9014069437980652
Loss at iteration 400 : 0.5804248452186584
Mean training loss eporch  43 :  0.6294786846557541
Loss at iteration 50 : 0.9370013475418091
Loss at iteration 100 : 0.5833045840263367
Loss at iteration 150 : 0.29235780239105225
Loss at iteration 200 : 0.8465566635131836
Loss at iteration 250 : 0.5258937478065491
Loss at iteration 300 : 0.8681553602218628
Loss at iteration 350 : 0.5890982151031494
Loss at iteration 400 : 0.40675944089889526
Mean training loss eporch  44 :  0.6343712932325799
Loss at iteration 50 : 0.6463042497634888
Loss at iteration 100 : 1.295225739479065
Loss at iteration 150 : 0.5585780143737793
Loss at iteration 200 : 0.41507112979888916
Loss at iteration 250 : 0.40175551176071167
Loss at iteration 300 : 0.7944208383560181
Loss at iteration 350 : 0.359386146068573
Loss at iteration 400 : 0.29506510496139526
Mean training loss eporch  45 :  0.6281233349389025
Loss at iteration 50 : 1.2660578489303589
Loss at iteration 100 : 0.9315553903579712
Loss at iteration 150 : 0.6383774280548096
Loss at iteration 200 : 0.45372021198272705
Loss at iteration 250 : 0.4034876227378845
Loss at iteration 300 : 0.4779186248779297
Loss at iteration 350 : 0.9477801322937012
Loss at iteration 400 : 0.5737653970718384
Mean training loss eporch  46 :  0.6279110051524479
Loss at iteration 50 : 0.6269844174385071
Loss at iteration 100 : 0.7224011421203613
Loss at iteration 150 : 0.9151029586791992
Loss at iteration 200 : 0.7531349658966064
Loss at iteration 250 : 0.6271340847015381
Loss at iteration 300 : 0.9401271939277649
Loss at iteration 350 : 0.6827829480171204
Loss at iteration 400 : 0.6059781312942505
Mean training loss eporch  47 :  0.6287313384659622
Loss at iteration 50 : 0.457640677690506
Loss at iteration 100 : 0.5658134818077087
Loss at iteration 150 : 0.5426149368286133
Loss at iteration 200 : 0.4134555757045746
Loss at iteration 250 : 0.9766390919685364
Loss at iteration 300 : 0.45517057180404663
Loss at iteration 350 : 0.3215916156768799
Loss at iteration 400 : 0.4064784348011017
Mean training loss eporch  48 :  0.6278433153247085
Loss at iteration 50 : 0.6231455206871033
Loss at iteration 100 : 0.7179133892059326
Loss at iteration 150 : 0.7069047689437866
Loss at iteration 200 : 1.098900556564331
Loss at iteration 250 : 0.3958075940608978
Loss at iteration 300 : 0.3871014714241028
Loss at iteration 350 : 0.5159682035446167
Loss at iteration 400 : 0.7388832569122314
Mean training loss eporch  49 :  0.6274669489451588
Loss at iteration 50 : 1.2525064945220947
Loss at iteration 100 : 1.077366590499878
Loss at iteration 150 : 0.4342675805091858
Loss at iteration 200 : 1.3734493255615234
Loss at iteration 250 : 0.9653448462486267
Loss at iteration 300 : 0.630386233329773
Loss at iteration 350 : 0.6075353622436523
Loss at iteration 400 : 0.9412100911140442
Mean training loss eporch  50 :  0.6264519317601829
Loss at iteration 50 : 0.7009360790252686
Loss at iteration 100 : 0.6849313974380493
Loss at iteration 150 : 0.5684468746185303
Loss at iteration 200 : 0.8987691402435303
Loss at iteration 250 : 0.4627312123775482
Loss at iteration 300 : 1.0675724744796753
Loss at iteration 350 : 0.5275936126708984
Loss at iteration 400 : 0.4835912883281708
Mean training loss eporch  51 :  0.6316157705046136
Loss at iteration 50 : 0.7750029563903809
Loss at iteration 100 : 0.5522557497024536
Loss at iteration 150 : 0.776247501373291
Loss at iteration 200 : 0.2772030234336853
Loss at iteration 250 : 0.4751579761505127
Loss at iteration 300 : 1.0442438125610352
Loss at iteration 350 : 0.8280168771743774
Loss at iteration 400 : 0.4359305202960968
Mean training loss eporch  52 :  0.626303939441127
Loss at iteration 50 : 0.5309196710586548
Loss at iteration 100 : 0.2732802927494049
Loss at iteration 150 : 0.2773052453994751
Loss at iteration 200 : 0.5481066703796387
Loss at iteration 250 : 0.4909035265445709
Loss at iteration 300 : 0.41932451725006104
Loss at iteration 350 : 0.409387469291687
Loss at iteration 400 : 1.2861027717590332
Mean training loss eporch  53 :  0.6317334594747945
Loss at iteration 50 : 1.0868027210235596
Loss at iteration 100 : 1.289801001548767
Loss at iteration 150 : 0.5489720106124878
Loss at iteration 200 : 0.6459633708000183
Loss at iteration 250 : 0.4337337017059326
Loss at iteration 300 : 0.5511205792427063
Loss at iteration 350 : 0.4745591878890991
Loss at iteration 400 : 0.5382413864135742
Mean training loss eporch  54 :  0.6254521419582345
Loss at iteration 50 : 0.7124205827713013
Loss at iteration 100 : 0.8467421531677246
Loss at iteration 150 : 0.8945573568344116
Loss at iteration 200 : 0.7776939868927002
Loss at iteration 250 : 0.5721365213394165
Loss at iteration 300 : 0.5601514577865601
Loss at iteration 350 : 0.3783619999885559
Loss at iteration 400 : 0.549506664276123
Mean training loss eporch  55 :  0.6259322867064733
Loss at iteration 50 : 0.6433804035186768
Loss at iteration 100 : 0.7284590005874634
Loss at iteration 150 : 0.40912926197052
Loss at iteration 200 : 0.8237940073013306
Loss at iteration 250 : 0.43375980854034424
Loss at iteration 300 : 0.6655644178390503
Loss at iteration 350 : 0.5446130037307739
Loss at iteration 400 : 0.2881896495819092
Mean training loss eporch  56 :  0.6254533168112216
Loss at iteration 50 : 0.7604753971099854
Loss at iteration 100 : 0.6118251085281372
Loss at iteration 150 : 1.0467162132263184
Loss at iteration 200 : 0.41932255029678345
Loss at iteration 250 : 0.563645601272583
Loss at iteration 300 : 0.8158413171768188
Loss at iteration 350 : 0.4025309085845947
Loss at iteration 400 : 0.7033277750015259
Mean training loss eporch  57 :  0.6246397890183958
Loss at iteration 50 : 0.6579367518424988
Loss at iteration 100 : 0.27993932366371155
Loss at iteration 150 : 0.34371209144592285
Loss at iteration 200 : 0.4562724232673645
Loss at iteration 250 : 0.3531094193458557
Loss at iteration 300 : 0.45102930068969727
Loss at iteration 350 : 0.8717163801193237
Loss at iteration 400 : 0.6285148859024048
Mean training loss eporch  58 :  0.6245244017764592
Loss at iteration 50 : 0.4304138123989105
Loss at iteration 100 : 0.8250705599784851
Loss at iteration 150 : 0.6856036186218262
Loss at iteration 200 : 0.9611403346061707
Loss at iteration 250 : 0.30694708228111267
Loss at iteration 300 : 0.3144208788871765
Loss at iteration 350 : 0.3226216435432434
Loss at iteration 400 : 0.6782506704330444
Mean training loss eporch  59 :  0.6241717719109604
Loss at iteration 50 : 0.7623035311698914
Loss at iteration 100 : 0.5943913459777832
Loss at iteration 150 : 0.4381546676158905
Loss at iteration 200 : 0.47360825538635254
Loss at iteration 250 : 0.2642673850059509
Loss at iteration 300 : 0.6980105638504028
Loss at iteration 350 : 0.910210371017456
Loss at iteration 400 : 0.2538471817970276
Mean training loss eporch  60 :  0.6239521358780262
Loss at iteration 50 : 0.7172269225120544
Loss at iteration 100 : 0.41001367568969727
Loss at iteration 150 : 0.5381808280944824
Loss at iteration 200 : 0.606657862663269
Loss at iteration 250 : 0.7718386054039001
Loss at iteration 300 : 0.4269457757472992
Loss at iteration 350 : 0.6131085753440857
Loss at iteration 400 : 0.6021144390106201
Mean training loss eporch  61 :  0.6245657423419269
Loss at iteration 50 : 0.9137294292449951
Loss at iteration 100 : 0.566009521484375
Loss at iteration 150 : 1.4653518199920654
Loss at iteration 200 : 0.8307650685310364
Loss at iteration 250 : 0.33949461579322815
Loss at iteration 300 : 0.431072473526001
Loss at iteration 350 : 0.563348114490509
Loss at iteration 400 : 0.3147004544734955
Mean training loss eporch  62 :  0.6229272113252649
Loss at iteration 50 : 1.025879144668579
Loss at iteration 100 : 0.5596644878387451
Loss at iteration 150 : 0.44322872161865234
Loss at iteration 200 : 0.2762490510940552
Loss at iteration 250 : 0.9594809412956238
Loss at iteration 300 : 0.46356552839279175
Loss at iteration 350 : 0.8282832503318787
Loss at iteration 400 : 0.7573778629302979
Mean training loss eporch  63 :  0.6236501253573349
Loss at iteration 50 : 1.0219740867614746
Loss at iteration 100 : 0.6685800552368164
Loss at iteration 150 : 0.6543278694152832
Loss at iteration 200 : 0.5534502267837524
Loss at iteration 250 : 0.6782407760620117
Loss at iteration 300 : 0.5335721373558044
Loss at iteration 350 : 0.4852333962917328
Loss at iteration 400 : 0.46135231852531433
Mean training loss eporch  64 :  0.6250841973742027
Loss at iteration 50 : 0.8061549067497253
Loss at iteration 100 : 1.5437817573547363
Loss at iteration 150 : 0.6645956635475159
Loss at iteration 200 : 0.578661322593689
Loss at iteration 250 : 0.7392297387123108
Loss at iteration 300 : 0.5883718729019165
Loss at iteration 350 : 0.2914615273475647
Loss at iteration 400 : 0.692660927772522
Mean training loss eporch  65 :  0.6229724938826711
Loss at iteration 50 : 0.4539496898651123
Loss at iteration 100 : 0.4961977005004883
Loss at iteration 150 : 0.5959067344665527
Loss at iteration 200 : 0.5672464370727539
Loss at iteration 250 : 0.6506248712539673
Loss at iteration 300 : 0.8768597841262817
Loss at iteration 350 : 0.7231503129005432
Loss at iteration 400 : 0.9510407447814941
Mean training loss eporch  66 :  0.622980232697164
Loss at iteration 50 : 0.8879314064979553
Loss at iteration 100 : 0.893774151802063
Loss at iteration 150 : 0.566716194152832
Loss at iteration 200 : 0.6182845234870911
Loss at iteration 250 : 0.3946866989135742
Loss at iteration 300 : 0.3121853172779083
Loss at iteration 350 : 0.8425394892692566
Loss at iteration 400 : 1.2715468406677246
Mean training loss eporch  67 :  0.6248706596916032
Loss at iteration 50 : 0.6497209072113037
Loss at iteration 100 : 0.406934529542923
Loss at iteration 150 : 0.748115062713623
Loss at iteration 200 : 0.3505062460899353
Loss at iteration 250 : 0.576123058795929
Loss at iteration 300 : 0.3549717366695404
Loss at iteration 350 : 1.0460879802703857
Loss at iteration 400 : 0.7424488663673401
Mean training loss eporch  68 :  0.6266667115848695
Loss at iteration 50 : 0.33994531631469727
Loss at iteration 100 : 0.3359209895133972
Loss at iteration 150 : 0.45655983686447144
Loss at iteration 200 : 0.6980634927749634
Loss at iteration 250 : 1.5763181447982788
Loss at iteration 300 : 0.5663217306137085
Loss at iteration 350 : 0.490336537361145
Loss at iteration 400 : 0.9979913234710693
Mean training loss eporch  69 :  0.6223617045066816
Loss at iteration 50 : 0.6499152183532715
Loss at iteration 100 : 0.6493963599205017
Loss at iteration 150 : 0.3866903781890869
Loss at iteration 200 : 0.6637575030326843
Loss at iteration 250 : 0.3796415328979492
Loss at iteration 300 : 0.6313221454620361
Loss at iteration 350 : 0.5101593732833862
Loss at iteration 400 : 0.5416296124458313
Mean training loss eporch  70 :  0.620810388984167
Loss at iteration 50 : 0.8815573453903198
Loss at iteration 100 : 0.4030982255935669
Loss at iteration 150 : 0.4252190589904785
Loss at iteration 200 : 0.30453208088874817
Loss at iteration 250 : 0.5276730060577393
Loss at iteration 300 : 0.705515444278717
Loss at iteration 350 : 0.7483326196670532
Loss at iteration 400 : 0.7778875827789307
Mean training loss eporch  71 :  0.6211468104623893
Loss at iteration 50 : 0.3767526149749756
Loss at iteration 100 : 0.39150428771972656
Loss at iteration 150 : 0.6852802038192749
Loss at iteration 200 : 0.9305081367492676
Loss at iteration 250 : 0.2753831744194031
Loss at iteration 300 : 0.41773027181625366
Loss at iteration 350 : 0.5258437991142273
Loss at iteration 400 : 0.8479098081588745
Mean training loss eporch  72 :  0.6213736335659241
Loss at iteration 50 : 1.2389124631881714
Loss at iteration 100 : 1.042999505996704
Loss at iteration 150 : 1.0510512590408325
Loss at iteration 200 : 0.6133211255073547
Loss at iteration 250 : 1.00193190574646
Loss at iteration 300 : 0.6648471355438232
Loss at iteration 350 : 0.5519572496414185
Loss at iteration 400 : 0.5213565826416016
Mean training loss eporch  73 :  0.6209567407054217
Loss at iteration 50 : 0.6138536930084229
Loss at iteration 100 : 0.4480447769165039
Loss at iteration 150 : 0.339734822511673
Loss at iteration 200 : 0.2819370627403259
Loss at iteration 250 : 0.33116278052330017
Loss at iteration 300 : 1.557590126991272
Loss at iteration 350 : 0.5240819454193115
Loss at iteration 400 : 0.5260622501373291
Mean training loss eporch  74 :  0.6204349364868194
Loss at iteration 50 : 0.6893789768218994
Loss at iteration 100 : 0.8932249546051025
Loss at iteration 150 : 0.33003848791122437
Loss at iteration 200 : 0.3676481246948242
Loss at iteration 250 : 0.3295018970966339
Loss at iteration 300 : 0.6825969219207764
Loss at iteration 350 : 0.26663410663604736
Loss at iteration 400 : 0.5694918632507324
Mean training loss eporch  75 :  0.6203913785216519
Loss at iteration 50 : 0.7698323726654053
Loss at iteration 100 : 1.0230917930603027
Loss at iteration 150 : 0.4659273624420166
Loss at iteration 200 : 0.5937713384628296
Loss at iteration 250 : 0.470317542552948
Loss at iteration 300 : 0.612652599811554
Loss at iteration 350 : 0.5857605934143066
Loss at iteration 400 : 0.7044174671173096
Mean training loss eporch  76 :  0.6199330534836102
Loss at iteration 50 : 0.9165288805961609
Loss at iteration 100 : 0.7009828090667725
Loss at iteration 150 : 0.4095420837402344
Loss at iteration 200 : 0.6725257635116577
Loss at iteration 250 : 0.3983721137046814
Loss at iteration 300 : 0.453372985124588
Loss at iteration 350 : 0.6447725892066956
Loss at iteration 400 : 0.8243948221206665
Mean training loss eporch  77 :  0.6195346018959321
Loss at iteration 50 : 0.340284526348114
Loss at iteration 100 : 0.3757261037826538
Loss at iteration 150 : 0.9550317525863647
Loss at iteration 200 : 0.2926989197731018
Loss at iteration 250 : 0.9020707607269287
Loss at iteration 300 : 0.31072378158569336
Loss at iteration 350 : 1.060968041419983
Loss at iteration 400 : 1.0014674663543701
Mean training loss eporch  78 :  0.6198874536584311
Loss at iteration 50 : 0.3877425789833069
Loss at iteration 100 : 0.4997604787349701
Loss at iteration 150 : 0.7453340888023376
Loss at iteration 200 : 0.4521605372428894
Loss at iteration 250 : 0.3773731589317322
Loss at iteration 300 : 0.6500146985054016
Loss at iteration 350 : 0.3819863498210907
Loss at iteration 400 : 0.4139457941055298
Mean training loss eporch  79 :  0.6203746972062663
Loss at iteration 50 : 0.3950750231742859
Loss at iteration 100 : 0.6038767695426941
Loss at iteration 150 : 0.34418579936027527
Loss at iteration 200 : 0.32668960094451904
Loss at iteration 250 : 0.7159156203269958
Loss at iteration 300 : 0.6977207660675049
Loss at iteration 350 : 0.7166387438774109
Loss at iteration 400 : 0.5791962146759033
Mean training loss eporch  80 :  0.6248001323286193
Loss at iteration 50 : 0.3236340284347534
Loss at iteration 100 : 0.6485624313354492
Loss at iteration 150 : 0.8015245795249939
Loss at iteration 200 : 0.4105228781700134
Loss at iteration 250 : 0.4310634136199951
Loss at iteration 300 : 0.9524973630905151
Loss at iteration 350 : 0.4268009662628174
Loss at iteration 400 : 0.36690253019332886
Mean training loss eporch  81 :  0.6212094173570385
Loss at iteration 50 : 1.2294833660125732
Loss at iteration 100 : 0.4239574074745178
Loss at iteration 150 : 0.3134303092956543
Loss at iteration 200 : 0.2971770167350769
Loss at iteration 250 : 0.532814621925354
Loss at iteration 300 : 0.7970947027206421
Loss at iteration 350 : 0.6615532636642456
Loss at iteration 400 : 0.9371036887168884
Mean training loss eporch  82 :  0.6192717578245386
Loss at iteration 50 : 0.5063053369522095
Loss at iteration 100 : 0.43040916323661804
Loss at iteration 150 : 0.624541163444519
Loss at iteration 200 : 1.2765074968338013
Loss at iteration 250 : 0.4602532386779785
Loss at iteration 300 : 0.6805686950683594
Loss at iteration 350 : 0.4521067142486572
Loss at iteration 400 : 0.3957652151584625
Mean training loss eporch  83 :  0.6202654591657121
Loss at iteration 50 : 0.5700714588165283
Loss at iteration 100 : 0.38989272713661194
Loss at iteration 150 : 0.5803189873695374
Loss at iteration 200 : 1.1503679752349854
Loss at iteration 250 : 0.8990496397018433
Loss at iteration 300 : 0.6851580142974854
Loss at iteration 350 : 0.6563090085983276
Loss at iteration 400 : 1.1041624546051025
Mean training loss eporch  84 :  0.6194182142214391
Loss at iteration 50 : 0.359155535697937
Loss at iteration 100 : 0.7436229586601257
Loss at iteration 150 : 0.43619227409362793
Loss at iteration 200 : 0.784744143486023
Loss at iteration 250 : 0.2553741931915283
Loss at iteration 300 : 0.6565386652946472
Loss at iteration 350 : 0.620589554309845
Loss at iteration 400 : 0.7942396402359009
Mean training loss eporch  85 :  0.618595332040915
Loss at iteration 50 : 0.8222677707672119
Loss at iteration 100 : 0.2973097562789917
Loss at iteration 150 : 0.5778599977493286
Loss at iteration 200 : 0.43015918135643005
Loss at iteration 250 : 0.5394195914268494
Loss at iteration 300 : 0.34790730476379395
Loss at iteration 350 : 1.0509223937988281
Loss at iteration 400 : 0.31441956758499146
Mean training loss eporch  86 :  0.6182623179838261
Loss at iteration 50 : 0.5288291573524475
Loss at iteration 100 : 0.278812050819397
Loss at iteration 150 : 1.1078438758850098
Loss at iteration 200 : 0.9091905951499939
Loss at iteration 250 : 0.3612220883369446
Loss at iteration 300 : 0.47324416041374207
Loss at iteration 350 : 0.8047359585762024
Loss at iteration 400 : 0.3597651720046997
Mean training loss eporch  87 :  0.6216845727554886
Loss at iteration 50 : 0.7729445099830627
Loss at iteration 100 : 0.6460895538330078
Loss at iteration 150 : 0.8232606649398804
Loss at iteration 200 : 0.5700632929801941
Loss at iteration 250 : 0.33923736214637756
Loss at iteration 300 : 0.8631207942962646
Loss at iteration 350 : 0.7030778527259827
Loss at iteration 400 : 0.7233545780181885
Mean training loss eporch  88 :  0.6179996954085046
Loss at iteration 50 : 0.9722914099693298
Loss at iteration 100 : 0.7419048547744751
Loss at iteration 150 : 0.3976914882659912
Loss at iteration 200 : 0.8386723399162292
Loss at iteration 250 : 0.37228479981422424
Loss at iteration 300 : 0.5072662830352783
Loss at iteration 350 : 1.207514762878418
Loss at iteration 400 : 0.6011186838150024
Mean training loss eporch  89 :  0.618156913651212
Loss at iteration 50 : 0.4676072895526886
Loss at iteration 100 : 0.652538537979126
Loss at iteration 150 : 0.30361032485961914
Loss at iteration 200 : 0.5604442358016968
Loss at iteration 250 : 0.8681347370147705
Loss at iteration 300 : 0.6070816516876221
Loss at iteration 350 : 0.42388486862182617
Loss at iteration 400 : 0.32708102464675903
Mean training loss eporch  90 :  0.6186936025595451
Loss at iteration 50 : 0.4052276015281677
Loss at iteration 100 : 0.505083441734314
Loss at iteration 150 : 0.533919095993042
Loss at iteration 200 : 0.2811378240585327
Loss at iteration 250 : 0.6667535901069641
Loss at iteration 300 : 0.8008735775947571
Loss at iteration 350 : 0.7342125773429871
Loss at iteration 400 : 0.9169989824295044
Mean training loss eporch  91 :  0.618042832699859
Loss at iteration 50 : 0.3715682029724121
Loss at iteration 100 : 1.2368371486663818
Loss at iteration 150 : 0.6551202535629272
Loss at iteration 200 : 0.9933983087539673
Loss at iteration 250 : 0.4099169373512268
Loss at iteration 300 : 0.826379656791687
Loss at iteration 350 : 0.5926230549812317
Loss at iteration 400 : 0.4441905617713928
Mean training loss eporch  92 :  0.6188574645484509
Loss at iteration 50 : 0.5001773238182068
Loss at iteration 100 : 0.3460870087146759
Loss at iteration 150 : 0.3579651713371277
Loss at iteration 200 : 0.644768238067627
Loss at iteration 250 : 0.5007297992706299
Loss at iteration 300 : 0.8136893510818481
Loss at iteration 350 : 0.7648021578788757
Loss at iteration 400 : 0.34637269377708435
Mean training loss eporch  93 :  0.6177793477950074
Loss at iteration 50 : 0.7551314234733582
Loss at iteration 100 : 0.7026504278182983
Loss at iteration 150 : 0.7522622346878052
Loss at iteration 200 : 0.5483502745628357
Loss at iteration 250 : 0.538528561592102
Loss at iteration 300 : 0.46469128131866455
Loss at iteration 350 : 0.5002920627593994
Loss at iteration 400 : 0.3336280584335327
Mean training loss eporch  94 :  0.6171307661006803
Loss at iteration 50 : 0.6248829364776611
Loss at iteration 100 : 0.3919854462146759
Loss at iteration 150 : 0.3458959460258484
Loss at iteration 200 : 0.9058220386505127
Loss at iteration 250 : 0.4962674081325531
Loss at iteration 300 : 0.5104824900627136
Loss at iteration 350 : 0.6825270056724548
Loss at iteration 400 : 1.0903035402297974
Mean training loss eporch  95 :  0.6171739815061937
Loss at iteration 50 : 0.8071615695953369
Loss at iteration 100 : 0.7413578033447266
Loss at iteration 150 : 0.6005749106407166
Loss at iteration 200 : 0.4488878846168518
Loss at iteration 250 : 0.9255739450454712
Loss at iteration 300 : 0.5220596194267273
Loss at iteration 350 : 0.4661616086959839
Loss at iteration 400 : 0.39581298828125
Mean training loss eporch  96 :  0.6162847879555727
Loss at iteration 50 : 0.6323590874671936
Loss at iteration 100 : 0.7160077095031738
Loss at iteration 150 : 1.0079598426818848
Loss at iteration 200 : 0.5301498770713806
Loss at iteration 250 : 0.686743974685669
Loss at iteration 300 : 0.5791428685188293
Loss at iteration 350 : 0.4147040843963623
Loss at iteration 400 : 0.5974001884460449
Mean training loss eporch  97 :  0.6175970128432517
Loss at iteration 50 : 0.38855940103530884
Loss at iteration 100 : 0.649767279624939
Loss at iteration 150 : 0.655429482460022
Loss at iteration 200 : 0.4335730969905853
Loss at iteration 250 : 0.43224048614501953
Loss at iteration 300 : 0.44572633504867554
Loss at iteration 350 : 0.5031933784484863
Loss at iteration 400 : 0.6603085398674011
Mean training loss eporch  98 :  0.6209295875154803
Loss at iteration 50 : 0.3027117848396301
Loss at iteration 100 : 0.3211190700531006
Loss at iteration 150 : 0.6103388667106628
Loss at iteration 200 : 0.47443267703056335
Loss at iteration 250 : 0.742715060710907
Loss at iteration 300 : 0.7868257761001587
Loss at iteration 350 : 1.0583240985870361
Loss at iteration 400 : 0.31124839186668396
Mean training loss eporch  99 :  0.6171258462450964
Loss at iteration 50 : 0.6071556210517883
Loss at iteration 100 : 0.6224781274795532
Loss at iteration 150 : 0.5023782253265381
Loss at iteration 200 : 0.8077090978622437
Loss at iteration 250 : 0.31004899740219116
Loss at iteration 300 : 0.2757648825645447
Loss at iteration 350 : 1.0092644691467285
Loss at iteration 400 : 0.7524619102478027
Mean training loss eporch  100 :  0.6155393601399366
Loss at iteration 50 : 0.5420801043510437
Loss at iteration 100 : 0.6974162459373474
Loss at iteration 150 : 0.46042829751968384
Loss at iteration 200 : 0.6711196899414062
Loss at iteration 250 : 1.1070911884307861
Loss at iteration 300 : 0.5345512628555298
Loss at iteration 350 : 0.9482768774032593
Loss at iteration 400 : 0.511713445186615
Mean training loss eporch  101 :  0.6160259927067522
Loss at iteration 50 : 0.6457623243331909
Loss at iteration 100 : 0.34412240982055664
Loss at iteration 150 : 0.6725547313690186
Loss at iteration 200 : 0.9888021945953369
Loss at iteration 250 : 0.5392530560493469
Loss at iteration 300 : 0.6337754726409912
Loss at iteration 350 : 0.697629451751709
Loss at iteration 400 : 0.7996426224708557
Mean training loss eporch  102 :  0.6215299002391875
Loss at iteration 50 : 0.7427449226379395
Loss at iteration 100 : 0.7905888557434082
Loss at iteration 150 : 0.8077578544616699
Loss at iteration 200 : 0.6108294725418091
Loss at iteration 250 : 0.9856606721878052
Loss at iteration 300 : 0.6455441117286682
Loss at iteration 350 : 0.270600825548172
Loss at iteration 400 : 0.4391094446182251
Mean training loss eporch  103 :  0.6160787316415075
Loss at iteration 50 : 0.5079162120819092
Loss at iteration 100 : 0.7802876830101013
Loss at iteration 150 : 0.6298627853393555
Loss at iteration 200 : 0.6427162885665894
Loss at iteration 250 : 0.5567731857299805
Loss at iteration 300 : 0.34171199798583984
Loss at iteration 350 : 1.3842556476593018
Loss at iteration 400 : 0.32380276918411255
Mean training loss eporch  104 :  0.61568542633356
Loss at iteration 50 : 0.9516326189041138
Loss at iteration 100 : 0.7867658734321594
Loss at iteration 150 : 0.46622589230537415
Loss at iteration 200 : 0.4668169319629669
Loss at iteration 250 : 0.3693728744983673
Loss at iteration 300 : 0.42831575870513916
Loss at iteration 350 : 0.2595892548561096
Loss at iteration 400 : 0.31929004192352295
Mean training loss eporch  105 :  0.6205687260280276
Loss at iteration 50 : 0.4698313772678375
Loss at iteration 100 : 0.31166166067123413
Loss at iteration 150 : 0.4472554922103882
Loss at iteration 200 : 0.28085857629776
Loss at iteration 250 : 0.5850263833999634
Loss at iteration 300 : 0.9010741114616394
Loss at iteration 350 : 0.6464622616767883
Loss at iteration 400 : 0.9425931572914124
Mean training loss eporch  106 :  0.615476170128771
Loss at iteration 50 : 0.6148386001586914
Loss at iteration 100 : 0.5988135933876038
Loss at iteration 150 : 0.8411873579025269
Loss at iteration 200 : 0.4012625217437744
Loss at iteration 250 : 0.3475862145423889
Loss at iteration 300 : 0.5809048414230347
Loss at iteration 350 : 0.6081562042236328
Loss at iteration 400 : 0.895707368850708
Mean training loss eporch  107 :  0.6203576187715937
Loss at iteration 50 : 0.9949712753295898
Loss at iteration 100 : 0.3715892434120178
Loss at iteration 150 : 0.3330816626548767
Loss at iteration 200 : 0.6785550117492676
Loss at iteration 250 : 0.5228209495544434
Loss at iteration 300 : 0.37021201848983765
Loss at iteration 350 : 0.4012654423713684
Loss at iteration 400 : 0.3111761808395386
Mean training loss eporch  108 :  0.614604532100561
Loss at iteration 50 : 1.061477780342102
Loss at iteration 100 : 0.693625271320343
Loss at iteration 150 : 0.4686356782913208
Loss at iteration 200 : 0.6959480047225952
Loss at iteration 250 : 0.7551757097244263
Loss at iteration 300 : 0.26493555307388306
Loss at iteration 350 : 0.3597070872783661
Loss at iteration 400 : 1.4846916198730469
Mean training loss eporch  109 :  0.617457685783305
Loss at iteration 50 : 0.6161192655563354
Loss at iteration 100 : 0.7518389225006104
Loss at iteration 150 : 0.357555627822876
Loss at iteration 200 : 0.3725067377090454
Loss at iteration 250 : 0.7979391813278198
Loss at iteration 300 : 0.9805781841278076
Loss at iteration 350 : 0.6413477659225464
Loss at iteration 400 : 0.499752402305603
Mean training loss eporch  110 :  0.614258181821605
Loss at iteration 50 : 0.8737413883209229
Loss at iteration 100 : 0.3813284635543823
Loss at iteration 150 : 0.29291945695877075
Loss at iteration 200 : 0.4631456732749939
Loss at iteration 250 : 0.8257917165756226
Loss at iteration 300 : 0.3804861903190613
Loss at iteration 350 : 0.370419442653656
Loss at iteration 400 : 0.4448280334472656
Mean training loss eporch  111 :  0.6145758966293036
Loss at iteration 50 : 0.33339059352874756
Loss at iteration 100 : 0.322440505027771
Loss at iteration 150 : 0.5678344964981079
Loss at iteration 200 : 0.771538496017456
Loss at iteration 250 : 0.91466224193573
Loss at iteration 300 : 0.6437463164329529
Loss at iteration 350 : 0.6128940582275391
Loss at iteration 400 : 0.5419269800186157
Mean training loss eporch  112 :  0.6144671837043335
Loss at iteration 50 : 0.5600000619888306
Loss at iteration 100 : 0.7029242515563965
Loss at iteration 150 : 0.694378137588501
Loss at iteration 200 : 0.4759543240070343
Loss at iteration 250 : 0.887306809425354
Loss at iteration 300 : 0.31605106592178345
Loss at iteration 350 : 0.35460370779037476
Loss at iteration 400 : 0.665903627872467
Mean training loss eporch  113 :  0.6146522840576856
Loss at iteration 50 : 1.415360689163208
Loss at iteration 100 : 0.8552408218383789
Loss at iteration 150 : 0.31974828243255615
Loss at iteration 200 : 0.9437316060066223
Loss at iteration 250 : 0.6830999851226807
Loss at iteration 300 : 0.3563654124736786
Loss at iteration 350 : 0.7408560514450073
Loss at iteration 400 : 0.42630693316459656
Mean training loss eporch  114 :  0.6141843617697468
Loss at iteration 50 : 0.4682813882827759
Loss at iteration 100 : 0.5184246301651001
Loss at iteration 150 : 0.7436187863349915
Loss at iteration 200 : 0.41303911805152893
Loss at iteration 250 : 0.7385332584381104
Loss at iteration 300 : 0.6129736304283142
Loss at iteration 350 : 0.6293776035308838
Loss at iteration 400 : 0.7922766208648682
Mean training loss eporch  115 :  0.6145977279998261
Loss at iteration 50 : 0.36163195967674255
Loss at iteration 100 : 0.8175373077392578
Loss at iteration 150 : 0.8591974973678589
Loss at iteration 200 : 0.5552592277526855
Loss at iteration 250 : 0.5415679216384888
Loss at iteration 300 : 0.49256742000579834
Loss at iteration 350 : 0.5332951545715332
Loss at iteration 400 : 0.7271649837493896
Mean training loss eporch  116 :  0.6151137922978187
Loss at iteration 50 : 0.2397298812866211
Loss at iteration 100 : 0.5076912641525269
Loss at iteration 150 : 0.9297059774398804
Loss at iteration 200 : 0.29588255286216736
Loss at iteration 250 : 0.49156272411346436
Loss at iteration 300 : 0.3984072208404541
Loss at iteration 350 : 0.27042800188064575
Loss at iteration 400 : 0.5288913249969482
Mean training loss eporch  117 :  0.6151824332272525
Loss at iteration 50 : 0.8125143051147461
Loss at iteration 100 : 0.9145638346672058
Loss at iteration 150 : 0.6753826141357422
Loss at iteration 200 : 0.5407269597053528
Loss at iteration 250 : 0.8277319073677063
Loss at iteration 300 : 0.7777443528175354
Loss at iteration 350 : 0.5272682905197144
Loss at iteration 400 : 1.0140295028686523
Mean training loss eporch  118 :  0.614214351225327
Loss at iteration 50 : 0.6312978863716125
Loss at iteration 100 : 0.7445082068443298
Loss at iteration 150 : 0.4320881962776184
Loss at iteration 200 : 0.3474089801311493
Loss at iteration 250 : 0.6870986223220825
Loss at iteration 300 : 0.9575016498565674
Loss at iteration 350 : 0.6989574432373047
Loss at iteration 400 : 0.4458421468734741
Mean training loss eporch  119 :  0.6135317909076075
Loss at iteration 50 : 0.9385937452316284
Loss at iteration 100 : 0.37025684118270874
Loss at iteration 150 : 0.5512170195579529
Loss at iteration 200 : 0.6298173666000366
Loss at iteration 250 : 1.4766590595245361
Loss at iteration 300 : 0.7113873958587646
Loss at iteration 350 : 1.5088140964508057
Loss at iteration 400 : 0.4191514551639557
Mean training loss eporch  120 :  0.6170855379358535
Loss at iteration 50 : 0.5881577730178833
Loss at iteration 100 : 0.5180989503860474
Loss at iteration 150 : 0.6347426176071167
Loss at iteration 200 : 1.2056857347488403
Loss at iteration 250 : 0.3600546717643738
Loss at iteration 300 : 1.0201456546783447
Loss at iteration 350 : 0.6453964710235596
Loss at iteration 400 : 0.8005136251449585
Mean training loss eporch  121 :  0.6156557504705784
Loss at iteration 50 : 0.7716448307037354
Loss at iteration 100 : 0.4702286720275879
Loss at iteration 150 : 0.46863651275634766
Loss at iteration 200 : 0.6510581970214844
Loss at iteration 250 : 0.34427008032798767
Loss at iteration 300 : 0.7428064942359924
Loss at iteration 350 : 0.6769990921020508
Loss at iteration 400 : 0.40169715881347656
Mean training loss eporch  122 :  0.6136830474830529
Loss at iteration 50 : 0.27651447057724
Loss at iteration 100 : 0.7558109760284424
Loss at iteration 150 : 1.0644276142120361
Loss at iteration 200 : 0.6196669340133667
Loss at iteration 250 : 0.3843056857585907
Loss at iteration 300 : 0.7173839211463928
Loss at iteration 350 : 1.4529800415039062
Loss at iteration 400 : 0.8435678482055664
Mean training loss eporch  123 :  0.6125824326490608
Loss at iteration 50 : 0.8702115416526794
Loss at iteration 100 : 0.6077411770820618
Loss at iteration 150 : 0.4964147210121155
Loss at iteration 200 : 0.8705345392227173
Loss at iteration 250 : 0.882850170135498
Loss at iteration 300 : 0.6441406011581421
Loss at iteration 350 : 1.1982810497283936
Loss at iteration 400 : 0.3874032199382782
Mean training loss eporch  124 :  0.6128363504070338
Loss at iteration 50 : 0.5325067639350891
Loss at iteration 100 : 0.7640066146850586
Loss at iteration 150 : 0.755595326423645
Loss at iteration 200 : 0.3696781098842621
Loss at iteration 250 : 0.5138919353485107
Loss at iteration 300 : 0.2154587209224701
Loss at iteration 350 : 0.6177036762237549
Loss at iteration 400 : 1.3813376426696777
Mean training loss eporch  125 :  0.6124781513494761
Loss at iteration 50 : 0.5692521929740906
Loss at iteration 100 : 0.5482479333877563
Loss at iteration 150 : 0.5209112167358398
Loss at iteration 200 : 0.6870815753936768
Loss at iteration 250 : 0.31390705704689026
Loss at iteration 300 : 0.3111540675163269
Loss at iteration 350 : 0.5494776964187622
Loss at iteration 400 : 0.4954185485839844
Mean training loss eporch  126 :  0.6125423973050352
Loss at iteration 50 : 0.46956688165664673
Loss at iteration 100 : 0.3933354616165161
Loss at iteration 150 : 0.6181063652038574
Loss at iteration 200 : 0.6294469237327576
Loss at iteration 250 : 0.3445025086402893
Loss at iteration 300 : 0.7201510071754456
Loss at iteration 350 : 0.35221055150032043
Loss at iteration 400 : 0.6891419291496277
Mean training loss eporch  127 :  0.612397821461406
Loss at iteration 50 : 0.6855698823928833
Loss at iteration 100 : 0.38209813833236694
Loss at iteration 150 : 0.3347734212875366
Loss at iteration 200 : 0.5737988352775574
Loss at iteration 250 : 0.39003023505210876
Loss at iteration 300 : 0.33725816011428833
Loss at iteration 350 : 0.6481980681419373
Loss at iteration 400 : 0.477044016122818
Mean training loss eporch  128 :  0.6138500534165066
Loss at iteration 50 : 0.44032683968544006
Loss at iteration 100 : 0.7326797246932983
Loss at iteration 150 : 0.27325713634490967
Loss at iteration 200 : 0.32770100235939026
Loss at iteration 250 : 0.33068495988845825
Loss at iteration 300 : 0.2080359309911728
Loss at iteration 350 : 1.246600866317749
Loss at iteration 400 : 0.273975133895874
Mean training loss eporch  129 :  0.6124120436842666
Loss at iteration 50 : 0.595852255821228
Loss at iteration 100 : 0.2820293605327606
Loss at iteration 150 : 0.5909804105758667
Loss at iteration 200 : 0.5931758880615234
Loss at iteration 250 : 0.7435314655303955
Loss at iteration 300 : 0.6486549377441406
Loss at iteration 350 : 0.3315880298614502
Loss at iteration 400 : 0.42974013090133667
Mean training loss eporch  130 :  0.6118458814317603
Loss at iteration 50 : 0.5156314373016357
Loss at iteration 100 : 0.31305304169654846
Loss at iteration 150 : 0.8302228450775146
Loss at iteration 200 : 0.8407914638519287
Loss at iteration 250 : 0.3277641534805298
Loss at iteration 300 : 0.4222795069217682
Loss at iteration 350 : 0.6995722651481628
Loss at iteration 400 : 0.41757792234420776
Mean training loss eporch  131 :  0.6129801762478234
Loss at iteration 50 : 1.189537525177002
Loss at iteration 100 : 1.1530228853225708
Loss at iteration 150 : 0.6260988712310791
Loss at iteration 200 : 0.6462147235870361
Loss at iteration 250 : 0.2975201904773712
Loss at iteration 300 : 0.878220796585083
Loss at iteration 350 : 0.2654649019241333
Loss at iteration 400 : 0.3528386354446411
Mean training loss eporch  132 :  0.6118386444623161
Loss at iteration 50 : 0.4757049083709717
Loss at iteration 100 : 0.4875020980834961
Loss at iteration 150 : 0.30913200974464417
Loss at iteration 200 : 0.7869070172309875
Loss at iteration 250 : 0.60507732629776
Loss at iteration 300 : 1.0251035690307617
Loss at iteration 350 : 0.3057211637496948
Loss at iteration 400 : 0.5831595063209534
Mean training loss eporch  133 :  0.6122989781167475
Loss at iteration 50 : 0.7877156734466553
Loss at iteration 100 : 1.0480284690856934
Loss at iteration 150 : 0.33024337887763977
Loss at iteration 200 : 0.294786274433136
Loss at iteration 250 : 0.46019721031188965
Loss at iteration 300 : 0.6068714261054993
Loss at iteration 350 : 0.3520779609680176
Loss at iteration 400 : 0.8611488342285156
Mean training loss eporch  134 :  0.6116566784312373
Loss at iteration 50 : 0.6021339893341064
Loss at iteration 100 : 0.3331497311592102
Loss at iteration 150 : 0.5450867414474487
Loss at iteration 200 : 0.5412939190864563
Loss at iteration 250 : 0.6275495290756226
Loss at iteration 300 : 1.0433045625686646
Loss at iteration 350 : 0.7059714794158936
Loss at iteration 400 : 0.5131687521934509
Mean training loss eporch  135 :  0.6110454924705317
Loss at iteration 50 : 0.9972375631332397
Loss at iteration 100 : 0.7945395112037659
Loss at iteration 150 : 1.0375616550445557
Loss at iteration 200 : 0.7388137578964233
Loss at iteration 250 : 0.5720986127853394
Loss at iteration 300 : 0.5093822479248047
Loss at iteration 350 : 0.632299542427063
Loss at iteration 400 : 0.33527448773384094
Mean training loss eporch  136 :  0.611281394858264
Loss at iteration 50 : 0.464538037776947
Loss at iteration 100 : 0.4207381010055542
Loss at iteration 150 : 0.5319532155990601
Loss at iteration 200 : 0.2862740755081177
Loss at iteration 250 : 0.45668068528175354
Loss at iteration 300 : 0.5539177656173706
Loss at iteration 350 : 0.7943449020385742
Loss at iteration 400 : 0.26533225178718567
Mean training loss eporch  137 :  0.611703422371582
Loss at iteration 50 : 1.3353747129440308
Loss at iteration 100 : 0.3231780529022217
Loss at iteration 150 : 0.8286469578742981
Loss at iteration 200 : 0.47023361921310425
Loss at iteration 250 : 0.35699546337127686
Loss at iteration 300 : 0.6840258836746216
Loss at iteration 350 : 0.3229078948497772
Loss at iteration 400 : 0.4434058666229248
Mean training loss eporch  138 :  0.6114757831454811
Loss at iteration 50 : 0.8737104535102844
Loss at iteration 100 : 0.39160898327827454
Loss at iteration 150 : 1.0766634941101074
Loss at iteration 200 : 0.6627994775772095
Loss at iteration 250 : 0.28766658902168274
Loss at iteration 300 : 0.6558358073234558
Loss at iteration 350 : 0.4464394152164459
Loss at iteration 400 : 0.27170443534851074
Mean training loss eporch  139 :  0.6112984220409607
Loss at iteration 50 : 0.8320761919021606
Loss at iteration 100 : 0.45576947927474976
Loss at iteration 150 : 0.5115124583244324
Loss at iteration 200 : 0.5419914126396179
Loss at iteration 250 : 0.8316477537155151
Loss at iteration 300 : 1.0931429862976074
Loss at iteration 350 : 0.5820202231407166
Loss at iteration 400 : 0.8275074362754822
Mean training loss eporch  140 :  0.6131387149472408
Loss at iteration 50 : 0.9289424419403076
Loss at iteration 100 : 0.7302634716033936
Loss at iteration 150 : 0.6266040802001953
Loss at iteration 200 : 0.3873445987701416
Loss at iteration 250 : 0.6506978273391724
Loss at iteration 300 : 0.48324912786483765
Loss at iteration 350 : 1.1737170219421387
Loss at iteration 400 : 0.48219406604766846
Mean training loss eporch  141 :  0.6119738751730042
Loss at iteration 50 : 0.6895455121994019
Loss at iteration 100 : 0.47087356448173523
Loss at iteration 150 : 0.4057009816169739
Loss at iteration 200 : 0.36754852533340454
Loss at iteration 250 : 0.641685962677002
Loss at iteration 300 : 0.3128010928630829
Loss at iteration 350 : 0.26283538341522217
Loss at iteration 400 : 0.3860182464122772
Mean training loss eporch  142 :  0.6106282871868044
Loss at iteration 50 : 0.6059454679489136
Loss at iteration 100 : 0.4280608594417572
Loss at iteration 150 : 0.24144905805587769
Loss at iteration 200 : 0.33359330892562866
Loss at iteration 250 : 1.164391279220581
Loss at iteration 300 : 0.8025112152099609
Loss at iteration 350 : 0.9385683536529541
Loss at iteration 400 : 0.30027735233306885
Mean training loss eporch  143 :  0.6109295866280928
Loss at iteration 50 : 0.24162361025810242
Loss at iteration 100 : 0.5850645899772644
Loss at iteration 150 : 0.5335362553596497
Loss at iteration 200 : 0.5131638050079346
Loss at iteration 250 : 0.3924565315246582
Loss at iteration 300 : 0.489929735660553
Loss at iteration 350 : 0.4285281300544739
Loss at iteration 400 : 1.0720726251602173
Mean training loss eporch  144 :  0.6111635120059342
Loss at iteration 50 : 0.5398582220077515
Loss at iteration 100 : 0.5117644667625427
Loss at iteration 150 : 0.7883204817771912
Loss at iteration 200 : 0.35126793384552
Loss at iteration 250 : 0.6033610105514526
Loss at iteration 300 : 0.6321858763694763
Loss at iteration 350 : 0.6138870716094971
Loss at iteration 400 : 1.1358202695846558
Mean training loss eporch  145 :  0.6104663561678788
Loss at iteration 50 : 0.34179243445396423
Loss at iteration 100 : 0.3395109474658966
Loss at iteration 150 : 0.3660610318183899
Loss at iteration 200 : 0.8620189428329468
Loss at iteration 250 : 0.6645359396934509
Loss at iteration 300 : 0.30422988533973694
Loss at iteration 350 : 0.40478527545928955
Loss at iteration 400 : 0.7222765684127808
Mean training loss eporch  146 :  0.6158988319103493
Loss at iteration 50 : 0.7575584650039673
Loss at iteration 100 : 0.5788190364837646
Loss at iteration 150 : 0.454941987991333
Loss at iteration 200 : 0.6581695079803467
Loss at iteration 250 : 0.3894592523574829
Loss at iteration 300 : 0.9069890975952148
Loss at iteration 350 : 0.4977041482925415
Loss at iteration 400 : 0.771704912185669
Mean training loss eporch  147 :  0.6106218370554694
Loss at iteration 50 : 0.5128927230834961
Loss at iteration 100 : 0.6393476724624634
Loss at iteration 150 : 0.7450036406517029
Loss at iteration 200 : 0.48255255818367004
Loss at iteration 250 : 0.24156183004379272
Loss at iteration 300 : 0.4267072081565857
Loss at iteration 350 : 0.9452338814735413
Loss at iteration 400 : 0.32080909609794617
Mean training loss eporch  148 :  0.6098123567989054
Loss at iteration 50 : 1.5605838298797607
Loss at iteration 100 : 1.053607702255249
Loss at iteration 150 : 0.5887064933776855
Loss at iteration 200 : 1.5040614604949951
Loss at iteration 250 : 0.9566318392753601
Loss at iteration 300 : 0.5140613317489624
Loss at iteration 350 : 0.7364500761032104
Loss at iteration 400 : 0.4069942235946655
Mean training loss eporch  149 :  0.6103686619499874
Loss at iteration 50 : 0.373762845993042
Loss at iteration 100 : 0.7090151309967041
Loss at iteration 150 : 0.4070834219455719
Loss at iteration 200 : 0.5819529294967651
Loss at iteration 250 : 0.39630043506622314
Loss at iteration 300 : 0.6504684090614319
Loss at iteration 350 : 0.46930307149887085
Loss at iteration 400 : 0.48456335067749023
Mean training loss eporch  150 :  0.6094564304023046
Loss at iteration 50 : 0.37844282388687134
Loss at iteration 100 : 0.7350345849990845
Loss at iteration 150 : 0.8546887636184692
Loss at iteration 200 : 0.7971625924110413
Loss at iteration 250 : 0.8448858261108398
Loss at iteration 300 : 0.6617931127548218
Loss at iteration 350 : 0.5649957656860352
Loss at iteration 400 : 0.49809539318084717
Mean training loss eporch  151 :  0.6111540786688103
Loss at iteration 50 : 0.44528135657310486
Loss at iteration 100 : 0.46032145619392395
Loss at iteration 150 : 0.29522639513015747
Loss at iteration 200 : 1.0115666389465332
Loss at iteration 250 : 0.6274920701980591
Loss at iteration 300 : 1.1947221755981445
Loss at iteration 350 : 0.6603013873100281
Loss at iteration 400 : 0.6361217498779297
Mean training loss eporch  152 :  0.6104327102479913
Loss at iteration 50 : 0.3177635073661804
Loss at iteration 100 : 0.6287534236907959
Loss at iteration 150 : 0.40444523096084595
Loss at iteration 200 : 0.4123860001564026
Loss at iteration 250 : 0.5297495722770691
Loss at iteration 300 : 0.31869393587112427
Loss at iteration 350 : 0.370930552482605
Loss at iteration 400 : 0.8611332178115845
Mean training loss eporch  153 :  0.6096036863834868
Loss at iteration 50 : 0.6228477954864502
Loss at iteration 100 : 0.3912849426269531
Loss at iteration 150 : 0.7027613520622253
Loss at iteration 200 : 0.6431680917739868
Loss at iteration 250 : 0.8116562366485596
Loss at iteration 300 : 0.7071699500083923
Loss at iteration 350 : 0.9608914256095886
Loss at iteration 400 : 0.7449452877044678
Mean training loss eporch  154 :  0.6097542179138671
Loss at iteration 50 : 0.5300823450088501
Loss at iteration 100 : 0.3447873592376709
Loss at iteration 150 : 0.4065644145011902
Loss at iteration 200 : 0.5242824554443359
Loss at iteration 250 : 0.5237415432929993
Loss at iteration 300 : 0.8127128481864929
Loss at iteration 350 : 0.7522035241127014
Loss at iteration 400 : 0.6330999732017517
Mean training loss eporch  155 :  0.6103122841416453
Loss at iteration 50 : 0.40362367033958435
Loss at iteration 100 : 0.3977653980255127
Loss at iteration 150 : 0.36867281794548035
Loss at iteration 200 : 0.6628902554512024
Loss at iteration 250 : 0.7895494699478149
Loss at iteration 300 : 0.38921982049942017
Loss at iteration 350 : 0.34538358449935913
Loss at iteration 400 : 0.23045438528060913
Mean training loss eporch  156 :  0.6137824741781025
Loss at iteration 50 : 0.5176862478256226
Loss at iteration 100 : 0.673258900642395
Loss at iteration 150 : 0.6233202815055847
Loss at iteration 200 : 0.8425561785697937
Loss at iteration 250 : 0.6500247716903687
Loss at iteration 300 : 0.4493834376335144
Loss at iteration 350 : 0.5628458261489868
Loss at iteration 400 : 0.6608853936195374
Mean training loss eporch  157 :  0.6098980892599966
Loss at iteration 50 : 0.6798380613327026
Loss at iteration 100 : 0.4430830180644989
Loss at iteration 150 : 1.4582998752593994
Loss at iteration 200 : 0.5810813903808594
Loss at iteration 250 : 0.756693422794342
Loss at iteration 300 : 0.5300555229187012
Loss at iteration 350 : 0.2350234091281891
Loss at iteration 400 : 0.6609300374984741
Mean training loss eporch  158 :  0.6089255271872063
Loss at iteration 50 : 0.32587242126464844
Loss at iteration 100 : 0.5960832238197327
Loss at iteration 150 : 0.6938279271125793
Loss at iteration 200 : 0.9841545820236206
Loss at iteration 250 : 0.27396705746650696
Loss at iteration 300 : 0.2867598235607147
Loss at iteration 350 : 0.4971509277820587
Loss at iteration 400 : 0.8020641803741455
Mean training loss eporch  159 :  0.612132816905398
Loss at iteration 50 : 0.7006234526634216
Loss at iteration 100 : 0.39946016669273376
Loss at iteration 150 : 0.4574066996574402
Loss at iteration 200 : 0.3769930601119995
Loss at iteration 250 : 0.8595890998840332
Loss at iteration 300 : 1.538253903388977
Loss at iteration 350 : 0.7400267720222473
Loss at iteration 400 : 1.2413227558135986
Mean training loss eporch  160 :  0.6097102011198955
Loss at iteration 50 : 0.6785304546356201
Loss at iteration 100 : 0.5542025566101074
Loss at iteration 150 : 1.1186017990112305
Loss at iteration 200 : 0.7355363368988037
Loss at iteration 250 : 0.5988439917564392
Loss at iteration 300 : 0.7870074510574341
Loss at iteration 350 : 0.28067654371261597
Loss at iteration 400 : 0.36237192153930664
Mean training loss eporch  161 :  0.6085494090155635
Loss at iteration 50 : 0.6295222640037537
Loss at iteration 100 : 0.619265615940094
Loss at iteration 150 : 0.8739467263221741
Loss at iteration 200 : 0.812583863735199
Loss at iteration 250 : 0.8172812461853027
Loss at iteration 300 : 0.844173789024353
Loss at iteration 350 : 0.4266607165336609
Loss at iteration 400 : 0.695773720741272
Mean training loss eporch  162 :  0.6095660516633047
Loss at iteration 50 : 0.5498213171958923
Loss at iteration 100 : 0.4242103099822998
Loss at iteration 150 : 0.4124276638031006
Loss at iteration 200 : 0.7819947004318237
Loss at iteration 250 : 0.5715370178222656
Loss at iteration 300 : 0.31860923767089844
Loss at iteration 350 : 0.6085097193717957
Loss at iteration 400 : 0.9556660652160645
Mean training loss eporch  163 :  0.6091422128570454
Loss at iteration 50 : 0.7013850212097168
Loss at iteration 100 : 0.5815238952636719
Loss at iteration 150 : 0.404083251953125
Loss at iteration 200 : 0.4872377812862396
Loss at iteration 250 : 1.148315668106079
Loss at iteration 300 : 0.6836708784103394
Loss at iteration 350 : 0.7625007629394531
Loss at iteration 400 : 0.7579151391983032
Mean training loss eporch  164 :  0.6082362127511224
Loss at iteration 50 : 0.5647455453872681
Loss at iteration 100 : 0.4609184265136719
Loss at iteration 150 : 0.6649163961410522
Loss at iteration 200 : 0.4565047025680542
Loss at iteration 250 : 0.48653385043144226
Loss at iteration 300 : 0.2936839461326599
Loss at iteration 350 : 1.0262788534164429
Loss at iteration 400 : 1.0300341844558716
Mean training loss eporch  165 :  0.6090844586291121
Loss at iteration 50 : 1.1358296871185303
Loss at iteration 100 : 0.7454723119735718
Loss at iteration 150 : 0.33640146255493164
Loss at iteration 200 : 0.4533206522464752
Loss at iteration 250 : 0.7975078821182251
Loss at iteration 300 : 0.32642921805381775
Loss at iteration 350 : 0.7457108497619629
Loss at iteration 400 : 0.3096327483654022
Mean training loss eporch  166 :  0.608729889875303
Loss at iteration 50 : 0.4832213521003723
Loss at iteration 100 : 0.6464508175849915
Loss at iteration 150 : 0.7552874088287354
Loss at iteration 200 : 0.525438666343689
Loss at iteration 250 : 0.44817453622817993
Loss at iteration 300 : 0.8576258420944214
Loss at iteration 350 : 0.761956512928009
Loss at iteration 400 : 0.5019522905349731
Mean training loss eporch  167 :  0.6084972640390888
Loss at iteration 50 : 0.3958815038204193
Loss at iteration 100 : 0.5327308177947998
Loss at iteration 150 : 0.42806944251060486
Loss at iteration 200 : 0.6222973465919495
Loss at iteration 250 : 0.5397359132766724
Loss at iteration 300 : 0.34627607464790344
Loss at iteration 350 : 0.6416061520576477
Loss at iteration 400 : 0.5752493143081665
Mean training loss eporch  168 :  0.6085224369114824
Loss at iteration 50 : 0.42389678955078125
Loss at iteration 100 : 0.7148503065109253
Loss at iteration 150 : 0.45361316204071045
Loss at iteration 200 : 0.7407764196395874
Loss at iteration 250 : 0.6707934141159058
Loss at iteration 300 : 0.4299851655960083
Loss at iteration 350 : 0.24467891454696655
Loss at iteration 400 : 1.0288872718811035
Mean training loss eporch  169 :  0.6083269585007509
Loss at iteration 50 : 0.5541509389877319
Loss at iteration 100 : 0.4435041844844818
Loss at iteration 150 : 0.29873543977737427
Loss at iteration 200 : 0.6778057813644409
Loss at iteration 250 : 0.412779301404953
Loss at iteration 300 : 0.8951734304428101
Loss at iteration 350 : 0.3623913526535034
Loss at iteration 400 : 0.38463544845581055
Mean training loss eporch  170 :  0.6099480797824838
Loss at iteration 50 : 0.7097386121749878
Loss at iteration 100 : 0.6090775728225708
Loss at iteration 150 : 0.7609443068504333
Loss at iteration 200 : 0.5043904781341553
Loss at iteration 250 : 0.7616338729858398
Loss at iteration 300 : 0.503028392791748
Loss at iteration 350 : 0.7705679535865784
Loss at iteration 400 : 0.3496975898742676
Mean training loss eporch  171 :  0.610976751409304
Loss at iteration 50 : 0.4185081422328949
Loss at iteration 100 : 0.8996480703353882
Loss at iteration 150 : 0.2977168560028076
Loss at iteration 200 : 1.1590542793273926
Loss at iteration 250 : 0.3195752203464508
Loss at iteration 300 : 0.4829903244972229
Loss at iteration 350 : 0.6969824433326721
Loss at iteration 400 : 0.8853838443756104
Mean training loss eporch  172 :  0.6081684670440285
Loss at iteration 50 : 1.235121726989746
Loss at iteration 100 : 1.3383147716522217
Loss at iteration 150 : 0.4306468963623047
Loss at iteration 200 : 0.5979560613632202
Loss at iteration 250 : 0.9750733375549316
Loss at iteration 300 : 0.9278295636177063
Loss at iteration 350 : 0.34456300735473633
Loss at iteration 400 : 0.5109776258468628
Mean training loss eporch  173 :  0.6079036096620453
Loss at iteration 50 : 0.8756614327430725
Loss at iteration 100 : 0.3574844300746918
Loss at iteration 150 : 0.40150701999664307
Loss at iteration 200 : 0.6452372670173645
Loss at iteration 250 : 0.41010066866874695
Loss at iteration 300 : 0.26888343691825867
Loss at iteration 350 : 0.5950003862380981
Loss at iteration 400 : 0.2739410996437073
Mean training loss eporch  174 :  0.6119351509306997
Loss at iteration 50 : 1.3615204095840454
Loss at iteration 100 : 0.9171996116638184
Loss at iteration 150 : 0.5863485336303711
Loss at iteration 200 : 0.7295718193054199
Loss at iteration 250 : 0.3402901887893677
Loss at iteration 300 : 0.6758124828338623
Loss at iteration 350 : 0.6947834491729736
Loss at iteration 400 : 0.3997495174407959
Mean training loss eporch  175 :  0.6084194837196526
Loss at iteration 50 : 1.0198346376419067
Loss at iteration 100 : 0.6479266285896301
Loss at iteration 150 : 0.3661189377307892
Loss at iteration 200 : 0.3349013328552246
Loss at iteration 250 : 0.8460148572921753
Loss at iteration 300 : 1.2454062700271606
Loss at iteration 350 : 0.5847049951553345
Loss at iteration 400 : 0.34210479259490967
Mean training loss eporch  176 :  0.6143715062884472
Loss at iteration 50 : 0.6680333614349365
Loss at iteration 100 : 0.2954025864601135
Loss at iteration 150 : 0.45486941933631897
Loss at iteration 200 : 0.8037029504776001
Loss at iteration 250 : 1.3678994178771973
Loss at iteration 300 : 1.0523126125335693
Loss at iteration 350 : 0.9776856899261475
Loss at iteration 400 : 0.6441358327865601
Mean training loss eporch  177 :  0.6094937155465908
Loss at iteration 50 : 1.152870535850525
Loss at iteration 100 : 0.6679877042770386
Loss at iteration 150 : 0.5022027492523193
Loss at iteration 200 : 0.4350441098213196
Loss at iteration 250 : 0.6934669017791748
Loss at iteration 300 : 0.6936941742897034
Loss at iteration 350 : 0.31599873304367065
Loss at iteration 400 : 0.43808138370513916
Mean training loss eporch  178 :  0.6135570521846481
Loss at iteration 50 : 0.5724513530731201
Loss at iteration 100 : 0.6699341535568237
Loss at iteration 150 : 0.36752915382385254
Loss at iteration 200 : 0.7375319004058838
Loss at iteration 250 : 0.3896964490413666
Loss at iteration 300 : 0.36084333062171936
Loss at iteration 350 : 1.1929450035095215
Loss at iteration 400 : 1.0170156955718994
Mean training loss eporch  179 :  0.6144674535957687
Loss at iteration 50 : 0.3994625210762024
Loss at iteration 100 : 0.6998172998428345
Loss at iteration 150 : 0.4764567017555237
Loss at iteration 200 : 0.6025702953338623
Loss at iteration 250 : 0.9207018613815308
Loss at iteration 300 : 0.6739003658294678
Loss at iteration 350 : 0.7037804126739502
Loss at iteration 400 : 0.37811458110809326
Mean training loss eporch  180 :  0.6082947780399045
Loss at iteration 50 : 0.30527830123901367
Loss at iteration 100 : 0.35826581716537476
Loss at iteration 150 : 0.4541889727115631
Loss at iteration 200 : 1.0092499256134033
Loss at iteration 250 : 0.7533704042434692
Loss at iteration 300 : 0.8634547591209412
Loss at iteration 350 : 0.3557471036911011
Loss at iteration 400 : 1.1554913520812988
Mean training loss eporch  181 :  0.6078322965802099
Loss at iteration 50 : 0.7870650291442871
Loss at iteration 100 : 0.7146580815315247
Loss at iteration 150 : 0.38038086891174316
Loss at iteration 200 : 0.7137171626091003
Loss at iteration 250 : 0.8992050886154175
Loss at iteration 300 : 0.4421933591365814
Loss at iteration 350 : 0.599984884262085
Loss at iteration 400 : 0.2714312672615051
Mean training loss eporch  182 :  0.6078380259764569
Loss at iteration 50 : 0.7684051990509033
Loss at iteration 100 : 0.6397372484207153
Loss at iteration 150 : 0.5036654472351074
Loss at iteration 200 : 0.7787772417068481
Loss at iteration 250 : 0.24282306432724
Loss at iteration 300 : 0.7188831567764282
Loss at iteration 350 : 1.025397777557373
Loss at iteration 400 : 0.3946153223514557
Mean training loss eporch  183 :  0.6094326325796645
Loss at iteration 50 : 0.559339165687561
Loss at iteration 100 : 0.31767719984054565
Loss at iteration 150 : 1.3123712539672852
Loss at iteration 200 : 0.41207683086395264
Loss at iteration 250 : 0.29835957288742065
Loss at iteration 300 : 0.4338294267654419
Loss at iteration 350 : 0.33834779262542725
Loss at iteration 400 : 0.5068149566650391
Mean training loss eporch  184 :  0.6072493209232129
Loss at iteration 50 : 0.298313170671463
Loss at iteration 100 : 0.27402448654174805
Loss at iteration 150 : 0.46119433641433716
Loss at iteration 200 : 0.30007684230804443
Loss at iteration 250 : 1.071813941001892
Loss at iteration 300 : 0.7322367429733276
Loss at iteration 350 : 0.5955593585968018
Loss at iteration 400 : 0.7406266927719116
Mean training loss eporch  185 :  0.6088135328327594
Loss at iteration 50 : 0.9021198153495789
Loss at iteration 100 : 0.580005407333374
Loss at iteration 150 : 0.40972524881362915
Loss at iteration 200 : 0.5534566044807434
Loss at iteration 250 : 0.35018205642700195
Loss at iteration 300 : 0.5398704409599304
Loss at iteration 350 : 0.6474895477294922
Loss at iteration 400 : 0.4157659709453583
Mean training loss eporch  186 :  0.6074921579973045
Loss at iteration 50 : 0.3595825135707855
Loss at iteration 100 : 0.3402748107910156
Loss at iteration 150 : 0.29917025566101074
Loss at iteration 200 : 0.7841873168945312
Loss at iteration 250 : 0.6023474931716919
Loss at iteration 300 : 0.7129886150360107
Loss at iteration 350 : 0.3780856132507324
Loss at iteration 400 : 0.37895268201828003
Mean training loss eporch  187 :  0.6074553566061862
Loss at iteration 50 : 1.3123420476913452
Loss at iteration 100 : 1.0279349088668823
Loss at iteration 150 : 0.46279990673065186
Loss at iteration 200 : 0.9413861036300659
Loss at iteration 250 : 0.5432499647140503
Loss at iteration 300 : 1.4334995746612549
Loss at iteration 350 : 0.938119113445282
Loss at iteration 400 : 0.2790863513946533
Mean training loss eporch  188 :  0.6126915536854299
Loss at iteration 50 : 0.6510273814201355
Loss at iteration 100 : 0.3041294813156128
Loss at iteration 150 : 0.6363571882247925
Loss at iteration 200 : 1.275567889213562
Loss at iteration 250 : 0.5260701179504395
Loss at iteration 300 : 0.4580448865890503
Loss at iteration 350 : 0.4923809766769409
Loss at iteration 400 : 0.4635950028896332
Mean training loss eporch  189 :  0.6071220932386381
Loss at iteration 50 : 0.8003056049346924
Loss at iteration 100 : 0.34145066142082214
Loss at iteration 150 : 0.7439936399459839
Loss at iteration 200 : 0.8872343301773071
Loss at iteration 250 : 0.4029792249202728
Loss at iteration 300 : 0.7191588878631592
Loss at iteration 350 : 0.35962700843811035
Loss at iteration 400 : 0.319751113653183
Mean training loss eporch  190 :  0.6070904012084541
Loss at iteration 50 : 0.791283130645752
Loss at iteration 100 : 0.721847414970398
Loss at iteration 150 : 0.6251857280731201
Loss at iteration 200 : 0.6548471450805664
Loss at iteration 250 : 0.5081093311309814
Loss at iteration 300 : 0.5970321893692017
Loss at iteration 350 : 0.620765209197998
Loss at iteration 400 : 0.4054693579673767
Mean training loss eporch  191 :  0.607367813553778
Loss at iteration 50 : 0.5095047354698181
Loss at iteration 100 : 0.6581541299819946
Loss at iteration 150 : 0.385320782661438
Loss at iteration 200 : 0.6509228348731995
Loss at iteration 250 : 0.6880754828453064
Loss at iteration 300 : 0.8632657527923584
Loss at iteration 350 : 0.5097392201423645
Loss at iteration 400 : 0.5651935935020447
Mean training loss eporch  192 :  0.6070181726072936
Loss at iteration 50 : 0.41648173332214355
Loss at iteration 100 : 0.49379104375839233
Loss at iteration 150 : 0.7789562940597534
Loss at iteration 200 : 0.9839071035385132
Loss at iteration 250 : 0.6729200482368469
Loss at iteration 300 : 0.7784870862960815
Loss at iteration 350 : 0.2631795406341553
Loss at iteration 400 : 0.7014425992965698
Mean training loss eporch  193 :  0.6078217588866238
Loss at iteration 50 : 1.1280443668365479
Loss at iteration 100 : 0.782102108001709
Loss at iteration 150 : 0.5069326162338257
Loss at iteration 200 : 0.561127781867981
Loss at iteration 250 : 0.22736310958862305
Loss at iteration 300 : 0.23411288857460022
Loss at iteration 350 : 1.0877771377563477
Loss at iteration 400 : 0.5324526429176331
Mean training loss eporch  194 :  0.6075095679661083
Loss at iteration 50 : 0.3396545946598053
Loss at iteration 100 : 0.5164496898651123
Loss at iteration 150 : 0.2653372287750244
Loss at iteration 200 : 0.4844958782196045
Loss at iteration 250 : 0.745344340801239
Loss at iteration 300 : 0.3128533363342285
Loss at iteration 350 : 0.3671453595161438
Loss at iteration 400 : 0.7420781850814819
Mean training loss eporch  195 :  0.6071910571824809
Loss at iteration 50 : 0.2424938976764679
Loss at iteration 100 : 0.288551390171051
Loss at iteration 150 : 0.9151867628097534
Loss at iteration 200 : 0.2625690698623657
Loss at iteration 250 : 0.5475471019744873
Loss at iteration 300 : 0.28093478083610535
Loss at iteration 350 : 0.313995361328125
Loss at iteration 400 : 0.2981729805469513
Mean training loss eporch  196 :  0.6068775660855353
Loss at iteration 50 : 0.47267499566078186
Loss at iteration 100 : 0.6359010934829712
Loss at iteration 150 : 0.7239193916320801
Loss at iteration 200 : 0.6941370368003845
Loss at iteration 250 : 1.151047945022583
Loss at iteration 300 : 0.5091016292572021
Loss at iteration 350 : 0.39462214708328247
Loss at iteration 400 : 0.3105008602142334
Mean training loss eporch  197 :  0.6064474801177936
Loss at iteration 50 : 0.44668352603912354
Loss at iteration 100 : 0.5661736130714417
Loss at iteration 150 : 0.6416396498680115
Loss at iteration 200 : 0.6307053565979004
Loss at iteration 250 : 0.5687012672424316
Loss at iteration 300 : 0.361146479845047
Loss at iteration 350 : 0.27024567127227783
Loss at iteration 400 : 0.8567719459533691
Mean training loss eporch  198 :  0.6066792057701825
Loss at iteration 50 : 0.5593305826187134
Loss at iteration 100 : 0.6890872716903687
Loss at iteration 150 : 0.638258695602417
Loss at iteration 200 : 1.0279674530029297
Loss at iteration 250 : 0.3393934369087219
Loss at iteration 300 : 0.8227444291114807
Loss at iteration 350 : 0.9890705347061157
Loss at iteration 400 : 0.7672936916351318
Mean training loss eporch  199 :  0.6067052998083055
Loss at iteration 50 : 0.4306974411010742
Loss at iteration 100 : 0.41882625222206116
Loss at iteration 150 : 0.37426209449768066
Loss at iteration 200 : 0.6618814468383789
Loss at iteration 250 : 0.8923747539520264
Loss at iteration 300 : 0.8762170076370239
Loss at iteration 350 : 0.3453061282634735
Loss at iteration 400 : 1.0703330039978027
Mean training loss eporch  200 :  0.6072680679872432
Loss at iteration 50 : 0.7538182735443115
Loss at iteration 100 : 0.9466685056686401
Loss at iteration 150 : 1.2977830171585083
Loss at iteration 200 : 0.776314914226532
Loss at iteration 250 : 0.6984177231788635
Loss at iteration 300 : 1.4049553871154785
Loss at iteration 350 : 0.4134518504142761
Loss at iteration 400 : 0.7643663287162781
Mean training loss eporch  201 :  0.6066762713105689
Loss at iteration 50 : 0.7226731181144714
Loss at iteration 100 : 0.4969918131828308
Loss at iteration 150 : 0.5724005699157715
Loss at iteration 200 : 1.0561108589172363
Loss at iteration 250 : 0.7862961292266846
Loss at iteration 300 : 0.3042914569377899
Loss at iteration 350 : 0.36105775833129883
Loss at iteration 400 : 0.5683973431587219
Mean training loss eporch  202 :  0.6080653079422066
Loss at iteration 50 : 0.2869904637336731
Loss at iteration 100 : 0.768741250038147
Loss at iteration 150 : 0.973563551902771
Loss at iteration 200 : 0.7660982608795166
Loss at iteration 250 : 0.6940868496894836
Loss at iteration 300 : 0.5109414458274841
Loss at iteration 350 : 0.5041033029556274
Loss at iteration 400 : 0.27060917019844055
Mean training loss eporch  203 :  0.6076282425178006
Loss at iteration 50 : 0.6105976104736328
Loss at iteration 100 : 0.5719352960586548
Loss at iteration 150 : 0.5177769660949707
Loss at iteration 200 : 0.8461847901344299
Loss at iteration 250 : 0.45016005635261536
Loss at iteration 300 : 0.9039944410324097
Loss at iteration 350 : 1.2254267930984497
Loss at iteration 400 : 0.2918858826160431
Mean training loss eporch  204 :  0.6061755448847074
Loss at iteration 50 : 0.44161853194236755
Loss at iteration 100 : 0.4664759933948517
Loss at iteration 150 : 0.8217504024505615
Loss at iteration 200 : 1.5136899948120117
Loss at iteration 250 : 0.763897180557251
Loss at iteration 300 : 0.25309160351753235
Loss at iteration 350 : 0.4357033967971802
Loss at iteration 400 : 0.8896014094352722
Mean training loss eporch  205 :  0.6070574362649511
Loss at iteration 50 : 0.7915710210800171
Loss at iteration 100 : 0.7921534776687622
Loss at iteration 150 : 0.6430382132530212
Loss at iteration 200 : 0.36079537868499756
Loss at iteration 250 : 0.6102768182754517
Loss at iteration 300 : 0.7496187090873718
Loss at iteration 350 : 0.7242878675460815
Loss at iteration 400 : 1.3561930656433105
Mean training loss eporch  206 :  0.6069884448575332
Loss at iteration 50 : 0.5994436740875244
Loss at iteration 100 : 0.6850588321685791
Loss at iteration 150 : 0.5942692756652832
Loss at iteration 200 : 0.4913422167301178
Loss at iteration 250 : 0.37668830156326294
Loss at iteration 300 : 0.4917023479938507
Loss at iteration 350 : 1.0423176288604736
Loss at iteration 400 : 0.3364384174346924
Mean training loss eporch  207 :  0.6069611433910147
Loss at iteration 50 : 0.24873891472816467
Loss at iteration 100 : 0.3261142671108246
Loss at iteration 150 : 0.3079090714454651
Loss at iteration 200 : 0.46858924627304077
Loss at iteration 250 : 0.7146979570388794
Loss at iteration 300 : 0.5579751133918762
Loss at iteration 350 : 0.23817069828510284
Loss at iteration 400 : 0.3103461265563965
Mean training loss eporch  208 :  0.6061947680174502
Loss at iteration 50 : 0.41232073307037354
Loss at iteration 100 : 1.1777737140655518
Loss at iteration 150 : 0.29212719202041626
Loss at iteration 200 : 0.5844655632972717
Loss at iteration 250 : 0.4832119941711426
Loss at iteration 300 : 0.5171279311180115
Loss at iteration 350 : 0.3143969476222992
Loss at iteration 400 : 0.6723031997680664
Mean training loss eporch  209 :  0.6067533660429476
Loss at iteration 50 : 0.6695187091827393
Loss at iteration 100 : 0.8274271488189697
Loss at iteration 150 : 1.0860129594802856
Loss at iteration 200 : 0.39395979046821594
Loss at iteration 250 : 0.6967155933380127
Loss at iteration 300 : 0.34555330872535706
Loss at iteration 350 : 0.8246099948883057
Loss at iteration 400 : 0.8752905130386353
Mean training loss eporch  210 :  0.6059031731745588
Loss at iteration 50 : 0.3631213307380676
Loss at iteration 100 : 0.5759237408638
Loss at iteration 150 : 0.503764271736145
Loss at iteration 200 : 0.39581215381622314
Loss at iteration 250 : 0.6831233501434326
Loss at iteration 300 : 0.3318300247192383
Loss at iteration 350 : 0.6033575534820557
Loss at iteration 400 : 0.49533772468566895
Mean training loss eporch  211 :  0.6105199006173109
Loss at iteration 50 : 0.5391031503677368
Loss at iteration 100 : 0.46846798062324524
Loss at iteration 150 : 0.5363857746124268
Loss at iteration 200 : 0.6139135360717773
Loss at iteration 250 : 0.82743239402771
Loss at iteration 300 : 0.9147545695304871
Loss at iteration 350 : 0.6887668371200562
Loss at iteration 400 : 0.46524229645729065
Mean training loss eporch  212 :  0.6062631944837592
Loss at iteration 50 : 0.7863184213638306
Loss at iteration 100 : 0.252438485622406
Loss at iteration 150 : 0.7875736951828003
Loss at iteration 200 : 0.6640756130218506
Loss at iteration 250 : 0.5906935930252075
Loss at iteration 300 : 0.3621135652065277
Loss at iteration 350 : 0.6916949152946472
Loss at iteration 400 : 0.6084975004196167
Mean training loss eporch  213 :  0.6114433589871688
Loss at iteration 50 : 0.7165338397026062
Loss at iteration 100 : 0.4169113039970398
Loss at iteration 150 : 0.5562638640403748
Loss at iteration 200 : 0.3127138614654541
Loss at iteration 250 : 0.46001529693603516
Loss at iteration 300 : 0.5199291706085205
Loss at iteration 350 : 0.7902480959892273
Loss at iteration 400 : 0.8795807957649231
Mean training loss eporch  214 :  0.6064419646501007
Loss at iteration 50 : 0.974510133266449
Loss at iteration 100 : 0.499002069234848
Loss at iteration 150 : 0.7426701784133911
Loss at iteration 200 : 1.1621129512786865
Loss at iteration 250 : 0.48532649874687195
Loss at iteration 300 : 0.4681951403617859
Loss at iteration 350 : 0.3012157678604126
Loss at iteration 400 : 0.5503178834915161
Mean training loss eporch  215 :  0.6063124270516661
Loss at iteration 50 : 1.1631851196289062
Loss at iteration 100 : 1.0165581703186035
Loss at iteration 150 : 0.28685247898101807
Loss at iteration 200 : 0.46102142333984375
Loss at iteration 250 : 0.7053815126419067
Loss at iteration 300 : 0.39052289724349976
Loss at iteration 350 : 0.4871590733528137
Loss at iteration 400 : 0.44979333877563477
Mean training loss eporch  216 :  0.6059463319489774
Loss at iteration 50 : 0.5245220065116882
Loss at iteration 100 : 0.6438665390014648
Loss at iteration 150 : 0.6146361827850342
Loss at iteration 200 : 0.7341269254684448
Loss at iteration 250 : 1.140607476234436
Loss at iteration 300 : 0.6974802613258362
Loss at iteration 350 : 0.42667508125305176
Loss at iteration 400 : 0.33928418159484863
Mean training loss eporch  217 :  0.606337889260508
Loss at iteration 50 : 0.8503741025924683
Loss at iteration 100 : 0.3385811150074005
Loss at iteration 150 : 0.3186188340187073
Loss at iteration 200 : 0.3047286868095398
Loss at iteration 250 : 0.5688804984092712
Loss at iteration 300 : 0.6371705532073975
Loss at iteration 350 : 0.28514939546585083
Loss at iteration 400 : 0.49410367012023926
Mean training loss eporch  218 :  0.6064465274123868
Loss at iteration 50 : 0.48972392082214355
Loss at iteration 100 : 0.8762281537055969
Loss at iteration 150 : 1.116688847541809
Loss at iteration 200 : 0.309827983379364
Loss at iteration 250 : 0.7494885921478271
Loss at iteration 300 : 0.4057772159576416
Loss at iteration 350 : 0.8088490962982178
Loss at iteration 400 : 0.6651638746261597
Mean training loss eporch  219 :  0.6056986935937886
Loss at iteration 50 : 0.6300529837608337
Loss at iteration 100 : 1.518651008605957
Loss at iteration 150 : 0.9661400318145752
Loss at iteration 200 : 0.2934662699699402
Loss at iteration 250 : 0.8187123537063599
Loss at iteration 300 : 1.0414572954177856
Loss at iteration 350 : 1.0047547817230225
Loss at iteration 400 : 0.6172841787338257
Mean training loss eporch  220 :  0.6071798794739984
Loss at iteration 50 : 0.5762213468551636
Loss at iteration 100 : 0.45395082235336304
Loss at iteration 150 : 0.5749097466468811
Loss at iteration 200 : 0.6653807163238525
Loss at iteration 250 : 0.45637908577919006
Loss at iteration 300 : 0.7189257144927979
Loss at iteration 350 : 0.9333646893501282
Loss at iteration 400 : 0.5845722556114197
Mean training loss eporch  221 :  0.6072242431376013
Loss at iteration 50 : 0.37262436747550964
Loss at iteration 100 : 1.1057987213134766
Loss at iteration 150 : 0.8164148330688477
Loss at iteration 200 : 0.4077473282814026
Loss at iteration 250 : 0.7464880347251892
Loss at iteration 300 : 0.43593305349349976
Loss at iteration 350 : 0.3349536657333374
Loss at iteration 400 : 0.5149873495101929
Mean training loss eporch  222 :  0.6064282952068633
Loss at iteration 50 : 0.34196633100509644
Loss at iteration 100 : 0.853814959526062
Loss at iteration 150 : 0.39364129304885864
Loss at iteration 200 : 0.5812851190567017
Loss at iteration 250 : 0.8620411157608032
Loss at iteration 300 : 0.37753766775131226
Loss at iteration 350 : 0.36802685260772705
Loss at iteration 400 : 0.6895922422409058
Mean training loss eporch  223 :  0.6055213032549273
Loss at iteration 50 : 0.8902593851089478
Loss at iteration 100 : 0.8996868133544922
Loss at iteration 150 : 0.6056927442550659
Loss at iteration 200 : 0.8595110774040222
Loss at iteration 250 : 0.9654438495635986
Loss at iteration 300 : 0.3321698307991028
Loss at iteration 350 : 0.3040074110031128
Loss at iteration 400 : 0.7373759746551514
Mean training loss eporch  224 :  0.6056606085311137
Loss at iteration 50 : 0.582521915435791
Loss at iteration 100 : 0.6247408986091614
Loss at iteration 150 : 0.518857479095459
Loss at iteration 200 : 0.7589420676231384
Loss at iteration 250 : 0.2856096625328064
Loss at iteration 300 : 0.6302694082260132
Loss at iteration 350 : 0.27672746777534485
Loss at iteration 400 : 0.9203572869300842
Mean training loss eporch  225 :  0.6050655208026882
Loss at iteration 50 : 0.4919103980064392
Loss at iteration 100 : 0.8337054252624512
Loss at iteration 150 : 0.5407193899154663
Loss at iteration 200 : 0.7529710531234741
Loss at iteration 250 : 0.5060856342315674
Loss at iteration 300 : 0.7946612238883972
Loss at iteration 350 : 0.2695356607437134
Loss at iteration 400 : 0.37272098660469055
Mean training loss eporch  226 :  0.6057624959558114
Loss at iteration 50 : 0.44890904426574707
Loss at iteration 100 : 0.44713646173477173
Loss at iteration 150 : 1.0502941608428955
Loss at iteration 200 : 0.7238810658454895
Loss at iteration 250 : 0.34310099482536316
Loss at iteration 300 : 0.42794835567474365
Loss at iteration 350 : 1.126619815826416
Loss at iteration 400 : 0.39292997121810913
Mean training loss eporch  227 :  0.6053818553151571
Loss at iteration 50 : 0.8187063932418823
Loss at iteration 100 : 0.4886109232902527
Loss at iteration 150 : 0.886455237865448
Loss at iteration 200 : 0.302805095911026
Loss at iteration 250 : 0.6500388383865356
Loss at iteration 300 : 0.6038892865180969
Loss at iteration 350 : 0.8570023775100708
Loss at iteration 400 : 0.34422338008880615
Mean training loss eporch  228 :  0.6060413448265315
Loss at iteration 50 : 0.5029089450836182
Loss at iteration 100 : 0.31184422969818115
Loss at iteration 150 : 0.7631461024284363
Loss at iteration 200 : 0.7323997616767883
Loss at iteration 250 : 0.6669309139251709
Loss at iteration 300 : 0.46727287769317627
Loss at iteration 350 : 1.0021125078201294
Loss at iteration 400 : 0.8286267518997192
Mean training loss eporch  229 :  0.6094708322743664
Loss at iteration 50 : 0.3261941075325012
Loss at iteration 100 : 0.5773414373397827
Loss at iteration 150 : 0.30299919843673706
Loss at iteration 200 : 0.3224518895149231
Loss at iteration 250 : 0.3754655122756958
Loss at iteration 300 : 0.42753279209136963
Loss at iteration 350 : 0.5502238869667053
Loss at iteration 400 : 0.5423170328140259
Mean training loss eporch  230 :  0.6060498724670688
Loss at iteration 50 : 0.3483443558216095
Loss at iteration 100 : 1.1149787902832031
Loss at iteration 150 : 0.656088650226593
Loss at iteration 200 : 0.36111679673194885
Loss at iteration 250 : 0.37540316581726074
Loss at iteration 300 : 0.3567386269569397
Loss at iteration 350 : 1.243281364440918
Loss at iteration 400 : 1.1348159313201904
Mean training loss eporch  231 :  0.6054872515289773
Loss at iteration 50 : 0.48111557960510254
Loss at iteration 100 : 0.2942561209201813
Loss at iteration 150 : 0.7809083461761475
Loss at iteration 200 : 1.130025029182434
Loss at iteration 250 : 0.4161614179611206
Loss at iteration 300 : 0.5827953815460205
Loss at iteration 350 : 0.47981399297714233
Loss at iteration 400 : 0.329494446516037
Mean training loss eporch  232 :  0.6058065867210183
Loss at iteration 50 : 0.8203173875808716
Loss at iteration 100 : 0.9407249689102173
Loss at iteration 150 : 0.8123475313186646
Loss at iteration 200 : 0.4622618854045868
Loss at iteration 250 : 0.3835146129131317
Loss at iteration 300 : 0.6883100271224976
Loss at iteration 350 : 1.2196459770202637
Loss at iteration 400 : 0.6029931306838989
Mean training loss eporch  233 :  0.6052359043215423
Loss at iteration 50 : 0.6923800706863403
Loss at iteration 100 : 0.28817933797836304
Loss at iteration 150 : 0.7265477180480957
Loss at iteration 200 : 0.3157806396484375
Loss at iteration 250 : 0.27316176891326904
Loss at iteration 300 : 0.7415419220924377
Loss at iteration 350 : 0.6394050717353821
Loss at iteration 400 : 0.4667705297470093
Mean training loss eporch  234 :  0.6057617543390513
Loss at iteration 50 : 0.6177103519439697
Loss at iteration 100 : 0.29538169503211975
Loss at iteration 150 : 0.2858240604400635
Loss at iteration 200 : 0.6864795684814453
Loss at iteration 250 : 0.6214805841445923
Loss at iteration 300 : 0.4827187657356262
Loss at iteration 350 : 0.6209204196929932
Loss at iteration 400 : 0.272121787071228
Mean training loss eporch  235 :  0.6111963654914244
Loss at iteration 50 : 0.7418062090873718
Loss at iteration 100 : 0.4298018217086792
Loss at iteration 150 : 0.3994670510292053
Loss at iteration 200 : 0.7694640159606934
Loss at iteration 250 : 0.3684892952442169
Loss at iteration 300 : 0.3710644245147705
Loss at iteration 350 : 0.7832052707672119
Loss at iteration 400 : 0.690808892250061
Mean training loss eporch  236 :  0.6051873652924337
Loss at iteration 50 : 0.8864465951919556
Loss at iteration 100 : 0.29355138540267944
Loss at iteration 150 : 0.7169744968414307
Loss at iteration 200 : 0.6012264490127563
Loss at iteration 250 : 0.21402761340141296
Loss at iteration 300 : 0.5392860174179077
Loss at iteration 350 : 0.4080977439880371
Loss at iteration 400 : 0.3873506486415863
Mean training loss eporch  237 :  0.6047302143389334
Loss at iteration 50 : 0.5277702808380127
Loss at iteration 100 : 0.7320089340209961
Loss at iteration 150 : 0.32740992307662964
Loss at iteration 200 : 0.655949592590332
Loss at iteration 250 : 1.0056395530700684
Loss at iteration 300 : 0.6592468619346619
Loss at iteration 350 : 0.23622407019138336
Loss at iteration 400 : 0.4805625081062317
Mean training loss eporch  238 :  0.6052690582024143
Loss at iteration 50 : 0.5563696622848511
Loss at iteration 100 : 0.4277176856994629
Loss at iteration 150 : 0.5094842314720154
Loss at iteration 200 : 0.7523286938667297
Loss at iteration 250 : 0.6839593648910522
Loss at iteration 300 : 0.7058682441711426
Loss at iteration 350 : 0.7864235043525696
Loss at iteration 400 : 0.5619114637374878
Mean training loss eporch  239 :  0.6063416261777215
Loss at iteration 50 : 0.3623194694519043
Loss at iteration 100 : 0.39315760135650635
Loss at iteration 150 : 0.45722612738609314
Loss at iteration 200 : 0.8199597001075745
Loss at iteration 250 : 1.0571441650390625
Loss at iteration 300 : 0.9799082279205322
Loss at iteration 350 : 0.6834625005722046
Loss at iteration 400 : 0.32133933901786804
Mean training loss eporch  240 :  0.604320562540683
Loss at iteration 50 : 1.4543789625167847
Loss at iteration 100 : 0.6080883145332336
Loss at iteration 150 : 0.4337795674800873
Loss at iteration 200 : 0.46252813935279846
Loss at iteration 250 : 0.3846312165260315
Loss at iteration 300 : 0.2259065806865692
Loss at iteration 350 : 0.8128402233123779
Loss at iteration 400 : 0.9631152749061584
Mean training loss eporch  241 :  0.6049603106195082
Loss at iteration 50 : 0.5500938892364502
Loss at iteration 100 : 0.6836099028587341
Loss at iteration 150 : 0.4644451141357422
Loss at iteration 200 : 0.6148849725723267
Loss at iteration 250 : 0.5352548956871033
Loss at iteration 300 : 0.5991841554641724
Loss at iteration 350 : 1.1349176168441772
Loss at iteration 400 : 0.5196189284324646
Mean training loss eporch  242 :  0.6048579427492993
Loss at iteration 50 : 0.6730416417121887
Loss at iteration 100 : 0.7216489911079407
Loss at iteration 150 : 0.2781674265861511
Loss at iteration 200 : 0.4057048559188843
Loss at iteration 250 : 0.43740415573120117
Loss at iteration 300 : 0.7465242147445679
Loss at iteration 350 : 0.8854632377624512
Loss at iteration 400 : 0.7658506631851196
Mean training loss eporch  243 :  0.6055895134205241
Loss at iteration 50 : 0.44558387994766235
Loss at iteration 100 : 0.5438799858093262
Loss at iteration 150 : 0.9460768699645996
Loss at iteration 200 : 1.0784871578216553
Loss at iteration 250 : 0.4351847469806671
Loss at iteration 300 : 0.27933502197265625
Loss at iteration 350 : 0.39009326696395874
Loss at iteration 400 : 0.4786728024482727
Mean training loss eporch  244 :  0.6061488531964242
Loss at iteration 50 : 0.4629800319671631
Loss at iteration 100 : 0.5878801941871643
Loss at iteration 150 : 1.0469205379486084
Loss at iteration 200 : 0.6995831727981567
Loss at iteration 250 : 0.34401607513427734
Loss at iteration 300 : 0.36833465099334717
Loss at iteration 350 : 0.588894784450531
Loss at iteration 400 : 1.2440319061279297
Mean training loss eporch  245 :  0.6053638182613882
Loss at iteration 50 : 1.0417873859405518
Loss at iteration 100 : 1.1781120300292969
Loss at iteration 150 : 0.36668407917022705
Loss at iteration 200 : 0.5356993675231934
Loss at iteration 250 : 0.8637359142303467
Loss at iteration 300 : 0.47928881645202637
Loss at iteration 350 : 0.8371257781982422
Loss at iteration 400 : 0.5074512958526611
Mean training loss eporch  246 :  0.6047202565878496
Loss at iteration 50 : 0.7253497838973999
Loss at iteration 100 : 0.6355483531951904
Loss at iteration 150 : 0.5247950553894043
Loss at iteration 200 : 0.9502083659172058
Loss at iteration 250 : 0.5373944640159607
Loss at iteration 300 : 0.7595458030700684
Loss at iteration 350 : 0.2516351342201233
Loss at iteration 400 : 0.35589417815208435
Mean training loss eporch  247 :  0.6051177566896105
Loss at iteration 50 : 0.7896316647529602
Loss at iteration 100 : 1.0793752670288086
Loss at iteration 150 : 0.35014787316322327
Loss at iteration 200 : 0.3661372661590576
Loss at iteration 250 : 0.4027329087257385
Loss at iteration 300 : 0.5418988466262817
Loss at iteration 350 : 0.48495203256607056
Loss at iteration 400 : 0.4466072916984558
Mean training loss eporch  248 :  0.6045325381071578
Loss at iteration 50 : 0.9707909226417542
Loss at iteration 100 : 0.4134758710861206
Loss at iteration 150 : 0.4875117540359497
Loss at iteration 200 : 0.35323911905288696
Loss at iteration 250 : 0.5109902024269104
Loss at iteration 300 : 0.5651713609695435
Loss at iteration 350 : 0.9249398708343506
Loss at iteration 400 : 0.8373120427131653
Mean training loss eporch  249 :  0.6048839019753474
Loss at iteration 50 : 0.4625861942768097
Loss at iteration 100 : 0.3648681640625
Loss at iteration 150 : 0.29518231749534607
Loss at iteration 200 : 0.9160595536231995
Loss at iteration 250 : 0.2968142032623291
Loss at iteration 300 : 0.8635398149490356
Loss at iteration 350 : 0.5293395519256592
Loss at iteration 400 : 0.3293651044368744
Mean training loss eporch  250 :  0.6048173506831909
Loss at iteration 50 : 1.1761736869812012
Loss at iteration 100 : 0.5220394134521484
Loss at iteration 150 : 0.6669065952301025
Loss at iteration 200 : 0.3391196131706238
Loss at iteration 250 : 0.5577350854873657
Loss at iteration 300 : 0.36185991764068604
Loss at iteration 350 : 0.4197463095188141
Loss at iteration 400 : 0.5431143641471863
Mean training loss eporch  251 :  0.604466947572381
Loss at iteration 50 : 0.4144015908241272
Loss at iteration 100 : 0.5328453779220581
Loss at iteration 150 : 0.965518057346344
Loss at iteration 200 : 0.6442618370056152
Loss at iteration 250 : 0.5819224119186401
Loss at iteration 300 : 0.6823589205741882
Loss at iteration 350 : 0.6204250454902649
Loss at iteration 400 : 0.7267305850982666
Mean training loss eporch  252 :  0.60446682532272
Loss at iteration 50 : 0.8755544424057007
Loss at iteration 100 : 0.6529104709625244
Loss at iteration 150 : 0.26862502098083496
Loss at iteration 200 : 0.6684045791625977
Loss at iteration 250 : 0.734281063079834
Loss at iteration 300 : 0.3068728446960449
Loss at iteration 350 : 0.7256919145584106
Loss at iteration 400 : 0.44642025232315063
Mean training loss eporch  253 :  0.6101277619934403
Loss at iteration 50 : 1.0191917419433594
Loss at iteration 100 : 1.2619708776474
Loss at iteration 150 : 0.19653654098510742
Loss at iteration 200 : 0.49921029806137085
Loss at iteration 250 : 0.6836234331130981
Loss at iteration 300 : 0.6734662055969238
Loss at iteration 350 : 0.6143243312835693
Loss at iteration 400 : 0.253984272480011
Mean training loss eporch  254 :  0.6046464314909794
Loss at iteration 50 : 0.2684100866317749
Loss at iteration 100 : 0.39783212542533875
Loss at iteration 150 : 0.7190794348716736
Loss at iteration 200 : 1.1033217906951904
Loss at iteration 250 : 0.6176410913467407
Loss at iteration 300 : 0.40938490629196167
Loss at iteration 350 : 0.7877018451690674
Loss at iteration 400 : 0.7030675411224365
Mean training loss eporch  255 :  0.606015444002344
Loss at iteration 50 : 0.4828750491142273
Loss at iteration 100 : 1.0679415464401245
Loss at iteration 150 : 0.7850576639175415
Loss at iteration 200 : 0.41793936491012573
Loss at iteration 250 : 0.9099152684211731
Loss at iteration 300 : 0.4103248119354248
Loss at iteration 350 : 1.0091972351074219
Loss at iteration 400 : 0.9039341807365417
Mean training loss eporch  256 :  0.6052125178845474
Loss at iteration 50 : 0.44725266098976135
Loss at iteration 100 : 0.4118494987487793
Loss at iteration 150 : 0.7776187062263489
Loss at iteration 200 : 0.2616127133369446
Loss at iteration 250 : 0.9099032878875732
Loss at iteration 300 : 0.4641037583351135
Loss at iteration 350 : 0.26343807578086853
Loss at iteration 400 : 0.4292762875556946
Mean training loss eporch  257 :  0.6103607669405873
Loss at iteration 50 : 0.3248569667339325
Loss at iteration 100 : 0.30634042620658875
Loss at iteration 150 : 0.4905726909637451
Loss at iteration 200 : 0.25465136766433716
Loss at iteration 250 : 0.7149665355682373
Loss at iteration 300 : 0.49042508006095886
Loss at iteration 350 : 0.6624112129211426
Loss at iteration 400 : 0.47502604126930237
Mean training loss eporch  258 :  0.6044668378517232
Loss at iteration 50 : 0.29129356145858765
Loss at iteration 100 : 0.5295393466949463
Loss at iteration 150 : 0.6970857381820679
Loss at iteration 200 : 0.7703697681427002
Loss at iteration 250 : 0.6378148794174194
Loss at iteration 300 : 0.4268707036972046
Loss at iteration 350 : 0.5624380707740784
Loss at iteration 400 : 0.30116236209869385
Mean training loss eporch  259 :  0.6048138507813081
Loss at iteration 50 : 0.8972089886665344
Loss at iteration 100 : 0.664836049079895
Loss at iteration 150 : 0.34883153438568115
Loss at iteration 200 : 0.5280534029006958
Loss at iteration 250 : 1.068936824798584
Loss at iteration 300 : 0.40981385111808777
Loss at iteration 350 : 0.5138863325119019
Loss at iteration 400 : 0.6552103757858276
Mean training loss eporch  260 :  0.6046208189340985
Loss at iteration 50 : 0.32309192419052124
Loss at iteration 100 : 0.2798437476158142
Loss at iteration 150 : 0.8797979354858398
Loss at iteration 200 : 0.9580838680267334
Loss at iteration 250 : 0.239019513130188
Loss at iteration 300 : 0.9249126315116882
Loss at iteration 350 : 0.4505442678928375
Loss at iteration 400 : 0.42667141556739807
Mean training loss eporch  261 :  0.60417862543878
Loss at iteration 50 : 0.9841398000717163
Loss at iteration 100 : 0.26853352785110474
Loss at iteration 150 : 0.38229119777679443
Loss at iteration 200 : 0.2317790389060974
Loss at iteration 250 : 0.8530072569847107
Loss at iteration 300 : 0.8024078011512756
Loss at iteration 350 : 0.8333513736724854
Loss at iteration 400 : 1.0817570686340332
Mean training loss eporch  262 :  0.6045694539298391
Loss at iteration 50 : 0.3730989098548889
Loss at iteration 100 : 0.5776442885398865
Loss at iteration 150 : 0.6619511842727661
Loss at iteration 200 : 0.7305232286453247
Loss at iteration 250 : 0.5914603471755981
Loss at iteration 300 : 0.6782522797584534
Loss at iteration 350 : 0.2971732020378113
Loss at iteration 400 : 0.4747089147567749
Mean training loss eporch  263 :  0.6038120804947588
Loss at iteration 50 : 0.33205652236938477
Loss at iteration 100 : 0.5470612049102783
Loss at iteration 150 : 0.5832499861717224
Loss at iteration 200 : 0.96260666847229
Loss at iteration 250 : 0.5516369938850403
Loss at iteration 300 : 0.30088573694229126
Loss at iteration 350 : 0.7436238527297974
Loss at iteration 400 : 0.43872278928756714
Mean training loss eporch  264 :  0.6041372116037014
Loss at iteration 50 : 0.49504464864730835
Loss at iteration 100 : 0.5206701755523682
Loss at iteration 150 : 1.1737934350967407
Loss at iteration 200 : 0.7821267247200012
Loss at iteration 250 : 0.6899068355560303
Loss at iteration 300 : 1.0184186697006226
Loss at iteration 350 : 0.6710475087165833
Loss at iteration 400 : 0.32576805353164673
Mean training loss eporch  265 :  0.6046179225292441
Loss at iteration 50 : 0.36079156398773193
Loss at iteration 100 : 1.01726233959198
Loss at iteration 150 : 0.448960542678833
Loss at iteration 200 : 0.9331041574478149
Loss at iteration 250 : 0.2672729790210724
Loss at iteration 300 : 0.4726424813270569
Loss at iteration 350 : 0.45398345589637756
Loss at iteration 400 : 0.8396329879760742
Mean training loss eporch  266 :  0.6077714608188702
Loss at iteration 50 : 0.48075032234191895
Loss at iteration 100 : 0.5924328565597534
Loss at iteration 150 : 0.4109651446342468
Loss at iteration 200 : 0.7904021739959717
Loss at iteration 250 : 1.2185256481170654
Loss at iteration 300 : 0.4226531982421875
Loss at iteration 350 : 0.35964760184288025
Loss at iteration 400 : 0.5366819500923157
Mean training loss eporch  267 :  0.6040512059101075
Loss at iteration 50 : 0.7230675220489502
Loss at iteration 100 : 0.5684145092964172
Loss at iteration 150 : 0.4486697316169739
Loss at iteration 200 : 0.9062859416007996
Loss at iteration 250 : 0.833416223526001
Loss at iteration 300 : 0.914656400680542
Loss at iteration 350 : 0.3309100866317749
Loss at iteration 400 : 0.8192377686500549
Mean training loss eporch  268 :  0.6041102695024066
Loss at iteration 50 : 0.5161648988723755
Loss at iteration 100 : 0.6280739307403564
Loss at iteration 150 : 0.8255703449249268
Loss at iteration 200 : 0.7288463115692139
Loss at iteration 250 : 0.44153034687042236
Loss at iteration 300 : 0.5977275371551514
Loss at iteration 350 : 0.28058740496635437
Loss at iteration 400 : 0.8859900832176208
Mean training loss eporch  269 :  0.6044243753557782
Loss at iteration 50 : 0.6746091842651367
Loss at iteration 100 : 0.4132300019264221
Loss at iteration 150 : 0.6059096455574036
Loss at iteration 200 : 0.47514212131500244
Loss at iteration 250 : 0.5733849406242371
Loss at iteration 300 : 0.611314058303833
Loss at iteration 350 : 0.8410300016403198
Loss at iteration 400 : 0.8321950435638428
Mean training loss eporch  270 :  0.6038931222306774
Loss at iteration 50 : 0.45989030599594116
Loss at iteration 100 : 1.1599102020263672
Loss at iteration 150 : 0.25329676270484924
Loss at iteration 200 : 0.6899875402450562
Loss at iteration 250 : 0.8199898600578308
Loss at iteration 300 : 0.73453688621521
Loss at iteration 350 : 1.1940338611602783
Loss at iteration 400 : 0.2403712272644043
Mean training loss eporch  271 :  0.6031046075738065
Loss at iteration 50 : 0.613701343536377
Loss at iteration 100 : 0.8705527782440186
Loss at iteration 150 : 0.3706327974796295
Loss at iteration 200 : 0.39730411767959595
Loss at iteration 250 : 0.646500825881958
Loss at iteration 300 : 0.7632738947868347
Loss at iteration 350 : 0.9262682199478149
Loss at iteration 400 : 1.8550997972488403
Mean training loss eporch  272 :  0.6042384771554994
Loss at iteration 50 : 0.40090394020080566
Loss at iteration 100 : 0.32386380434036255
Loss at iteration 150 : 0.7657197713851929
Loss at iteration 200 : 0.2886173725128174
Loss at iteration 250 : 0.3163321316242218
Loss at iteration 300 : 0.42657309770584106
Loss at iteration 350 : 0.25789350271224976
Loss at iteration 400 : 0.5952011942863464
Mean training loss eporch  273 :  0.6057877056133587
Loss at iteration 50 : 0.6437406539916992
Loss at iteration 100 : 1.0778393745422363
Loss at iteration 150 : 0.30543169379234314
Loss at iteration 200 : 0.2741270661354065
Loss at iteration 250 : 0.3481013774871826
Loss at iteration 300 : 0.19720473885536194
Loss at iteration 350 : 0.36976951360702515
Loss at iteration 400 : 0.7957545518875122
Mean training loss eporch  274 :  0.6056393940887109
Loss at iteration 50 : 0.741417407989502
Loss at iteration 100 : 0.30954933166503906
Loss at iteration 150 : 0.5765178799629211
Loss at iteration 200 : 0.6897556781768799
Loss at iteration 250 : 0.5979470610618591
Loss at iteration 300 : 0.3448551893234253
Loss at iteration 350 : 0.7260102033615112
Loss at iteration 400 : 0.7115810513496399
Mean training loss eporch  275 :  0.6083702544594025
Loss at iteration 50 : 0.6102771759033203
Loss at iteration 100 : 0.6970241069793701
Loss at iteration 150 : 0.6620151400566101
Loss at iteration 200 : 0.7235938906669617
Loss at iteration 250 : 0.9473941326141357
Loss at iteration 300 : 0.5016286373138428
Loss at iteration 350 : 0.34554803371429443
Loss at iteration 400 : 0.455744206905365
Mean training loss eporch  276 :  0.6038738123304106
Loss at iteration 50 : 0.5153080224990845
Loss at iteration 100 : 0.640301525592804
Loss at iteration 150 : 0.688412606716156
Loss at iteration 200 : 0.38145866990089417
Loss at iteration 250 : 0.7647557258605957
Loss at iteration 300 : 0.7002774477005005
Loss at iteration 350 : 1.0394734144210815
Loss at iteration 400 : 0.3950822353363037
Mean training loss eporch  277 :  0.6068853890615193
Loss at iteration 50 : 0.9280340075492859
Loss at iteration 100 : 0.9322031736373901
Loss at iteration 150 : 0.7063902616500854
Loss at iteration 200 : 0.3340844511985779
Loss at iteration 250 : 0.5302112698554993
Loss at iteration 300 : 0.9276161193847656
Loss at iteration 350 : 0.40530750155448914
Loss at iteration 400 : 0.36072495579719543
Mean training loss eporch  278 :  0.6073926068809
Loss at iteration 50 : 0.42627623677253723
Loss at iteration 100 : 0.3801068067550659
Loss at iteration 150 : 0.5827630758285522
Loss at iteration 200 : 0.7986699342727661
Loss at iteration 250 : 0.5135641098022461
Loss at iteration 300 : 0.5303178429603577
Loss at iteration 350 : 0.8361266255378723
Loss at iteration 400 : 1.0833957195281982
Mean training loss eporch  279 :  0.6036009873042192
Loss at iteration 50 : 0.661221981048584
Loss at iteration 100 : 0.3954554796218872
Loss at iteration 150 : 0.5304299592971802
Loss at iteration 200 : 0.6145283579826355
Loss at iteration 250 : 0.5023553371429443
Loss at iteration 300 : 0.9277439117431641
Loss at iteration 350 : 0.37851017713546753
Loss at iteration 400 : 0.49194180965423584
Mean training loss eporch  280 :  0.6036213530486475
Loss at iteration 50 : 0.4522634744644165
Loss at iteration 100 : 0.48898977041244507
Loss at iteration 150 : 0.5825039148330688
Loss at iteration 200 : 0.3658202886581421
Loss at iteration 250 : 0.6763213872909546
Loss at iteration 300 : 0.6604911088943481
Loss at iteration 350 : 0.5152506828308105
Loss at iteration 400 : 0.40364933013916016
Mean training loss eporch  281 :  0.6036709635915243
Loss at iteration 50 : 0.31064096093177795
Loss at iteration 100 : 0.5204408168792725
Loss at iteration 150 : 0.7173119783401489
Loss at iteration 200 : 0.44836243987083435
Loss at iteration 250 : 0.3457656502723694
Loss at iteration 300 : 0.2750921845436096
Loss at iteration 350 : 0.42558884620666504
Loss at iteration 400 : 0.28617754578590393
Mean training loss eporch  282 :  0.6053924510764969
Loss at iteration 50 : 0.29297465085983276
Loss at iteration 100 : 0.75150465965271
Loss at iteration 150 : 0.8211774826049805
Loss at iteration 200 : 0.6383523941040039
Loss at iteration 250 : 0.43282169103622437
Loss at iteration 300 : 0.5578732490539551
Loss at iteration 350 : 0.9683361053466797
Loss at iteration 400 : 0.32795238494873047
Mean training loss eporch  283 :  0.6034490615463578
Loss at iteration 50 : 0.270755410194397
Loss at iteration 100 : 0.9466593861579895
Loss at iteration 150 : 0.9986284971237183
Loss at iteration 200 : 0.8725179433822632
Loss at iteration 250 : 0.9965366125106812
Loss at iteration 300 : 0.3455727994441986
Loss at iteration 350 : 1.133387565612793
Loss at iteration 400 : 0.31393611431121826
Mean training loss eporch  284 :  0.603208602911421
Loss at iteration 50 : 0.8844698667526245
Loss at iteration 100 : 0.7569153308868408
Loss at iteration 150 : 0.7229105234146118
Loss at iteration 200 : 0.30011555552482605
Loss at iteration 250 : 0.7496457099914551
Loss at iteration 300 : 0.9063842296600342
Loss at iteration 350 : 1.0736483335494995
Loss at iteration 400 : 0.8467155694961548
Mean training loss eporch  285 :  0.6042048117571882
Loss at iteration 50 : 0.8764435648918152
Loss at iteration 100 : 1.03653883934021
Loss at iteration 150 : 0.7381730079650879
Loss at iteration 200 : 0.7750664353370667
Loss at iteration 250 : 0.8365039825439453
Loss at iteration 300 : 0.38601821660995483
Loss at iteration 350 : 0.3485059142112732
Loss at iteration 400 : 0.43086761236190796
Mean training loss eporch  286 :  0.6035271169358839
Loss at iteration 50 : 0.9916460514068604
Loss at iteration 100 : 0.3325536847114563
Loss at iteration 150 : 0.45952093601226807
Loss at iteration 200 : 0.5126864314079285
Loss at iteration 250 : 1.13535475730896
Loss at iteration 300 : 0.473218709230423
Loss at iteration 350 : 0.4049069881439209
Loss at iteration 400 : 0.3304087817668915
Mean training loss eporch  287 :  0.6045570875697607
Loss at iteration 50 : 0.689570426940918
Loss at iteration 100 : 0.6509690880775452
Loss at iteration 150 : 0.39995884895324707
Loss at iteration 200 : 0.3627173900604248
Loss at iteration 250 : 0.4744046926498413
Loss at iteration 300 : 0.6425368189811707
Loss at iteration 350 : 0.8201370239257812
Loss at iteration 400 : 0.6630674004554749
Mean training loss eporch  288 :  0.607802247620217
Loss at iteration 50 : 0.5136767625808716
Loss at iteration 100 : 0.7452226877212524
Loss at iteration 150 : 0.8921693563461304
Loss at iteration 200 : 0.3425368368625641
Loss at iteration 250 : 1.0602167844772339
Loss at iteration 300 : 0.5232875347137451
Loss at iteration 350 : 0.6313872337341309
Loss at iteration 400 : 0.9365326166152954
Mean training loss eporch  289 :  0.6034846593178975
Loss at iteration 50 : 0.605929434299469
Loss at iteration 100 : 1.0705480575561523
Loss at iteration 150 : 0.6110739707946777
Loss at iteration 200 : 0.2659643888473511
Loss at iteration 250 : 0.43737196922302246
Loss at iteration 300 : 0.4074273705482483
Loss at iteration 350 : 0.44887208938598633
Loss at iteration 400 : 0.512149453163147
Mean training loss eporch  290 :  0.6028372770334038
Loss at iteration 50 : 0.23732762038707733
Loss at iteration 100 : 0.428060382604599
Loss at iteration 150 : 0.710898756980896
Loss at iteration 200 : 0.6689133644104004
Loss at iteration 250 : 0.38446974754333496
Loss at iteration 300 : 0.30667316913604736
Loss at iteration 350 : 0.8475569486618042
Loss at iteration 400 : 0.7078966498374939
Mean training loss eporch  291 :  0.6038154530685579
Loss at iteration 50 : 0.4693235456943512
Loss at iteration 100 : 0.4325828552246094
Loss at iteration 150 : 0.44635674357414246
Loss at iteration 200 : 0.4686412513256073
Loss at iteration 250 : 0.3691171705722809
Loss at iteration 300 : 0.9133071899414062
Loss at iteration 350 : 0.46882155537605286
Loss at iteration 400 : 0.444466769695282
Mean training loss eporch  292 :  0.6035991716344795
Loss at iteration 50 : 0.3802233040332794
Loss at iteration 100 : 0.4583597779273987
Loss at iteration 150 : 0.40540969371795654
Loss at iteration 200 : 0.8993539810180664
Loss at iteration 250 : 0.8862544298171997
Loss at iteration 300 : 1.0397875308990479
Loss at iteration 350 : 0.36715731024742126
Loss at iteration 400 : 0.6296536922454834
Mean training loss eporch  293 :  0.6035634150619998
Loss at iteration 50 : 0.6145343780517578
Loss at iteration 100 : 0.2717607021331787
Loss at iteration 150 : 0.7190521955490112
Loss at iteration 200 : 0.5120153427124023
Loss at iteration 250 : 0.5299884080886841
Loss at iteration 300 : 0.9752185344696045
Loss at iteration 350 : 1.0709340572357178
Loss at iteration 400 : 0.7711795568466187
Mean training loss eporch  294 :  0.603395924673754
Loss at iteration 50 : 0.48113375902175903
Loss at iteration 100 : 0.672020435333252
Loss at iteration 150 : 0.4196969270706177
Loss at iteration 200 : 1.2161402702331543
Loss at iteration 250 : 1.1686980724334717
Loss at iteration 300 : 0.4104490876197815
Loss at iteration 350 : 0.4259487986564636
Loss at iteration 400 : 0.7038459181785583
Mean training loss eporch  295 :  0.6033299737380224
Loss at iteration 50 : 0.29257190227508545
Loss at iteration 100 : 0.3293769061565399
Loss at iteration 150 : 0.8543887138366699
Loss at iteration 200 : 0.33373507857322693
Loss at iteration 250 : 0.826535701751709
Loss at iteration 300 : 0.6724276542663574
Loss at iteration 350 : 0.8441556692123413
Loss at iteration 400 : 0.3137530982494354
Mean training loss eporch  296 :  0.6036616517690265
Loss at iteration 50 : 0.6930443644523621
Loss at iteration 100 : 0.5986121296882629
Loss at iteration 150 : 0.5846143960952759
Loss at iteration 200 : 0.7133238315582275
Loss at iteration 250 : 0.3092891573905945
Loss at iteration 300 : 0.5459057092666626
Loss at iteration 350 : 0.8390721082687378
Loss at iteration 400 : 0.5972813367843628
Mean training loss eporch  297 :  0.6064347839943497
Loss at iteration 50 : 0.3046530783176422
Loss at iteration 100 : 0.4270399212837219
Loss at iteration 150 : 0.3711380362510681
Loss at iteration 200 : 0.772608757019043
Loss at iteration 250 : 0.3337043821811676
Loss at iteration 300 : 0.29517316818237305
Loss at iteration 350 : 0.45865994691848755
Loss at iteration 400 : 0.3427465558052063
Mean training loss eporch  298 :  0.6042776341328706
Loss at iteration 50 : 0.23620563745498657
Loss at iteration 100 : 0.4393198490142822
Loss at iteration 150 : 1.1239557266235352
Loss at iteration 200 : 0.6364549398422241
Loss at iteration 250 : 0.43552136421203613
Loss at iteration 300 : 0.6643359065055847
Loss at iteration 350 : 0.3082706034183502
Loss at iteration 400 : 0.6850326657295227
Mean training loss eporch  299 :  0.6035260725515841
Loss at iteration 50 : 0.51606285572052
Loss at iteration 100 : 0.736897349357605
Loss at iteration 150 : 1.0539144277572632
Loss at iteration 200 : 1.1904886960983276
Loss at iteration 250 : 0.689615786075592
Loss at iteration 300 : 0.30398449301719666
Loss at iteration 350 : 0.4894445538520813
Loss at iteration 400 : 0.7241196036338806
Mean training loss eporch  300 :  0.6031554123144514
Loss at iteration 50 : 0.6377407908439636
Loss at iteration 100 : 0.233503520488739
Loss at iteration 150 : 0.5202325582504272
Loss at iteration 200 : 0.7157875299453735
Loss at iteration 250 : 1.1737152338027954
Loss at iteration 300 : 0.46531906723976135
Loss at iteration 350 : 1.1519032716751099
Loss at iteration 400 : 0.41387081146240234
Mean training loss eporch  301 :  0.6072712297062702
Loss at iteration 50 : 0.8838461637496948
Loss at iteration 100 : 0.6645464301109314
Loss at iteration 150 : 0.27260470390319824
Loss at iteration 200 : 0.6276682615280151
Loss at iteration 250 : 1.0057425498962402
Loss at iteration 300 : 0.3861393332481384
Loss at iteration 350 : 0.23594364523887634
Loss at iteration 400 : 0.3951355218887329
Mean training loss eporch  302 :  0.6029563409531064
Loss at iteration 50 : 0.6981712579727173
Loss at iteration 100 : 0.3853786587715149
Loss at iteration 150 : 0.7634916305541992
Loss at iteration 200 : 0.3281044065952301
Loss at iteration 250 : 1.0595614910125732
Loss at iteration 300 : 0.9317177534103394
Loss at iteration 350 : 0.9430257081985474
Loss at iteration 400 : 0.40613389015197754
Mean training loss eporch  303 :  0.6030759680097413
Loss at iteration 50 : 0.6993788480758667
Loss at iteration 100 : 0.7823948860168457
Loss at iteration 150 : 0.4822451174259186
Loss at iteration 200 : 0.685475766658783
Loss at iteration 250 : 0.2666085660457611
Loss at iteration 300 : 0.38934993743896484
Loss at iteration 350 : 0.4774988889694214
Loss at iteration 400 : 0.6305074691772461
Mean training loss eporch  304 :  0.6029072342098026
Loss at iteration 50 : 0.422256201505661
Loss at iteration 100 : 0.7803460955619812
Loss at iteration 150 : 1.1344518661499023
Loss at iteration 200 : 0.6827006936073303
Loss at iteration 250 : 0.2638821601867676
Loss at iteration 300 : 0.328215628862381
Loss at iteration 350 : 0.5336489081382751
Loss at iteration 400 : 0.39500802755355835
Mean training loss eporch  305 :  0.6025294536900093
Loss at iteration 50 : 0.6812946200370789
Loss at iteration 100 : 0.25006303191185
Loss at iteration 150 : 0.8077381253242493
Loss at iteration 200 : 0.3680392801761627
Loss at iteration 250 : 0.2983414828777313
Loss at iteration 300 : 0.5316078662872314
Loss at iteration 350 : 0.754879355430603
Loss at iteration 400 : 0.624923050403595
Mean training loss eporch  306 :  0.6033845445299897
Loss at iteration 50 : 0.7327040433883667
Loss at iteration 100 : 1.0129045248031616
Loss at iteration 150 : 0.21661293506622314
Loss at iteration 200 : 0.368647038936615
Loss at iteration 250 : 0.40416595339775085
Loss at iteration 300 : 0.7703752517700195
Loss at iteration 350 : 0.4612351059913635
Loss at iteration 400 : 0.4615415334701538
Mean training loss eporch  307 :  0.6030325456250943
Loss at iteration 50 : 0.5141608715057373
Loss at iteration 100 : 0.7749736905097961
Loss at iteration 150 : 0.5613724589347839
Loss at iteration 200 : 0.6475968360900879
Loss at iteration 250 : 0.49699273705482483
Loss at iteration 300 : 0.5268455743789673
Loss at iteration 350 : 0.43621161580085754
Loss at iteration 400 : 0.5373366475105286
Mean training loss eporch  308 :  0.6034667580117025
Loss at iteration 50 : 0.20032571256160736
Loss at iteration 100 : 0.7459852695465088
Loss at iteration 150 : 0.6074936389923096
Loss at iteration 200 : 0.26910698413848877
Loss at iteration 250 : 0.24973593652248383
Loss at iteration 300 : 0.752631664276123
Loss at iteration 350 : 0.32676997780799866
Loss at iteration 400 : 1.074371337890625
Mean training loss eporch  309 :  0.6034669916191443
Loss at iteration 50 : 0.5378101468086243
Loss at iteration 100 : 0.7089133262634277
Loss at iteration 150 : 1.1526894569396973
Loss at iteration 200 : 0.3206346333026886
Loss at iteration 250 : 0.7495632171630859
Loss at iteration 300 : 0.6416735649108887
Loss at iteration 350 : 0.679125189781189
Loss at iteration 400 : 0.32034313678741455
Mean training loss eporch  310 :  0.6025148923822048
Loss at iteration 50 : 0.386174738407135
Loss at iteration 100 : 0.6588078141212463
Loss at iteration 150 : 0.4853416383266449
Loss at iteration 200 : 0.48937422037124634
Loss at iteration 250 : 0.30717453360557556
Loss at iteration 300 : 0.6999248266220093
Loss at iteration 350 : 0.45146775245666504
Loss at iteration 400 : 0.4837390184402466
Mean training loss eporch  311 :  0.6027574525486193
Loss at iteration 50 : 0.5097405314445496
Loss at iteration 100 : 1.255960464477539
Loss at iteration 150 : 0.6081218719482422
Loss at iteration 200 : 1.1475660800933838
Loss at iteration 250 : 0.8726646304130554
Loss at iteration 300 : 1.112226128578186
Loss at iteration 350 : 0.2632650136947632
Loss at iteration 400 : 0.2902892231941223
Mean training loss eporch  312 :  0.6034992564085353
Loss at iteration 50 : 0.5153892040252686
Loss at iteration 100 : 0.3827510178089142
Loss at iteration 150 : 0.517273485660553
Loss at iteration 200 : 0.49500951170921326
Loss at iteration 250 : 0.8055976629257202
Loss at iteration 300 : 1.5310368537902832
Loss at iteration 350 : 0.7732334136962891
Loss at iteration 400 : 0.8191227912902832
Mean training loss eporch  313 :  0.6085102526729952
Loss at iteration 50 : 0.46962815523147583
Loss at iteration 100 : 0.9119789004325867
Loss at iteration 150 : 0.5021799802780151
Loss at iteration 200 : 0.4817690849304199
Loss at iteration 250 : 0.792752742767334
Loss at iteration 300 : 0.6189931631088257
Loss at iteration 350 : 0.4818064272403717
Loss at iteration 400 : 0.4383958578109741
Mean training loss eporch  314 :  0.6032751140439457
Loss at iteration 50 : 0.5200047492980957
Loss at iteration 100 : 0.8210108280181885
Loss at iteration 150 : 0.8004738092422485
Loss at iteration 200 : 0.5966838598251343
Loss at iteration 250 : 0.5167476534843445
Loss at iteration 300 : 0.42538541555404663
Loss at iteration 350 : 0.8097366094589233
Loss at iteration 400 : 0.5791657567024231
Mean training loss eporch  315 :  0.6034216490561652
Loss at iteration 50 : 0.25966963171958923
Loss at iteration 100 : 0.7101138830184937
Loss at iteration 150 : 0.5196631550788879
Loss at iteration 200 : 0.6545106172561646
Loss at iteration 250 : 0.7649130821228027
Loss at iteration 300 : 0.724678635597229
Loss at iteration 350 : 0.22603803873062134
Loss at iteration 400 : 0.2665308713912964
Mean training loss eporch  316 :  0.6041272606349847
Loss at iteration 50 : 0.39351993799209595
Loss at iteration 100 : 0.5323951840400696
Loss at iteration 150 : 0.49272724986076355
Loss at iteration 200 : 1.0330841541290283
Loss at iteration 250 : 0.42828282713890076
Loss at iteration 300 : 0.8755518198013306
Loss at iteration 350 : 0.26247042417526245
Loss at iteration 400 : 0.48593905568122864
Mean training loss eporch  317 :  0.6046617487487237
Loss at iteration 50 : 0.745093822479248
Loss at iteration 100 : 0.39238590002059937
Loss at iteration 150 : 0.4298350214958191
Loss at iteration 200 : 0.6297774314880371
Loss at iteration 250 : 0.4114915728569031
Loss at iteration 300 : 0.4729467034339905
Loss at iteration 350 : 0.6015337109565735
Loss at iteration 400 : 0.5600296854972839
Mean training loss eporch  318 :  0.6031837382859179
Loss at iteration 50 : 0.5103285908699036
Loss at iteration 100 : 0.7608434557914734
Loss at iteration 150 : 0.3202514052391052
Loss at iteration 200 : 0.3958013653755188
Loss at iteration 250 : 0.3722766935825348
Loss at iteration 300 : 0.4156183898448944
Loss at iteration 350 : 0.20450572669506073
Loss at iteration 400 : 0.4647742509841919
Mean training loss eporch  319 :  0.6028827690757443
Loss at iteration 50 : 0.4156349301338196
Loss at iteration 100 : 0.7885783910751343
Loss at iteration 150 : 0.6053529381752014
Loss at iteration 200 : 0.764403223991394
Loss at iteration 250 : 0.3626522719860077
Loss at iteration 300 : 0.7970149517059326
Loss at iteration 350 : 1.2176063060760498
Loss at iteration 400 : 0.9044185280799866
Mean training loss eporch  320 :  0.6026830676224734
Loss at iteration 50 : 1.0927138328552246
Loss at iteration 100 : 0.3928118050098419
Loss at iteration 150 : 0.691338062286377
Loss at iteration 200 : 0.7221739292144775
Loss at iteration 250 : 0.39080899953842163
Loss at iteration 300 : 0.7043499946594238
Loss at iteration 350 : 0.7857625484466553
Loss at iteration 400 : 0.9108452796936035
Mean training loss eporch  321 :  0.6027161830810688
Loss at iteration 50 : 0.6058993339538574
Loss at iteration 100 : 0.5898305773735046
Loss at iteration 150 : 0.530891478061676
Loss at iteration 200 : 0.8393311500549316
Loss at iteration 250 : 0.6992146968841553
Loss at iteration 300 : 0.5975514650344849
Loss at iteration 350 : 0.7430357933044434
Loss at iteration 400 : 0.5048562288284302
Mean training loss eporch  322 :  0.6028509949889418
Loss at iteration 50 : 0.5107197165489197
Loss at iteration 100 : 0.4747234284877777
Loss at iteration 150 : 0.7424885034561157
Loss at iteration 200 : 0.7680959701538086
Loss at iteration 250 : 0.6556225419044495
Loss at iteration 300 : 0.7654260993003845
Loss at iteration 350 : 0.4651293456554413
Loss at iteration 400 : 0.6454808712005615
Mean training loss eporch  323 :  0.6030153837973762
Loss at iteration 50 : 0.9703959822654724
Loss at iteration 100 : 0.32394468784332275
Loss at iteration 150 : 0.9035990238189697
Loss at iteration 200 : 0.5724559426307678
Loss at iteration 250 : 0.7793205976486206
Loss at iteration 300 : 0.641157329082489
Loss at iteration 350 : 0.4826330244541168
Loss at iteration 400 : 0.725849986076355
Mean training loss eporch  324 :  0.6031370767697091
Loss at iteration 50 : 0.8088042736053467
Loss at iteration 100 : 0.39696553349494934
Loss at iteration 150 : 0.6085770726203918
Loss at iteration 200 : 0.4589192271232605
Loss at iteration 250 : 0.39563247561454773
Loss at iteration 300 : 0.918293833732605
Loss at iteration 350 : 0.6053325533866882
Loss at iteration 400 : 1.1671295166015625
Mean training loss eporch  325 :  0.6033188871738622
Loss at iteration 50 : 0.6787270307540894
Loss at iteration 100 : 0.7348684072494507
Loss at iteration 150 : 1.4533449411392212
Loss at iteration 200 : 0.540630042552948
Loss at iteration 250 : 0.5834652185440063
Loss at iteration 300 : 0.3951789140701294
Loss at iteration 350 : 0.2743627727031708
Loss at iteration 400 : 0.7608587741851807
Mean training loss eporch  326 :  0.6028692468733531
Loss at iteration 50 : 0.8348621129989624
Loss at iteration 100 : 0.5280537605285645
Loss at iteration 150 : 0.8979660272598267
Loss at iteration 200 : 0.718085527420044
Loss at iteration 250 : 0.40001943707466125
Loss at iteration 300 : 0.6721281409263611
Loss at iteration 350 : 0.47712016105651855
Loss at iteration 400 : 0.4525235891342163
Mean training loss eporch  327 :  0.602458928832826
Loss at iteration 50 : 0.6989157795906067
Loss at iteration 100 : 0.8259086608886719
Loss at iteration 150 : 0.4618246555328369
Loss at iteration 200 : 0.4753623306751251
Loss at iteration 250 : 0.7813011407852173
Loss at iteration 300 : 0.4940086603164673
Loss at iteration 350 : 0.5350082516670227
Loss at iteration 400 : 0.4261658787727356
Mean training loss eporch  328 :  0.60228260095344
Loss at iteration 50 : 0.7153489589691162
Loss at iteration 100 : 0.40753495693206787
Loss at iteration 150 : 0.4248301088809967
Loss at iteration 200 : 0.40682515501976013
Loss at iteration 250 : 0.4645325541496277
Loss at iteration 300 : 0.8028401136398315
Loss at iteration 350 : 0.937591552734375
Loss at iteration 400 : 1.2354568243026733
Mean training loss eporch  329 :  0.602918209015254
Loss at iteration 50 : 0.6724132895469666
Loss at iteration 100 : 0.28445762395858765
Loss at iteration 150 : 0.33980852365493774
Loss at iteration 200 : 0.3236169219017029
Loss at iteration 250 : 0.8262249827384949
Loss at iteration 300 : 0.7123998403549194
Loss at iteration 350 : 1.089342474937439
Loss at iteration 400 : 0.5837669968605042
Mean training loss eporch  330 :  0.6020696508643874
Loss at iteration 50 : 0.34023016691207886
Loss at iteration 100 : 0.9625260829925537
Loss at iteration 150 : 0.6639083623886108
Loss at iteration 200 : 0.4701785445213318
Loss at iteration 250 : 0.502159595489502
Loss at iteration 300 : 0.898099422454834
Loss at iteration 350 : 1.0383315086364746
Loss at iteration 400 : 0.42716875672340393
Mean training loss eporch  331 :  0.6027104792773991
Loss at iteration 50 : 0.4335564374923706
Loss at iteration 100 : 0.4447505474090576
Loss at iteration 150 : 0.2800899147987366
Loss at iteration 200 : 0.2903710901737213
Loss at iteration 250 : 0.4071807861328125
Loss at iteration 300 : 0.8074675798416138
Loss at iteration 350 : 0.24005337059497833
Loss at iteration 400 : 0.38220059871673584
Mean training loss eporch  332 :  0.6027260719393401
Loss at iteration 50 : 0.6441469192504883
Loss at iteration 100 : 1.1029812097549438
Loss at iteration 150 : 0.45090341567993164
Loss at iteration 200 : 0.5792610049247742
Loss at iteration 250 : 0.6018404364585876
Loss at iteration 300 : 0.3310620188713074
Loss at iteration 350 : 0.6036520600318909
Loss at iteration 400 : 1.163754940032959
Mean training loss eporch  333 :  0.6022190677812281
Loss at iteration 50 : 0.36203983426094055
Loss at iteration 100 : 0.28365960717201233
Loss at iteration 150 : 0.4433633089065552
Loss at iteration 200 : 0.621228814125061
Loss at iteration 250 : 0.5537101030349731
Loss at iteration 300 : 1.040259599685669
Loss at iteration 350 : 0.7884582281112671
Loss at iteration 400 : 0.3207705020904541
Mean training loss eporch  334 :  0.6024144966105175
Loss at iteration 50 : 0.9295654296875
Loss at iteration 100 : 0.4164942502975464
Loss at iteration 150 : 0.4355965852737427
Loss at iteration 200 : 0.8006595373153687
Loss at iteration 250 : 0.6676630973815918
Loss at iteration 300 : 0.29508137702941895
Loss at iteration 350 : 0.3841624855995178
Loss at iteration 400 : 0.21901853382587433
Mean training loss eporch  335 :  0.6023888519793883
Loss at iteration 50 : 0.8346496820449829
Loss at iteration 100 : 0.2563551068305969
Loss at iteration 150 : 0.6588793396949768
Loss at iteration 200 : 0.4085390567779541
Loss at iteration 250 : 0.7316489219665527
Loss at iteration 300 : 0.6316618919372559
Loss at iteration 350 : 1.1667873859405518
Loss at iteration 400 : 0.5617206692695618
Mean training loss eporch  336 :  0.6061085739210582
Loss at iteration 50 : 0.8183472156524658
Loss at iteration 100 : 1.0647079944610596
Loss at iteration 150 : 1.0567481517791748
Loss at iteration 200 : 0.5835131406784058
Loss at iteration 250 : 0.3422483205795288
Loss at iteration 300 : 1.1583600044250488
Loss at iteration 350 : 0.7591099739074707
Loss at iteration 400 : 0.3265591859817505
Mean training loss eporch  337 :  0.6046947737312103
Loss at iteration 50 : 0.6538812518119812
Loss at iteration 100 : 0.9735615849494934
Loss at iteration 150 : 0.614151120185852
Loss at iteration 200 : 0.2640947997570038
Loss at iteration 250 : 0.2916281819343567
Loss at iteration 300 : 0.654853343963623
Loss at iteration 350 : 0.33038458228111267
Loss at iteration 400 : 0.708324670791626
Mean training loss eporch  338 :  0.6028647857129307
Loss at iteration 50 : 0.5701436996459961
Loss at iteration 100 : 0.9197565913200378
Loss at iteration 150 : 0.7288521528244019
Loss at iteration 200 : 0.5464972257614136
Loss at iteration 250 : 0.5390076041221619
Loss at iteration 300 : 0.7289817929267883
Loss at iteration 350 : 0.4925422668457031
Loss at iteration 400 : 0.5100393295288086
Mean training loss eporch  339 :  0.6022406706561422
Loss at iteration 50 : 0.40944188833236694
Loss at iteration 100 : 0.3530384302139282
Loss at iteration 150 : 0.9690905809402466
Loss at iteration 200 : 0.46272701025009155
Loss at iteration 250 : 0.4010879695415497
Loss at iteration 300 : 0.8519651889801025
Loss at iteration 350 : 0.4545571804046631
Loss at iteration 400 : 0.7358677387237549
Mean training loss eporch  340 :  0.6023564966136564
Loss at iteration 50 : 0.44053134322166443
Loss at iteration 100 : 0.3664991855621338
Loss at iteration 150 : 0.8660464882850647
Loss at iteration 200 : 0.49354955554008484
Loss at iteration 250 : 0.47449690103530884
Loss at iteration 300 : 0.7492077946662903
Loss at iteration 350 : 0.4736744463443756
Loss at iteration 400 : 0.4504150450229645
Mean training loss eporch  341 :  0.6047878068926088
Loss at iteration 50 : 0.3277648985385895
Loss at iteration 100 : 1.08094322681427
Loss at iteration 150 : 0.4529644250869751
Loss at iteration 200 : 0.5094865560531616
Loss at iteration 250 : 0.8536734580993652
Loss at iteration 300 : 1.082099199295044
Loss at iteration 350 : 0.3975900411605835
Loss at iteration 400 : 0.3690667748451233
Mean training loss eporch  342 :  0.6032098956706813
Loss at iteration 50 : 0.5904174447059631
Loss at iteration 100 : 0.26658010482788086
Loss at iteration 150 : 0.37996047735214233
Loss at iteration 200 : 0.49222320318222046
Loss at iteration 250 : 0.5843831300735474
Loss at iteration 300 : 0.48445188999176025
Loss at iteration 350 : 0.5220680236816406
Loss at iteration 400 : 0.6152540445327759
Mean training loss eporch  343 :  0.6080349674740715
Loss at iteration 50 : 0.32843783497810364
Loss at iteration 100 : 0.6351915001869202
Loss at iteration 150 : 0.41263991594314575
Loss at iteration 200 : 0.49131497740745544
Loss at iteration 250 : 0.5962027311325073
Loss at iteration 300 : 0.3162483870983124
Loss at iteration 350 : 0.46239152550697327
Loss at iteration 400 : 0.8958604335784912
Mean training loss eporch  344 :  0.6026879696367567
Loss at iteration 50 : 0.6708936095237732
Loss at iteration 100 : 0.5579231977462769
Loss at iteration 150 : 0.5376933217048645
Loss at iteration 200 : 0.42660242319107056
Loss at iteration 250 : 0.314972460269928
Loss at iteration 300 : 0.727775514125824
Loss at iteration 350 : 0.5196918845176697
Loss at iteration 400 : 0.9894529581069946
Mean training loss eporch  345 :  0.6019402849300025
Loss at iteration 50 : 0.45899516344070435
Loss at iteration 100 : 0.8069518804550171
Loss at iteration 150 : 0.25611579418182373
Loss at iteration 200 : 0.28669968247413635
Loss at iteration 250 : 0.5856363773345947
Loss at iteration 300 : 0.3333156108856201
Loss at iteration 350 : 1.0199947357177734
Loss at iteration 400 : 0.266084223985672
Mean training loss eporch  346 :  0.6024515866364599
Loss at iteration 50 : 0.2665855884552002
Loss at iteration 100 : 0.4098837077617645
Loss at iteration 150 : 0.5764688849449158
Loss at iteration 200 : 0.35064297914505005
Loss at iteration 250 : 0.8162959814071655
Loss at iteration 300 : 0.9611945748329163
Loss at iteration 350 : 0.4594286382198334
Loss at iteration 400 : 0.438493013381958
Mean training loss eporch  347 :  0.6022501919301636
Loss at iteration 50 : 0.45068538188934326
Loss at iteration 100 : 0.8700469732284546
Loss at iteration 150 : 0.3662026822566986
Loss at iteration 200 : 0.8645190000534058
Loss at iteration 250 : 0.6644330620765686
Loss at iteration 300 : 0.4097962975502014
Loss at iteration 350 : 0.45020413398742676
Loss at iteration 400 : 0.6867268085479736
Mean training loss eporch  348 :  0.6027002115086589
Loss at iteration 50 : 0.27336451411247253
Loss at iteration 100 : 0.4596480131149292
Loss at iteration 150 : 0.9278198480606079
Loss at iteration 200 : 0.8742542266845703
Loss at iteration 250 : 0.3432472348213196
Loss at iteration 300 : 0.3485336899757385
Loss at iteration 350 : 0.39960724115371704
Loss at iteration 400 : 0.6106282472610474
Mean training loss eporch  349 :  0.6028278289287614
Loss at iteration 50 : 0.64360511302948
Loss at iteration 100 : 0.23683173954486847
Loss at iteration 150 : 0.2968199551105499
Loss at iteration 200 : 0.5155874490737915
Loss at iteration 250 : 0.7218101024627686
Loss at iteration 300 : 0.593761682510376
Loss at iteration 350 : 0.5507720708847046
Loss at iteration 400 : 0.7558895945549011
Mean training loss eporch  350 :  0.6025719962045216
Loss at iteration 50 : 0.7409229874610901
Loss at iteration 100 : 0.48252934217453003
Loss at iteration 150 : 0.32272982597351074
Loss at iteration 200 : 1.2420156002044678
Loss at iteration 250 : 0.39050114154815674
Loss at iteration 300 : 0.8526319265365601
Loss at iteration 350 : 0.3533681035041809
Loss at iteration 400 : 0.42935290932655334
Mean training loss eporch  351 :  0.6053481987706749
Loss at iteration 50 : 0.6808153390884399
Loss at iteration 100 : 0.8593651652336121
Loss at iteration 150 : 0.25548332929611206
Loss at iteration 200 : 0.3203139007091522
Loss at iteration 250 : 0.9972297549247742
Loss at iteration 300 : 0.4446883201599121
Loss at iteration 350 : 0.7047454118728638
Loss at iteration 400 : 0.266787052154541
Mean training loss eporch  352 :  0.606159023138707
Loss at iteration 50 : 0.531899094581604
Loss at iteration 100 : 1.032142162322998
Loss at iteration 150 : 0.4401509165763855
Loss at iteration 200 : 0.7138761281967163
Loss at iteration 250 : 0.4918501377105713
Loss at iteration 300 : 0.38795381784439087
Loss at iteration 350 : 0.3847149610519409
Loss at iteration 400 : 0.3060777485370636
Mean training loss eporch  353 :  0.6024692023147916
Loss at iteration 50 : 0.32560300827026367
Loss at iteration 100 : 0.7265295386314392
Loss at iteration 150 : 0.3309270739555359
Loss at iteration 200 : 0.6337501406669617
Loss at iteration 250 : 0.5459940433502197
Loss at iteration 300 : 0.8424729108810425
Loss at iteration 350 : 0.43830373883247375
Loss at iteration 400 : 0.3562477231025696
Mean training loss eporch  354 :  0.6025190174312335
Loss at iteration 50 : 0.8310501575469971
Loss at iteration 100 : 0.7479935884475708
Loss at iteration 150 : 0.3533506989479065
Loss at iteration 200 : 0.6326385736465454
Loss at iteration 250 : 0.2742443084716797
Loss at iteration 300 : 1.0697327852249146
Loss at iteration 350 : 1.128589391708374
Loss at iteration 400 : 0.7263688445091248
Mean training loss eporch  355 :  0.6042901408846069
Loss at iteration 50 : 0.6273153424263
Loss at iteration 100 : 0.8309120535850525
Loss at iteration 150 : 0.8233859539031982
Loss at iteration 200 : 0.3439665138721466
Loss at iteration 250 : 1.3790184259414673
Loss at iteration 300 : 0.6968374848365784
Loss at iteration 350 : 1.0137608051300049
Loss at iteration 400 : 0.5013982057571411
Mean training loss eporch  356 :  0.6019743507218468
Loss at iteration 50 : 0.28612613677978516
Loss at iteration 100 : 0.5972492694854736
Loss at iteration 150 : 0.2827572226524353
Loss at iteration 200 : 0.6138550639152527
Loss at iteration 250 : 0.5157854557037354
Loss at iteration 300 : 0.6201744675636292
Loss at iteration 350 : 0.2292400598526001
Loss at iteration 400 : 0.46668362617492676
Mean training loss eporch  357 :  0.6029793659534155
Loss at iteration 50 : 1.0545967817306519
Loss at iteration 100 : 0.861311137676239
Loss at iteration 150 : 0.44940394163131714
Loss at iteration 200 : 0.518256664276123
Loss at iteration 250 : 0.8254150152206421
Loss at iteration 300 : 0.6408761739730835
Loss at iteration 350 : 0.9635797142982483
Loss at iteration 400 : 0.6160100698471069
Mean training loss eporch  358 :  0.6020048909524096
Loss at iteration 50 : 0.4961060583591461
Loss at iteration 100 : 0.5066890716552734
Loss at iteration 150 : 0.36312100291252136
Loss at iteration 200 : 0.6651411056518555
Loss at iteration 250 : 0.7348895072937012
Loss at iteration 300 : 0.5187233686447144
Loss at iteration 350 : 1.0846481323242188
Loss at iteration 400 : 0.38587087392807007
Mean training loss eporch  359 :  0.6019908086033413
Loss at iteration 50 : 0.47703585028648376
Loss at iteration 100 : 0.9034535884857178
Loss at iteration 150 : 0.6909078359603882
Loss at iteration 200 : 0.6962997913360596
Loss at iteration 250 : 0.9985339641571045
Loss at iteration 300 : 0.6528958082199097
Loss at iteration 350 : 0.5186620354652405
Loss at iteration 400 : 0.6865004897117615
Mean training loss eporch  360 :  0.6027433763638206
Loss at iteration 50 : 0.4183967709541321
Loss at iteration 100 : 0.5042659044265747
Loss at iteration 150 : 0.6547797322273254
Loss at iteration 200 : 0.545899510383606
Loss at iteration 250 : 0.5027544498443604
Loss at iteration 300 : 0.505231499671936
Loss at iteration 350 : 0.9974637627601624
Loss at iteration 400 : 0.9590240716934204
Mean training loss eporch  361 :  0.6043610874179233
Loss at iteration 50 : 0.380928099155426
Loss at iteration 100 : 0.993361234664917
Loss at iteration 150 : 0.8074830174446106
Loss at iteration 200 : 0.5199223756790161
Loss at iteration 250 : 0.3157159090042114
Loss at iteration 300 : 1.1448692083358765
Loss at iteration 350 : 0.5509544610977173
Loss at iteration 400 : 0.833609402179718
Mean training loss eporch  362 :  0.6020599175328096
Loss at iteration 50 : 0.7300230264663696
Loss at iteration 100 : 0.4173131585121155
Loss at iteration 150 : 0.9997669458389282
Loss at iteration 200 : 0.9949547052383423
Loss at iteration 250 : 0.7348476648330688
Loss at iteration 300 : 0.34480300545692444
Loss at iteration 350 : 1.252833604812622
Loss at iteration 400 : 0.44838303327560425
Mean training loss eporch  363 :  0.6027731110630014
Loss at iteration 50 : 0.5607554316520691
Loss at iteration 100 : 0.6205524206161499
Loss at iteration 150 : 0.569412112236023
Loss at iteration 200 : 0.5936744213104248
Loss at iteration 250 : 1.096174716949463
Loss at iteration 300 : 0.43049395084381104
Loss at iteration 350 : 0.2500028610229492
Loss at iteration 400 : 1.2081364393234253
Mean training loss eporch  364 :  0.6013200822632944
Loss at iteration 50 : 0.5440071821212769
Loss at iteration 100 : 1.038757562637329
Loss at iteration 150 : 0.40357208251953125
Loss at iteration 200 : 0.7742846012115479
Loss at iteration 250 : 0.5372942090034485
Loss at iteration 300 : 0.5949643850326538
Loss at iteration 350 : 0.8356611132621765
Loss at iteration 400 : 0.8403195142745972
Mean training loss eporch  365 :  0.6022417665262928
Loss at iteration 50 : 0.2543849050998688
Loss at iteration 100 : 0.6190424561500549
Loss at iteration 150 : 0.712562084197998
Loss at iteration 200 : 0.34696972370147705
Loss at iteration 250 : 0.8517887592315674
Loss at iteration 300 : 0.6636052131652832
Loss at iteration 350 : 0.26371777057647705
Loss at iteration 400 : 0.64029860496521
Mean training loss eporch  366 :  0.6037725141363828
Loss at iteration 50 : 0.2623913884162903
Loss at iteration 100 : 0.5189657807350159
Loss at iteration 150 : 0.45593565702438354
Loss at iteration 200 : 0.38182902336120605
Loss at iteration 250 : 0.8960151672363281
Loss at iteration 300 : 0.6301795244216919
Loss at iteration 350 : 0.8617123365402222
Loss at iteration 400 : 0.4172668159008026
Mean training loss eporch  367 :  0.6014527021301701
Loss at iteration 50 : 0.2989271283149719
Loss at iteration 100 : 1.4512498378753662
Loss at iteration 150 : 0.9382613897323608
Loss at iteration 200 : 0.42040765285491943
Loss at iteration 250 : 0.862160325050354
Loss at iteration 300 : 0.6598160266876221
Loss at iteration 350 : 0.8392136693000793
Loss at iteration 400 : 0.4794490337371826
Mean training loss eporch  368 :  0.6032774295707989
Loss at iteration 50 : 0.33312368392944336
Loss at iteration 100 : 0.5244328379631042
Loss at iteration 150 : 0.36569154262542725
Loss at iteration 200 : 0.4981808066368103
Loss at iteration 250 : 0.3999228775501251
Loss at iteration 300 : 0.5813015699386597
Loss at iteration 350 : 0.6270155310630798
Loss at iteration 400 : 0.8154826164245605
Mean training loss eporch  369 :  0.6021865919031905
Loss at iteration 50 : 0.30475345253944397
Loss at iteration 100 : 0.9596416354179382
Loss at iteration 150 : 0.6976979970932007
Loss at iteration 200 : 0.33540084958076477
Loss at iteration 250 : 0.5770176649093628
Loss at iteration 300 : 0.45966023206710815
Loss at iteration 350 : 0.7202794551849365
Loss at iteration 400 : 0.3213879466056824
Mean training loss eporch  370 :  0.6020592068209242
Loss at iteration 50 : 0.405105322599411
Loss at iteration 100 : 0.2289932519197464
Loss at iteration 150 : 0.30278217792510986
Loss at iteration 200 : 0.5651789307594299
Loss at iteration 250 : 0.542773425579071
Loss at iteration 300 : 0.7444831132888794
Loss at iteration 350 : 0.6473807692527771
Loss at iteration 400 : 0.32678407430648804
Mean training loss eporch  371 :  0.6021775258496204
Loss at iteration 50 : 0.7633596658706665
Loss at iteration 100 : 0.6799798607826233
Loss at iteration 150 : 0.760456383228302
Loss at iteration 200 : 0.680919885635376
Loss at iteration 250 : 0.4982045888900757
Loss at iteration 300 : 0.5377417802810669
Loss at iteration 350 : 0.5306931734085083
Loss at iteration 400 : 1.2051702737808228
Mean training loss eporch  372 :  0.6020232599078272
Loss at iteration 50 : 0.999994695186615
Loss at iteration 100 : 0.3805563449859619
Loss at iteration 150 : 0.29086169600486755
Loss at iteration 200 : 0.567165732383728
Loss at iteration 250 : 0.2406632900238037
Loss at iteration 300 : 0.6840462684631348
Loss at iteration 350 : 0.3885682225227356
Loss at iteration 400 : 0.6865322589874268
Mean training loss eporch  373 :  0.6051460060838092
Loss at iteration 50 : 0.674984335899353
Loss at iteration 100 : 0.7945802211761475
Loss at iteration 150 : 0.3686172664165497
Loss at iteration 200 : 0.7843018770217896
Loss at iteration 250 : 0.49162232875823975
Loss at iteration 300 : 0.9230858087539673
Loss at iteration 350 : 0.3757461905479431
Loss at iteration 400 : 0.3396483063697815
Mean training loss eporch  374 :  0.6023563562888201
Loss at iteration 50 : 0.9625282287597656
Loss at iteration 100 : 0.45447397232055664
Loss at iteration 150 : 1.0679978132247925
Loss at iteration 200 : 0.46024197340011597
Loss at iteration 250 : 0.3504430055618286
Loss at iteration 300 : 1.3199481964111328
Loss at iteration 350 : 0.9738085269927979
Loss at iteration 400 : 0.7497405409812927
Mean training loss eporch  375 :  0.6015635087016986
Loss at iteration 50 : 0.4815906882286072
Loss at iteration 100 : 0.8620800971984863
Loss at iteration 150 : 0.5242316722869873
Loss at iteration 200 : 0.987627387046814
Loss at iteration 250 : 0.7377195358276367
Loss at iteration 300 : 0.3127690851688385
Loss at iteration 350 : 0.4851062297821045
Loss at iteration 400 : 0.563165545463562
Mean training loss eporch  376 :  0.6023253709278299
Loss at iteration 50 : 0.3048080801963806
Loss at iteration 100 : 0.605522096157074
Loss at iteration 150 : 0.23972105979919434
Loss at iteration 200 : 0.6466400027275085
Loss at iteration 250 : 1.1014807224273682
Loss at iteration 300 : 0.36655953526496887
Loss at iteration 350 : 0.3285171389579773
Loss at iteration 400 : 0.7465927600860596
Mean training loss eporch  377 :  0.602153489107241
Loss at iteration 50 : 0.5098457336425781
Loss at iteration 100 : 0.6845327615737915
Loss at iteration 150 : 0.7887351512908936
Loss at iteration 200 : 0.562372088432312
Loss at iteration 250 : 0.7547528147697449
Loss at iteration 300 : 0.4578261971473694
Loss at iteration 350 : 0.714685320854187
Loss at iteration 400 : 0.39137932658195496
Mean training loss eporch  378 :  0.6016531562424294
Loss at iteration 50 : 0.5649620890617371
Loss at iteration 100 : 0.4156121015548706
Loss at iteration 150 : 0.4950929284095764
Loss at iteration 200 : 0.30590787529945374
Loss at iteration 250 : 0.3807467818260193
Loss at iteration 300 : 0.884698212146759
Loss at iteration 350 : 0.4663234353065491
Loss at iteration 400 : 1.0428690910339355
Mean training loss eporch  379 :  0.6022068399418096
Loss at iteration 50 : 0.7574204206466675
Loss at iteration 100 : 0.3223625421524048
Loss at iteration 150 : 0.6653212308883667
Loss at iteration 200 : 0.4585447907447815
Loss at iteration 250 : 0.8747802972793579
Loss at iteration 300 : 0.522278368473053
Loss at iteration 350 : 0.8021557331085205
Loss at iteration 400 : 0.6676949262619019
Mean training loss eporch  380 :  0.6023226874826201
Loss at iteration 50 : 0.6579740643501282
Loss at iteration 100 : 0.614924430847168
Loss at iteration 150 : 0.670285701751709
Loss at iteration 200 : 0.34887242317199707
Loss at iteration 250 : 0.2828684151172638
Loss at iteration 300 : 0.3956432342529297
Loss at iteration 350 : 0.3589518070220947
Loss at iteration 400 : 0.6497184038162231
Mean training loss eporch  381 :  0.6044412079547018
Loss at iteration 50 : 0.2593536376953125
Loss at iteration 100 : 1.195814847946167
Loss at iteration 150 : 0.6274925470352173
Loss at iteration 200 : 0.958125114440918
Loss at iteration 250 : 0.2919404208660126
Loss at iteration 300 : 0.26255688071250916
Loss at iteration 350 : 0.3377906084060669
Loss at iteration 400 : 0.3773581385612488
Mean training loss eporch  382 :  0.6056679790263219
Loss at iteration 50 : 0.6687688231468201
Loss at iteration 100 : 0.5955846905708313
Loss at iteration 150 : 0.400306761264801
Loss at iteration 200 : 0.3141487240791321
Loss at iteration 250 : 0.47912174463272095
Loss at iteration 300 : 1.0521769523620605
Loss at iteration 350 : 0.6843779683113098
Loss at iteration 400 : 0.6099358797073364
Mean training loss eporch  383 :  0.602043325223463
Loss at iteration 50 : 1.1122853755950928
Loss at iteration 100 : 0.6609252095222473
Loss at iteration 150 : 0.22635415196418762
Loss at iteration 200 : 0.39149612188339233
Loss at iteration 250 : 0.5900095701217651
Loss at iteration 300 : 0.31344708800315857
Loss at iteration 350 : 0.2551935017108917
Loss at iteration 400 : 0.5314371585845947
Mean training loss eporch  384 :  0.6033726271759768
Loss at iteration 50 : 0.894144594669342
Loss at iteration 100 : 0.2800707221031189
Loss at iteration 150 : 0.809410572052002
Loss at iteration 200 : 0.6385881900787354
Loss at iteration 250 : 0.7045403718948364
Loss at iteration 300 : 0.4196435809135437
Loss at iteration 350 : 0.23650535941123962
Loss at iteration 400 : 0.2915324568748474
Mean training loss eporch  385 :  0.601033101935825
Loss at iteration 50 : 0.5629562139511108
Loss at iteration 100 : 0.34149467945098877
Loss at iteration 150 : 1.0749404430389404
Loss at iteration 200 : 0.3149510622024536
Loss at iteration 250 : 0.3166230022907257
Loss at iteration 300 : 0.534270167350769
Loss at iteration 350 : 0.4606381058692932
Loss at iteration 400 : 0.6281603574752808
Mean training loss eporch  386 :  0.602383201632799
Loss at iteration 50 : 0.2640189826488495
Loss at iteration 100 : 0.5795015096664429
Loss at iteration 150 : 0.36602893471717834
Loss at iteration 200 : 0.41422343254089355
Loss at iteration 250 : 0.6308114528656006
Loss at iteration 300 : 0.706955075263977
Loss at iteration 350 : 0.6108828186988831
Loss at iteration 400 : 0.8222405910491943
Mean training loss eporch  387 :  0.6018398681095898
Loss at iteration 50 : 0.9226376414299011
Loss at iteration 100 : 0.29748570919036865
Loss at iteration 150 : 1.1028512716293335
Loss at iteration 200 : 0.6198737025260925
Loss at iteration 250 : 0.7902154922485352
Loss at iteration 300 : 0.48817020654678345
Loss at iteration 350 : 0.7879599332809448
Loss at iteration 400 : 0.2861795425415039
Mean training loss eporch  388 :  0.6018151515936103
Loss at iteration 50 : 0.40872853994369507
Loss at iteration 100 : 0.7330498695373535
Loss at iteration 150 : 0.511073112487793
Loss at iteration 200 : 0.2898475229740143
Loss at iteration 250 : 0.3905556797981262
Loss at iteration 300 : 0.3429388105869293
Loss at iteration 350 : 0.7378968596458435
Loss at iteration 400 : 0.6759755611419678
Mean training loss eporch  389 :  0.6014861510941266
Loss at iteration 50 : 0.3347576856613159
Loss at iteration 100 : 0.2818676829338074
Loss at iteration 150 : 0.7234148383140564
Loss at iteration 200 : 0.3791314661502838
Loss at iteration 250 : 0.47580599784851074
Loss at iteration 300 : 0.7235264778137207
Loss at iteration 350 : 1.0492186546325684
Loss at iteration 400 : 0.4718410074710846
Mean training loss eporch  390 :  0.6018031583305432
Loss at iteration 50 : 0.8097554445266724
Loss at iteration 100 : 1.1164097785949707
Loss at iteration 150 : 0.34572550654411316
Loss at iteration 200 : 0.7081449031829834
Loss at iteration 250 : 0.5386530160903931
Loss at iteration 300 : 0.5415486693382263
Loss at iteration 350 : 0.6579235196113586
Loss at iteration 400 : 0.6566203832626343
Mean training loss eporch  391 :  0.6014544345739177
Loss at iteration 50 : 0.3548421561717987
Loss at iteration 100 : 0.3993754982948303
Loss at iteration 150 : 0.30871880054473877
Loss at iteration 200 : 0.6870341897010803
Loss at iteration 250 : 0.5348552465438843
Loss at iteration 300 : 0.33041685819625854
Loss at iteration 350 : 0.7503050565719604
Loss at iteration 400 : 0.37813109159469604
Mean training loss eporch  392 :  0.6015469364387572
Loss at iteration 50 : 0.3662097454071045
Loss at iteration 100 : 0.7503591775894165
Loss at iteration 150 : 0.4302746057510376
Loss at iteration 200 : 0.5081919431686401
Loss at iteration 250 : 0.508442759513855
Loss at iteration 300 : 0.8969573378562927
Loss at iteration 350 : 0.48409655690193176
Loss at iteration 400 : 0.4153446555137634
Mean training loss eporch  393 :  0.6039977924305228
Loss at iteration 50 : 0.25382745265960693
Loss at iteration 100 : 0.3538164794445038
Loss at iteration 150 : 0.7155171632766724
Loss at iteration 200 : 0.4547237157821655
Loss at iteration 250 : 0.3800979554653168
Loss at iteration 300 : 0.6221540570259094
Loss at iteration 350 : 0.3770996928215027
Loss at iteration 400 : 0.2931672930717468
Mean training loss eporch  394 :  0.6014801372728006
Loss at iteration 50 : 0.5899748802185059
Loss at iteration 100 : 0.37595000863075256
Loss at iteration 150 : 0.8480473756790161
Loss at iteration 200 : 1.1542167663574219
Loss at iteration 250 : 0.4292224049568176
Loss at iteration 300 : 0.3372335135936737
Loss at iteration 350 : 0.4405890107154846
Loss at iteration 400 : 0.421066552400589
Mean training loss eporch  395 :  0.601871836713344
Loss at iteration 50 : 0.3295632004737854
Loss at iteration 100 : 0.39118459820747375
Loss at iteration 150 : 0.6190311312675476
Loss at iteration 200 : 0.7865354418754578
Loss at iteration 250 : 0.9118883609771729
Loss at iteration 300 : 0.6749823689460754
Loss at iteration 350 : 0.3040076494216919
Loss at iteration 400 : 0.2883148491382599
Mean training loss eporch  396 :  0.6015531692336493
Loss at iteration 50 : 0.5627447366714478
Loss at iteration 100 : 0.5169212818145752
Loss at iteration 150 : 0.4029077887535095
Loss at iteration 200 : 0.44460174441337585
Loss at iteration 250 : 0.7628120183944702
Loss at iteration 300 : 0.4529118537902832
Loss at iteration 350 : 0.23113447427749634
Loss at iteration 400 : 0.4722819924354553
Mean training loss eporch  397 :  0.6058138750259652
Loss at iteration 50 : 0.8188157677650452
Loss at iteration 100 : 0.3007628321647644
Loss at iteration 150 : 0.5053556561470032
Loss at iteration 200 : 0.2536541819572449
Loss at iteration 250 : 0.33172371983528137
Loss at iteration 300 : 0.41063469648361206
Loss at iteration 350 : 0.5046412944793701
Loss at iteration 400 : 0.8862913846969604
Mean training loss eporch  398 :  0.6019346373230887
Loss at iteration 50 : 0.3602938652038574
Loss at iteration 100 : 0.401801735162735
Loss at iteration 150 : 1.0583815574645996
Loss at iteration 200 : 0.4181445240974426
Loss at iteration 250 : 0.8643877506256104
Loss at iteration 300 : 0.6808969974517822
Loss at iteration 350 : 0.6991597414016724
Loss at iteration 400 : 0.5393196940422058
Mean training loss eporch  399 :  0.6012549226259971
Loss at iteration 50 : 0.3166491985321045
Loss at iteration 100 : 0.3305935561656952
Loss at iteration 150 : 0.7394025325775146
Loss at iteration 200 : 0.38139647245407104
Loss at iteration 250 : 0.4979124069213867
Loss at iteration 300 : 0.4530439078807831
Loss at iteration 350 : 0.28626778721809387
Loss at iteration 400 : 0.3184860348701477
Mean training loss eporch  400 :  0.6018453865842435
Loss at iteration 50 : 1.2420508861541748
Loss at iteration 100 : 0.3794293999671936
Loss at iteration 150 : 0.45636308193206787
Loss at iteration 200 : 0.4487472176551819
Loss at iteration 250 : 0.7898529767990112
Loss at iteration 300 : 0.6250331997871399
Loss at iteration 350 : 0.6132674217224121
Loss at iteration 400 : 1.101790189743042
Mean training loss eporch  401 :  0.601415076664745
Loss at iteration 50 : 1.1486046314239502
Loss at iteration 100 : 0.6381525993347168
Loss at iteration 150 : 0.36447909474372864
Loss at iteration 200 : 0.5085175037384033
Loss at iteration 250 : 0.9188897609710693
Loss at iteration 300 : 0.3708342909812927
Loss at iteration 350 : 0.47212642431259155
Loss at iteration 400 : 0.7299725413322449
Mean training loss eporch  402 :  0.6018639026468645
Loss at iteration 50 : 0.43630921840667725
Loss at iteration 100 : 0.5111037492752075
Loss at iteration 150 : 1.2860431671142578
Loss at iteration 200 : 0.7274207472801208
Loss at iteration 250 : 0.6297527551651001
Loss at iteration 300 : 0.649484395980835
Loss at iteration 350 : 0.48965850472450256
Loss at iteration 400 : 0.7872225046157837
Mean training loss eporch  403 :  0.6016116043711457
Loss at iteration 50 : 0.8123464584350586
Loss at iteration 100 : 0.8336952328681946
Loss at iteration 150 : 0.8311241865158081
Loss at iteration 200 : 0.6378309726715088
Loss at iteration 250 : 0.218564972281456
Loss at iteration 300 : 1.1473565101623535
Loss at iteration 350 : 0.7840188145637512
Loss at iteration 400 : 0.8552853465080261
Mean training loss eporch  404 :  0.6046837238519716
Loss at iteration 50 : 0.8986366391181946
Loss at iteration 100 : 0.5638800263404846
Loss at iteration 150 : 0.2508109211921692
Loss at iteration 200 : 0.7799108624458313
Loss at iteration 250 : 0.7334198951721191
Loss at iteration 300 : 0.7586784362792969
Loss at iteration 350 : 0.3789699673652649
Loss at iteration 400 : 0.704116940498352
Mean training loss eporch  405 :  0.6015331492228893
Loss at iteration 50 : 0.42629843950271606
Loss at iteration 100 : 1.1330887079238892
Loss at iteration 150 : 0.32575711607933044
Loss at iteration 200 : 0.36564236879348755
Loss at iteration 250 : 1.0616681575775146
Loss at iteration 300 : 0.6067521572113037
Loss at iteration 350 : 0.6855820417404175
Loss at iteration 400 : 0.43984484672546387
Mean training loss eporch  406 :  0.6014224311562397
Loss at iteration 50 : 0.8485195636749268
Loss at iteration 100 : 0.35196083784103394
Loss at iteration 150 : 0.46434250473976135
Loss at iteration 200 : 0.38515138626098633
Loss at iteration 250 : 0.7533363103866577
Loss at iteration 300 : 0.4437876343727112
Loss at iteration 350 : 1.1890145540237427
Loss at iteration 400 : 0.6894233822822571
Mean training loss eporch  407 :  0.6015394337709175
Loss at iteration 50 : 0.665135383605957
Loss at iteration 100 : 0.5913522839546204
Loss at iteration 150 : 0.4320119023323059
Loss at iteration 200 : 0.49723517894744873
Loss at iteration 250 : 1.404823899269104
Loss at iteration 300 : 0.41603314876556396
Loss at iteration 350 : 0.21864217519760132
Loss at iteration 400 : 0.2677420973777771
Mean training loss eporch  408 :  0.6013248058311608
Loss at iteration 50 : 0.3548378348350525
Loss at iteration 100 : 0.6279491186141968
Loss at iteration 150 : 1.0157300233840942
Loss at iteration 200 : 0.5406769514083862
Loss at iteration 250 : 0.5763828754425049
Loss at iteration 300 : 0.3088245391845703
Loss at iteration 350 : 1.1913814544677734
Loss at iteration 400 : 0.4235886335372925
Mean training loss eporch  409 :  0.6056458477281669
Loss at iteration 50 : 0.34892672300338745
Loss at iteration 100 : 0.4091621935367584
Loss at iteration 150 : 0.36235055327415466
Loss at iteration 200 : 0.22044828534126282
Loss at iteration 250 : 0.9491738677024841
Loss at iteration 300 : 0.6841466426849365
Loss at iteration 350 : 0.9643126130104065
Loss at iteration 400 : 0.9709484577178955
Mean training loss eporch  410 :  0.6015820999383392
Loss at iteration 50 : 0.5742067098617554
Loss at iteration 100 : 0.5429645776748657
Loss at iteration 150 : 0.5224957466125488
Loss at iteration 200 : 0.40259575843811035
Loss at iteration 250 : 0.8177645802497864
Loss at iteration 300 : 0.3140641748905182
Loss at iteration 350 : 0.42326515913009644
Loss at iteration 400 : 1.1369833946228027
Mean training loss eporch  411 :  0.6011362744019171
Loss at iteration 50 : 0.5585895776748657
Loss at iteration 100 : 0.8307069540023804
Loss at iteration 150 : 0.9073508381843567
Loss at iteration 200 : 0.671522855758667
Loss at iteration 250 : 0.6822131276130676
Loss at iteration 300 : 0.5837905406951904
Loss at iteration 350 : 0.749739944934845
Loss at iteration 400 : 0.51031893491745
Mean training loss eporch  412 :  0.6034648310175925
Loss at iteration 50 : 1.1133794784545898
Loss at iteration 100 : 0.7197582721710205
Loss at iteration 150 : 0.8037591576576233
Loss at iteration 200 : 0.30875861644744873
Loss at iteration 250 : 0.46017324924468994
Loss at iteration 300 : 0.3337308168411255
Loss at iteration 350 : 0.9948046803474426
Loss at iteration 400 : 0.6735020875930786
Mean training loss eporch  413 :  0.6018800838378513
Loss at iteration 50 : 0.6644479632377625
Loss at iteration 100 : 0.7636656761169434
Loss at iteration 150 : 0.3096390962600708
Loss at iteration 200 : 1.2736891508102417
Loss at iteration 250 : 0.47356075048446655
Loss at iteration 300 : 0.21390865743160248
Loss at iteration 350 : 0.2951754033565521
Loss at iteration 400 : 0.5180851221084595
Mean training loss eporch  414 :  0.6017226502967522
Loss at iteration 50 : 0.6746008992195129
Loss at iteration 100 : 1.0246844291687012
Loss at iteration 150 : 0.41781601309776306
Loss at iteration 200 : 0.4715256690979004
Loss at iteration 250 : 0.7740792036056519
Loss at iteration 300 : 0.9910703301429749
Loss at iteration 350 : 0.5416089296340942
Loss at iteration 400 : 0.552183985710144
Mean training loss eporch  415 :  0.6020972396293028
Loss at iteration 50 : 0.3707314133644104
Loss at iteration 100 : 0.37020182609558105
Loss at iteration 150 : 0.761286735534668
Loss at iteration 200 : 0.5796813368797302
Loss at iteration 250 : 0.6936361193656921
Loss at iteration 300 : 0.23128941655158997
Loss at iteration 350 : 0.6771534085273743
Loss at iteration 400 : 0.7790218591690063
Mean training loss eporch  416 :  0.6015219723162629
Loss at iteration 50 : 0.2662698030471802
Loss at iteration 100 : 0.3560507893562317
Loss at iteration 150 : 1.0930075645446777
Loss at iteration 200 : 0.2959825098514557
Loss at iteration 250 : 0.378359317779541
Loss at iteration 300 : 0.8063271045684814
Loss at iteration 350 : 0.7035669088363647
Loss at iteration 400 : 0.5866162180900574
Mean training loss eporch  417 :  0.6021263891070947
Loss at iteration 50 : 0.5478000044822693
Loss at iteration 100 : 0.8686767220497131
Loss at iteration 150 : 0.5392941832542419
Loss at iteration 200 : 0.7597315311431885
Loss at iteration 250 : 0.5723024606704712
Loss at iteration 300 : 0.4577511250972748
Loss at iteration 350 : 0.8156386613845825
Loss at iteration 400 : 0.4235389530658722
Mean training loss eporch  418 :  0.6013325592127081
Loss at iteration 50 : 0.3741661608219147
Loss at iteration 100 : 0.5977026224136353
Loss at iteration 150 : 0.7692975401878357
Loss at iteration 200 : 0.4685037136077881
Loss at iteration 250 : 0.2646920084953308
Loss at iteration 300 : 0.4825515151023865
Loss at iteration 350 : 0.3745175302028656
Loss at iteration 400 : 0.3209550976753235
Mean training loss eporch  419 :  0.6017371397047834
Loss at iteration 50 : 0.564117431640625
Loss at iteration 100 : 0.7276555299758911
Loss at iteration 150 : 0.5160921812057495
Loss at iteration 200 : 0.9008516669273376
Loss at iteration 250 : 0.8061807751655579
Loss at iteration 300 : 0.3050954043865204
Loss at iteration 350 : 0.9133867621421814
Loss at iteration 400 : 0.2825187146663666
Mean training loss eporch  420 :  0.6018374214124252
Loss at iteration 50 : 0.2595401704311371
Loss at iteration 100 : 0.535631537437439
Loss at iteration 150 : 0.44567322731018066
Loss at iteration 200 : 0.9179535508155823
Loss at iteration 250 : 0.5530112385749817
Loss at iteration 300 : 0.8242015838623047
Loss at iteration 350 : 0.29171743988990784
Loss at iteration 400 : 0.2730412483215332
Mean training loss eporch  421 :  0.6012176594993459
Loss at iteration 50 : 0.42477208375930786
Loss at iteration 100 : 0.32480359077453613
Loss at iteration 150 : 1.0492826700210571
Loss at iteration 200 : 0.4058031439781189
Loss at iteration 250 : 0.6709407567977905
Loss at iteration 300 : 0.6673791408538818
Loss at iteration 350 : 0.35245534777641296
Loss at iteration 400 : 0.8564797043800354
Mean training loss eporch  422 :  0.6012856671294289
Loss at iteration 50 : 0.2746659219264984
Loss at iteration 100 : 0.6138419508934021
Loss at iteration 150 : 0.48213785886764526
Loss at iteration 200 : 1.0249005556106567
Loss at iteration 250 : 0.8241240382194519
Loss at iteration 300 : 0.8495807647705078
Loss at iteration 350 : 0.38814646005630493
Loss at iteration 400 : 0.661858081817627
Mean training loss eporch  423 :  0.6009521048699794
Loss at iteration 50 : 0.3939547836780548
Loss at iteration 100 : 0.43470299243927
Loss at iteration 150 : 0.5325196981430054
Loss at iteration 200 : 0.5037071704864502
Loss at iteration 250 : 0.3020409047603607
Loss at iteration 300 : 0.5569007992744446
Loss at iteration 350 : 0.43576836585998535
Loss at iteration 400 : 1.0011436939239502
Mean training loss eporch  424 :  0.602137065087466
Loss at iteration 50 : 0.3770369291305542
Loss at iteration 100 : 0.451582670211792
Loss at iteration 150 : 1.11561119556427
Loss at iteration 200 : 0.539229154586792
Loss at iteration 250 : 0.7114601135253906
Loss at iteration 300 : 0.49975699186325073
Loss at iteration 350 : 0.44193705916404724
Loss at iteration 400 : 0.48866599798202515
Mean training loss eporch  425 :  0.6032307557260509
Loss at iteration 50 : 0.6046819686889648
Loss at iteration 100 : 0.5421101450920105
Loss at iteration 150 : 0.9970162510871887
Loss at iteration 200 : 0.8634006381034851
Loss at iteration 250 : 0.5524283647537231
Loss at iteration 300 : 0.9188274145126343
Loss at iteration 350 : 0.5719583034515381
Loss at iteration 400 : 0.9764481782913208
Mean training loss eporch  426 :  0.6014101300137995
Loss at iteration 50 : 0.5207432508468628
Loss at iteration 100 : 0.5637338161468506
Loss at iteration 150 : 0.508522093296051
Loss at iteration 200 : 0.6464354395866394
Loss at iteration 250 : 0.5979889631271362
Loss at iteration 300 : 0.9479073882102966
Loss at iteration 350 : 0.638886570930481
Loss at iteration 400 : 0.47306787967681885
Mean training loss eporch  427 :  0.6015283652485219
Loss at iteration 50 : 0.32332393527030945
Loss at iteration 100 : 0.6915974020957947
Loss at iteration 150 : 0.44702857732772827
Loss at iteration 200 : 0.40124964714050293
Loss at iteration 250 : 0.5453580617904663
Loss at iteration 300 : 0.5270751714706421
Loss at iteration 350 : 0.8049899935722351
Loss at iteration 400 : 0.3735629618167877
Mean training loss eporch  428 :  0.6014979581929109
Loss at iteration 50 : 0.7701237797737122
Loss at iteration 100 : 0.38529062271118164
Loss at iteration 150 : 0.6692409515380859
Loss at iteration 200 : 0.83531653881073
Loss at iteration 250 : 0.4074508547782898
Loss at iteration 300 : 0.9431305527687073
Loss at iteration 350 : 0.6056124567985535
Loss at iteration 400 : 1.1460399627685547
Mean training loss eporch  429 :  0.6014728419850225
Loss at iteration 50 : 0.6275973320007324
Loss at iteration 100 : 0.9711436033248901
Loss at iteration 150 : 0.6466877460479736
Loss at iteration 200 : 0.8527233600616455
Loss at iteration 250 : 0.37621617317199707
Loss at iteration 300 : 0.24960748851299286
Loss at iteration 350 : 0.8010959029197693
Loss at iteration 400 : 0.8830013275146484
Mean training loss eporch  430 :  0.6007997951566371
Loss at iteration 50 : 0.8912078142166138
Loss at iteration 100 : 0.4126648008823395
Loss at iteration 150 : 0.22521048784255981
Loss at iteration 200 : 0.7425577044487
Loss at iteration 250 : 0.23355694115161896
Loss at iteration 300 : 0.38954493403434753
Loss at iteration 350 : 0.3402908742427826
Loss at iteration 400 : 0.5560205578804016
Mean training loss eporch  431 :  0.6009298841648573
Loss at iteration 50 : 0.9408453702926636
Loss at iteration 100 : 0.2717907726764679
Loss at iteration 150 : 0.4912832975387573
Loss at iteration 200 : 0.4203171730041504
Loss at iteration 250 : 0.9259918928146362
Loss at iteration 300 : 0.48437967896461487
Loss at iteration 350 : 0.9970670342445374
Loss at iteration 400 : 0.5957483053207397
Mean training loss eporch  432 :  0.6007122935000556
Loss at iteration 50 : 0.2944667935371399
Loss at iteration 100 : 0.5627225041389465
Loss at iteration 150 : 0.4773121476173401
Loss at iteration 200 : 0.31066328287124634
Loss at iteration 250 : 0.9308789968490601
Loss at iteration 300 : 0.6135507822036743
Loss at iteration 350 : 0.35582566261291504
Loss at iteration 400 : 0.48036399483680725
Mean training loss eporch  433 :  0.6012376508771571
Loss at iteration 50 : 0.32328832149505615
Loss at iteration 100 : 0.7376017570495605
Loss at iteration 150 : 0.39857739210128784
Loss at iteration 200 : 0.7646584510803223
Loss at iteration 250 : 0.41441482305526733
Loss at iteration 300 : 1.1638331413269043
Loss at iteration 350 : 0.6615928411483765
Loss at iteration 400 : 0.6677664518356323
Mean training loss eporch  434 :  0.6008568766606228
Loss at iteration 50 : 1.0022082328796387
Loss at iteration 100 : 0.7733529210090637
Loss at iteration 150 : 0.24656052887439728
Loss at iteration 200 : 0.9877316951751709
Loss at iteration 250 : 0.32033467292785645
Loss at iteration 300 : 0.6050770282745361
Loss at iteration 350 : 0.36265942454338074
Loss at iteration 400 : 0.3351954221725464
Mean training loss eporch  435 :  0.6006712115666257
Loss at iteration 50 : 0.9501398801803589
Loss at iteration 100 : 0.32198208570480347
Loss at iteration 150 : 0.5661290884017944
Loss at iteration 200 : 0.8516826629638672
Loss at iteration 250 : 0.8422691822052002
Loss at iteration 300 : 0.4735788404941559
Loss at iteration 350 : 0.36911869049072266
Loss at iteration 400 : 0.8029211759567261
Mean training loss eporch  436 :  0.6026612363521828
Loss at iteration 50 : 0.6537079811096191
Loss at iteration 100 : 0.3627709746360779
Loss at iteration 150 : 0.959788978099823
Loss at iteration 200 : 0.9412107467651367
Loss at iteration 250 : 0.3925647437572479
Loss at iteration 300 : 0.8620489239692688
Loss at iteration 350 : 0.5757871866226196
Loss at iteration 400 : 0.5168902277946472
Mean training loss eporch  437 :  0.6010262293933218
Loss at iteration 50 : 0.6265856027603149
Loss at iteration 100 : 0.5578538775444031
Loss at iteration 150 : 1.021385669708252
Loss at iteration 200 : 0.37258732318878174
Loss at iteration 250 : 0.7406420707702637
Loss at iteration 300 : 0.30251777172088623
Loss at iteration 350 : 0.2828332185745239
Loss at iteration 400 : 0.7237454652786255
Mean training loss eporch  438 :  0.6025550615693956
Loss at iteration 50 : 0.3390484154224396
Loss at iteration 100 : 0.5237168073654175
Loss at iteration 150 : 0.7017159461975098
Loss at iteration 200 : 1.0979623794555664
Loss at iteration 250 : 0.7581659555435181
Loss at iteration 300 : 0.7637581825256348
Loss at iteration 350 : 0.24516242742538452
Loss at iteration 400 : 0.3171030282974243
Mean training loss eporch  439 :  0.6009972909039446
Loss at iteration 50 : 0.9250651597976685
Loss at iteration 100 : 0.4149879515171051
Loss at iteration 150 : 0.5627838373184204
Loss at iteration 200 : 0.3631606101989746
Loss at iteration 250 : 0.39973145723342896
Loss at iteration 300 : 0.32445427775382996
Loss at iteration 350 : 0.4717363715171814
Loss at iteration 400 : 0.538416862487793
Mean training loss eporch  440 :  0.6012367603156065
Loss at iteration 50 : 0.6346420049667358
Loss at iteration 100 : 0.6381591558456421
Loss at iteration 150 : 0.34468305110931396
Loss at iteration 200 : 0.6676235198974609
Loss at iteration 250 : 0.5056920051574707
Loss at iteration 300 : 0.8506236672401428
Loss at iteration 350 : 0.8056869506835938
Loss at iteration 400 : 0.9039744138717651
Mean training loss eporch  441 :  0.6022050449198671
Loss at iteration 50 : 0.7524228692054749
Loss at iteration 100 : 0.8276636004447937
Loss at iteration 150 : 0.3193133473396301
Loss at iteration 200 : 0.585951566696167
Loss at iteration 250 : 1.0519438982009888
Loss at iteration 300 : 0.5113216638565063
Loss at iteration 350 : 0.5082165598869324
Loss at iteration 400 : 0.47569990158081055
Mean training loss eporch  442 :  0.6011287799865142
Loss at iteration 50 : 0.852751612663269
Loss at iteration 100 : 0.5188363790512085
Loss at iteration 150 : 0.7409161329269409
Loss at iteration 200 : 0.37437260150909424
Loss at iteration 250 : 1.119800329208374
Loss at iteration 300 : 0.45305344462394714
Loss at iteration 350 : 0.29185017943382263
Loss at iteration 400 : 0.33886849880218506
Mean training loss eporch  443 :  0.601509528410007
Loss at iteration 50 : 0.4157731533050537
Loss at iteration 100 : 1.2209477424621582
Loss at iteration 150 : 0.6590842008590698
Loss at iteration 200 : 0.48634451627731323
Loss at iteration 250 : 0.33055227994918823
Loss at iteration 300 : 0.8561607599258423
Loss at iteration 350 : 0.5649729371070862
Loss at iteration 400 : 0.7012789845466614
Mean training loss eporch  444 :  0.6010196128768237
Loss at iteration 50 : 0.4367324411869049
Loss at iteration 100 : 0.5537336468696594
Loss at iteration 150 : 0.41908225417137146
Loss at iteration 200 : 0.47255998849868774
Loss at iteration 250 : 0.5657746195793152
Loss at iteration 300 : 0.2582470774650574
Loss at iteration 350 : 0.6338520646095276
Loss at iteration 400 : 0.4056868553161621
Mean training loss eporch  445 :  0.6016860913669048
Loss at iteration 50 : 0.6548449397087097
Loss at iteration 100 : 0.3077017664909363
Loss at iteration 150 : 0.5559284090995789
Loss at iteration 200 : 0.38828539848327637
Loss at iteration 250 : 1.1266541481018066
Loss at iteration 300 : 0.5670596361160278
Loss at iteration 350 : 0.8796027898788452
Loss at iteration 400 : 0.304345965385437
Mean training loss eporch  446 :  0.6009670672061197
Loss at iteration 50 : 0.3752444088459015
Loss at iteration 100 : 0.4816753566265106
Loss at iteration 150 : 0.5941332578659058
Loss at iteration 200 : 0.4996194839477539
Loss at iteration 250 : 0.4054258465766907
Loss at iteration 300 : 0.5548009872436523
Loss at iteration 350 : 0.7333118319511414
Loss at iteration 400 : 0.3553217649459839
Mean training loss eporch  447 :  0.6007856195082579
Loss at iteration 50 : 0.3620264530181885
Loss at iteration 100 : 0.9557390213012695
Loss at iteration 150 : 0.3352752923965454
Loss at iteration 200 : 0.6337348222732544
Loss at iteration 250 : 0.6774076223373413
Loss at iteration 300 : 0.48786717653274536
Loss at iteration 350 : 0.699933648109436
Loss at iteration 400 : 0.37358585000038147
Mean training loss eporch  448 :  0.6013470044852373
Loss at iteration 50 : 0.5508020520210266
Loss at iteration 100 : 0.48822885751724243
Loss at iteration 150 : 0.6130807995796204
Loss at iteration 200 : 0.6806321740150452
Loss at iteration 250 : 0.6639747023582458
Loss at iteration 300 : 0.6356832981109619
Loss at iteration 350 : 0.40865427255630493
Loss at iteration 400 : 0.5592882037162781
Mean training loss eporch  449 :  0.602034676736513
Loss at iteration 50 : 0.8101938962936401
Loss at iteration 100 : 0.7090954780578613
Loss at iteration 150 : 0.7157434225082397
Loss at iteration 200 : 0.49009284377098083
Loss at iteration 250 : 0.7644553184509277
Loss at iteration 300 : 0.46430814266204834
Loss at iteration 350 : 0.7108095288276672
Loss at iteration 400 : 0.9473524689674377
Mean training loss eporch  450 :  0.6012753001978045
Loss at iteration 50 : 0.29801011085510254
Loss at iteration 100 : 0.4325907528400421
Loss at iteration 150 : 0.6234731078147888
Loss at iteration 200 : 0.6697381138801575
Loss at iteration 250 : 0.41830044984817505
Loss at iteration 300 : 0.35769352316856384
Loss at iteration 350 : 0.29627856612205505
Loss at iteration 400 : 0.37590497732162476
Mean training loss eporch  451 :  0.601012870970061
Loss at iteration 50 : 0.6984856724739075
Loss at iteration 100 : 0.6568245887756348
Loss at iteration 150 : 0.36944153904914856
Loss at iteration 200 : 0.618671178817749
Loss at iteration 250 : 0.6290189623832703
Loss at iteration 300 : 0.755400538444519
Loss at iteration 350 : 0.9605478048324585
Loss at iteration 400 : 0.2699190080165863
Mean training loss eporch  452 :  0.6006167421760581
Loss at iteration 50 : 0.3736383616924286
Loss at iteration 100 : 0.6332234144210815
Loss at iteration 150 : 0.566699206829071
Loss at iteration 200 : 0.4843057096004486
Loss at iteration 250 : 0.8682754635810852
Loss at iteration 300 : 0.3607730269432068
Loss at iteration 350 : 0.42095625400543213
Loss at iteration 400 : 0.5955313444137573
Mean training loss eporch  453 :  0.6007173460294313
Loss at iteration 50 : 0.2574472427368164
Loss at iteration 100 : 0.3480382263660431
Loss at iteration 150 : 1.065468668937683
Loss at iteration 200 : 0.6649183034896851
Loss at iteration 250 : 0.6472089886665344
Loss at iteration 300 : 0.784809947013855
Loss at iteration 350 : 0.5882374048233032
Loss at iteration 400 : 1.5936238765716553
Mean training loss eporch  454 :  0.6012774191028334
Loss at iteration 50 : 1.1219236850738525
Loss at iteration 100 : 0.3868165612220764
Loss at iteration 150 : 0.5532159209251404
Loss at iteration 200 : 0.5866106748580933
Loss at iteration 250 : 0.6313859224319458
Loss at iteration 300 : 0.8267298936843872
Loss at iteration 350 : 0.6985857486724854
Loss at iteration 400 : 0.6827167272567749
Mean training loss eporch  455 :  0.6009908624227271
Loss at iteration 50 : 0.4629489779472351
Loss at iteration 100 : 0.6184549331665039
Loss at iteration 150 : 1.1789453029632568
Loss at iteration 200 : 0.6794931888580322
Loss at iteration 250 : 0.36135250329971313
Loss at iteration 300 : 1.032218098640442
Loss at iteration 350 : 1.3734123706817627
Loss at iteration 400 : 0.2548224627971649
Mean training loss eporch  456 :  0.602903667319516
Loss at iteration 50 : 0.7221556901931763
Loss at iteration 100 : 0.541792094707489
Loss at iteration 150 : 0.44319653511047363
Loss at iteration 200 : 1.1957907676696777
Loss at iteration 250 : 0.2978616952896118
Loss at iteration 300 : 0.8399947881698608
Loss at iteration 350 : 0.4025989770889282
Loss at iteration 400 : 1.0436384677886963
Mean training loss eporch  457 :  0.6004041730689361
Loss at iteration 50 : 0.37201982736587524
Loss at iteration 100 : 0.3067268133163452
Loss at iteration 150 : 0.3793867826461792
Loss at iteration 200 : 0.5725719332695007
Loss at iteration 250 : 1.0259830951690674
Loss at iteration 300 : 0.49333640933036804
Loss at iteration 350 : 0.7735034227371216
Loss at iteration 400 : 0.2777164578437805
Mean training loss eporch  458 :  0.6012951053445115
Loss at iteration 50 : 0.3256065547466278
Loss at iteration 100 : 0.4263220429420471
Loss at iteration 150 : 0.37749597430229187
Loss at iteration 200 : 0.3321935832500458
Loss at iteration 250 : 0.41236740350723267
Loss at iteration 300 : 0.37795960903167725
Loss at iteration 350 : 0.38053327798843384
Loss at iteration 400 : 0.6891510486602783
Mean training loss eporch  459 :  0.6007414309500044
Loss at iteration 50 : 0.43747037649154663
Loss at iteration 100 : 0.6884404420852661
Loss at iteration 150 : 0.7364351153373718
Loss at iteration 200 : 0.6401925086975098
Loss at iteration 250 : 0.36396360397338867
Loss at iteration 300 : 0.6063729524612427
Loss at iteration 350 : 1.0892475843429565
Loss at iteration 400 : 0.5758368968963623
Mean training loss eporch  460 :  0.6012930143039857
Loss at iteration 50 : 1.0367472171783447
Loss at iteration 100 : 0.34061533212661743
Loss at iteration 150 : 0.29468244314193726
Loss at iteration 200 : 0.4966284930706024
Loss at iteration 250 : 0.5593036413192749
Loss at iteration 300 : 0.5475404262542725
Loss at iteration 350 : 0.8621402978897095
Loss at iteration 400 : 0.3919975459575653
Mean training loss eporch  461 :  0.6023985478241882
Loss at iteration 50 : 0.42125141620635986
Loss at iteration 100 : 0.5921381711959839
Loss at iteration 150 : 0.6577191948890686
Loss at iteration 200 : 0.2900720238685608
Loss at iteration 250 : 0.23712903261184692
Loss at iteration 300 : 1.1278083324432373
Loss at iteration 350 : 0.6181244850158691
Loss at iteration 400 : 0.725530743598938
Mean training loss eporch  462 :  0.6009323471664313
Loss at iteration 50 : 0.3829633593559265
Loss at iteration 100 : 0.5303902626037598
Loss at iteration 150 : 0.33434224128723145
Loss at iteration 200 : 0.5307413339614868
Loss at iteration 250 : 0.7624565362930298
Loss at iteration 300 : 0.8908191323280334
Loss at iteration 350 : 1.2491267919540405
Loss at iteration 400 : 0.7666364908218384
Mean training loss eporch  463 :  0.6010872112929554
Loss at iteration 50 : 0.5199429988861084
Loss at iteration 100 : 0.27058956027030945
Loss at iteration 150 : 0.5460048317909241
Loss at iteration 200 : 0.767375111579895
Loss at iteration 250 : 1.2156254053115845
Loss at iteration 300 : 0.4775810241699219
Loss at iteration 350 : 0.5549193620681763
Loss at iteration 400 : 1.1605998277664185
Mean training loss eporch  464 :  0.6012822099598
Loss at iteration 50 : 0.3250347971916199
Loss at iteration 100 : 1.0137805938720703
Loss at iteration 150 : 0.3075813353061676
Loss at iteration 200 : 0.6111398339271545
Loss at iteration 250 : 0.3986364006996155
Loss at iteration 300 : 0.3223782181739807
Loss at iteration 350 : 0.9363850355148315
Loss at iteration 400 : 0.3333674669265747
Mean training loss eporch  465 :  0.6010484455545921
Loss at iteration 50 : 0.31967487931251526
Loss at iteration 100 : 0.5530905723571777
Loss at iteration 150 : 0.4539366364479065
Loss at iteration 200 : 0.7108234763145447
Loss at iteration 250 : 0.5288582444190979
Loss at iteration 300 : 1.045062780380249
Loss at iteration 350 : 0.5218982100486755
Loss at iteration 400 : 0.46285754442214966
Mean training loss eporch  466 :  0.6008852608334858
Loss at iteration 50 : 0.38307514786720276
Loss at iteration 100 : 0.5120123624801636
Loss at iteration 150 : 0.41491079330444336
Loss at iteration 200 : 0.4864603877067566
Loss at iteration 250 : 0.5613608956336975
Loss at iteration 300 : 0.7250791788101196
Loss at iteration 350 : 0.32325103878974915
Loss at iteration 400 : 0.4272749125957489
Mean training loss eporch  467 :  0.6008745821134391
Loss at iteration 50 : 0.7742446660995483
Loss at iteration 100 : 0.4043368101119995
Loss at iteration 150 : 0.8584599494934082
Loss at iteration 200 : 0.635878324508667
Loss at iteration 250 : 0.7196483016014099
Loss at iteration 300 : 0.5139333009719849
Loss at iteration 350 : 0.5061043500900269
Loss at iteration 400 : 0.31795743107795715
Mean training loss eporch  468 :  0.601031591305551
Loss at iteration 50 : 0.6977057456970215
Loss at iteration 100 : 0.3273053765296936
Loss at iteration 150 : 0.40165644884109497
Loss at iteration 200 : 1.2256519794464111
Loss at iteration 250 : 0.6316196918487549
Loss at iteration 300 : 0.2943030595779419
Loss at iteration 350 : 0.5405786633491516
Loss at iteration 400 : 0.3377057611942291
Mean training loss eporch  469 :  0.6019619144666355
Loss at iteration 50 : 0.39584988355636597
Loss at iteration 100 : 0.923565685749054
Loss at iteration 150 : 0.35176852345466614
Loss at iteration 200 : 1.2438915967941284
Loss at iteration 250 : 0.918049156665802
Loss at iteration 300 : 0.7706456184387207
Loss at iteration 350 : 0.44968876242637634
Loss at iteration 400 : 0.3431839942932129
Mean training loss eporch  470 :  0.6008268337479621
Loss at iteration 50 : 0.506401538848877
Loss at iteration 100 : 0.7591687440872192
Loss at iteration 150 : 0.7920146584510803
Loss at iteration 200 : 0.6455338001251221
Loss at iteration 250 : 0.9337625503540039
Loss at iteration 300 : 0.6883878707885742
Loss at iteration 350 : 0.5743461847305298
Loss at iteration 400 : 0.5682194828987122
Mean training loss eporch  471 :  0.6011158580403156
Loss at iteration 50 : 0.6830278635025024
Loss at iteration 100 : 0.642579972743988
Loss at iteration 150 : 0.3999772369861603
Loss at iteration 200 : 0.38779711723327637
Loss at iteration 250 : 0.8027279376983643
Loss at iteration 300 : 0.25723522901535034
Loss at iteration 350 : 0.4785331189632416
Loss at iteration 400 : 0.6650433540344238
Mean training loss eporch  472 :  0.6006479086897298
Loss at iteration 50 : 0.34111183881759644
Loss at iteration 100 : 0.5339343547821045
Loss at iteration 150 : 0.45916345715522766
Loss at iteration 200 : 0.30001306533813477
Loss at iteration 250 : 0.32258760929107666
Loss at iteration 300 : 0.9286095499992371
Loss at iteration 350 : 0.7771648168563843
Loss at iteration 400 : 0.7270711660385132
Mean training loss eporch  473 :  0.6007695905028971
Loss at iteration 50 : 0.7496582269668579
Loss at iteration 100 : 0.857006311416626
Loss at iteration 150 : 0.28770628571510315
Loss at iteration 200 : 0.3404685854911804
Loss at iteration 250 : 0.4320454001426697
Loss at iteration 300 : 0.6679363250732422
Loss at iteration 350 : 0.5204465389251709
Loss at iteration 400 : 0.5248231887817383
Mean training loss eporch  474 :  0.6004991415437027
Loss at iteration 50 : 0.49820226430892944
Loss at iteration 100 : 0.3420090973377228
Loss at iteration 150 : 0.9114589691162109
Loss at iteration 200 : 0.38130611181259155
Loss at iteration 250 : 0.47534292936325073
Loss at iteration 300 : 0.5055727958679199
Loss at iteration 350 : 0.36045682430267334
Loss at iteration 400 : 0.49220144748687744
Mean training loss eporch  475 :  0.6006279218891811
Loss at iteration 50 : 0.4236709475517273
Loss at iteration 100 : 0.5860732793807983
Loss at iteration 150 : 0.35146188735961914
Loss at iteration 200 : 0.29814255237579346
Loss at iteration 250 : 0.26868027448654175
Loss at iteration 300 : 0.5604296922683716
Loss at iteration 350 : 0.4775535762310028
Loss at iteration 400 : 0.2629879117012024
Mean training loss eporch  476 :  0.6011535309154891
Loss at iteration 50 : 0.5681556463241577
Loss at iteration 100 : 0.33428364992141724
Loss at iteration 150 : 0.6412020921707153
Loss at iteration 200 : 0.3262362480163574
Loss at iteration 250 : 0.5201983451843262
Loss at iteration 300 : 0.4130101799964905
Loss at iteration 350 : 0.8152222633361816
Loss at iteration 400 : 0.8048217296600342
Mean training loss eporch  477 :  0.6005037560925356
Loss at iteration 50 : 0.5473880171775818
Loss at iteration 100 : 0.28216293454170227
Loss at iteration 150 : 0.9543032646179199
Loss at iteration 200 : 1.197476863861084
Loss at iteration 250 : 0.9299886226654053
Loss at iteration 300 : 0.373710036277771
Loss at iteration 350 : 1.0897648334503174
Loss at iteration 400 : 0.9154353141784668
Mean training loss eporch  478 :  0.6024281634143115
Loss at iteration 50 : 0.6096468567848206
Loss at iteration 100 : 0.4124027490615845
Loss at iteration 150 : 0.5063447952270508
Loss at iteration 200 : 0.29186123609542847
Loss at iteration 250 : 1.0011451244354248
Loss at iteration 300 : 0.6296432018280029
Loss at iteration 350 : 0.4233252704143524
Loss at iteration 400 : 0.3051203191280365
Mean training loss eporch  479 :  0.6004971567491245
Loss at iteration 50 : 0.22911417484283447
Loss at iteration 100 : 0.644491970539093
Loss at iteration 150 : 0.47989389300346375
Loss at iteration 200 : 0.2811247408390045
Loss at iteration 250 : 0.51984703540802
Loss at iteration 300 : 0.4550502300262451
Loss at iteration 350 : 1.303098201751709
Loss at iteration 400 : 0.720705509185791
Mean training loss eporch  480 :  0.6004097336744514
Loss at iteration 50 : 0.5202465057373047
Loss at iteration 100 : 0.36516183614730835
Loss at iteration 150 : 0.46878480911254883
Loss at iteration 200 : 0.4477362036705017
Loss at iteration 250 : 0.4719659686088562
Loss at iteration 300 : 0.47259652614593506
Loss at iteration 350 : 0.7589566707611084
Loss at iteration 400 : 0.9739009737968445
Mean training loss eporch  481 :  0.6005614598703491
Loss at iteration 50 : 0.33173826336860657
Loss at iteration 100 : 0.7363786697387695
Loss at iteration 150 : 0.2647162675857544
Loss at iteration 200 : 0.6001754999160767
Loss at iteration 250 : 0.491855263710022
Loss at iteration 300 : 1.0014419555664062
Loss at iteration 350 : 0.5534582138061523
Loss at iteration 400 : 0.3802163004875183
Mean training loss eporch  482 :  0.600753366446014
Loss at iteration 50 : 0.8755156993865967
Loss at iteration 100 : 0.47276851534843445
Loss at iteration 150 : 0.47495800256729126
Loss at iteration 200 : 0.8205868601799011
Loss at iteration 250 : 0.42628878355026245
Loss at iteration 300 : 0.6084083318710327
Loss at iteration 350 : 0.6638591885566711
Loss at iteration 400 : 0.8024401068687439
Mean training loss eporch  483 :  0.6004109576411312
Loss at iteration 50 : 0.8419544696807861
Loss at iteration 100 : 0.505452573299408
Loss at iteration 150 : 0.7998606562614441
Loss at iteration 200 : 1.3386502265930176
Loss at iteration 250 : 0.4818035066127777
Loss at iteration 300 : 0.9209746718406677
Loss at iteration 350 : 0.7042248249053955
Loss at iteration 400 : 0.4420855641365051
Mean training loss eporch  484 :  0.6038869889327764
Loss at iteration 50 : 0.34354719519615173
Loss at iteration 100 : 0.42649227380752563
Loss at iteration 150 : 0.5956293940544128
Loss at iteration 200 : 0.28988558053970337
Loss at iteration 250 : 0.38885554671287537
Loss at iteration 300 : 0.9966965913772583
Loss at iteration 350 : 0.7314375638961792
Loss at iteration 400 : 0.9784402847290039
Mean training loss eporch  485 :  0.6007008140263536
Loss at iteration 50 : 0.8068065047264099
Loss at iteration 100 : 0.7689463496208191
Loss at iteration 150 : 0.3414890468120575
Loss at iteration 200 : 1.2876505851745605
Loss at iteration 250 : 0.5472605228424072
Loss at iteration 300 : 0.5553959608078003
Loss at iteration 350 : 0.7467286586761475
Loss at iteration 400 : 0.5199809074401855
Mean training loss eporch  486 :  0.6001538161539176
Loss at iteration 50 : 0.3047695457935333
Loss at iteration 100 : 0.2241867035627365
Loss at iteration 150 : 0.6022007465362549
Loss at iteration 200 : 0.41678059101104736
Loss at iteration 250 : 0.8520873785018921
Loss at iteration 300 : 0.7554999589920044
Loss at iteration 350 : 0.7510071992874146
Loss at iteration 400 : 0.6138169169425964
Mean training loss eporch  487 :  0.602051714176287
Loss at iteration 50 : 0.7855987548828125
Loss at iteration 100 : 0.48906514048576355
Loss at iteration 150 : 0.25562378764152527
Loss at iteration 200 : 0.8056783080101013
Loss at iteration 250 : 0.449313223361969
Loss at iteration 300 : 0.35037946701049805
Loss at iteration 350 : 0.42193809151649475
Loss at iteration 400 : 0.4861714839935303
Mean training loss eporch  488 :  0.6013290530630291
Loss at iteration 50 : 0.5023123025894165
Loss at iteration 100 : 0.9141308069229126
Loss at iteration 150 : 0.67853844165802
Loss at iteration 200 : 0.3542789816856384
Loss at iteration 250 : 1.1468567848205566
Loss at iteration 300 : 0.5666037201881409
Loss at iteration 350 : 0.2809414863586426
Loss at iteration 400 : 0.31919077038764954
Mean training loss eporch  489 :  0.6026228979029463
Loss at iteration 50 : 0.4729069173336029
Loss at iteration 100 : 0.3308725357055664
Loss at iteration 150 : 0.6562595963478088
Loss at iteration 200 : 0.4016571044921875
Loss at iteration 250 : 0.8003476858139038
Loss at iteration 300 : 0.40349507331848145
Loss at iteration 350 : 0.6639453172683716
Loss at iteration 400 : 0.39342600107192993
Mean training loss eporch  490 :  0.600048921133638
Loss at iteration 50 : 0.5580024719238281
Loss at iteration 100 : 0.9638087749481201
Loss at iteration 150 : 0.7423884868621826
Loss at iteration 200 : 0.7193526029586792
Loss at iteration 250 : 0.4443195164203644
Loss at iteration 300 : 0.5409472584724426
Loss at iteration 350 : 0.40461817383766174
Loss at iteration 400 : 1.2330830097198486
Mean training loss eporch  491 :  0.6003644801843326
Loss at iteration 50 : 0.5694738030433655
Loss at iteration 100 : 0.313620388507843
Loss at iteration 150 : 0.8242838382720947
Loss at iteration 200 : 0.684283971786499
Loss at iteration 250 : 0.7333078384399414
Loss at iteration 300 : 0.770501971244812
Loss at iteration 350 : 0.3667563199996948
Loss at iteration 400 : 0.7099235653877258
Mean training loss eporch  492 :  0.6014108706950607
Loss at iteration 50 : 0.7232617139816284
Loss at iteration 100 : 0.578517735004425
Loss at iteration 150 : 0.6314584016799927
Loss at iteration 200 : 0.7548346519470215
Loss at iteration 250 : 0.9032701253890991
Loss at iteration 300 : 0.4342505931854248
Loss at iteration 350 : 1.008353352546692
Loss at iteration 400 : 0.8293296098709106
Mean training loss eporch  493 :  0.6004393573499581
Loss at iteration 50 : 0.363807737827301
Loss at iteration 100 : 0.6674480438232422
Loss at iteration 150 : 0.3231436014175415
Loss at iteration 200 : 0.5296977758407593
Loss at iteration 250 : 0.7771944403648376
Loss at iteration 300 : 1.2104032039642334
Loss at iteration 350 : 0.7751260995864868
Loss at iteration 400 : 0.7656876444816589
Mean training loss eporch  494 :  0.600562991716402
Loss at iteration 50 : 0.4028719663619995
Loss at iteration 100 : 0.6784921884536743
Loss at iteration 150 : 0.5162363052368164
Loss at iteration 200 : 0.3699379861354828
Loss at iteration 250 : 0.6488417387008667
Loss at iteration 300 : 0.4424510598182678
Loss at iteration 350 : 0.49446871876716614
Loss at iteration 400 : 0.6655833721160889
Mean training loss eporch  495 :  0.6021843813860898
Loss at iteration 50 : 0.5231426954269409
Loss at iteration 100 : 0.5636491775512695
Loss at iteration 150 : 1.1956918239593506
Loss at iteration 200 : 0.7635318636894226
Loss at iteration 250 : 0.7877324819564819
Loss at iteration 300 : 0.4236919581890106
Loss at iteration 350 : 0.6032832860946655
Loss at iteration 400 : 0.6562814116477966
Mean training loss eporch  496 :  0.6006224157096559
Loss at iteration 50 : 0.2565603256225586
Loss at iteration 100 : 0.6547796130180359
Loss at iteration 150 : 0.46175408363342285
Loss at iteration 200 : 0.5050727128982544
Loss at iteration 250 : 0.6027517318725586
Loss at iteration 300 : 0.8959468603134155
Loss at iteration 350 : 0.5527445077896118
Loss at iteration 400 : 0.6690937280654907
Mean training loss eporch  497 :  0.6009142645070905
Loss at iteration 50 : 0.284354567527771
Loss at iteration 100 : 0.7517591714859009
Loss at iteration 150 : 0.420276015996933
Loss at iteration 200 : 0.7434005737304688
Loss at iteration 250 : 1.2570337057113647
Loss at iteration 300 : 0.34028905630111694
Loss at iteration 350 : 0.3174925446510315
Loss at iteration 400 : 0.6001837253570557
Mean training loss eporch  498 :  0.6012732509206229
Loss at iteration 50 : 0.27060216665267944
Loss at iteration 100 : 0.5128002166748047
Loss at iteration 150 : 0.3702089190483093
Loss at iteration 200 : 0.4411865472793579
Loss at iteration 250 : 1.2095447778701782
Loss at iteration 300 : 0.31320124864578247
Loss at iteration 350 : 0.9325040578842163
Loss at iteration 400 : 0.40669193863868713
Mean training loss eporch  499 :  0.6045236551774992
Loss at iteration 50 : 0.7155483961105347
Loss at iteration 100 : 0.39866867661476135
Loss at iteration 150 : 0.8063923120498657
Loss at iteration 200 : 0.34235334396362305
Loss at iteration 250 : 0.7173896431922913
Loss at iteration 300 : 0.4628470838069916
Loss at iteration 350 : 0.39361393451690674
Loss at iteration 400 : 0.28072088956832886
Mean training loss eporch  500 :  0.6021995673064694
Loss at iteration 50 : 0.621010422706604
Loss at iteration 100 : 0.26422351598739624
Loss at iteration 150 : 0.3506229519844055
Loss at iteration 200 : 0.8816285729408264
Loss at iteration 250 : 0.7024433016777039
Loss at iteration 300 : 1.380944013595581
Loss at iteration 350 : 0.5080592036247253
Loss at iteration 400 : 0.8522583246231079
Mean training loss eporch  501 :  0.603300340674116
Loss at iteration 50 : 0.47025299072265625
Loss at iteration 100 : 0.41490232944488525
Loss at iteration 150 : 0.43255680799484253
Loss at iteration 200 : 0.8954634666442871
Loss at iteration 250 : 0.7841479778289795
Loss at iteration 300 : 0.6878637075424194
Loss at iteration 350 : 0.6460418105125427
Loss at iteration 400 : 0.7133376002311707
Mean training loss eporch  502 :  0.5999085024801071
Loss at iteration 50 : 0.5938308835029602
Loss at iteration 100 : 0.3716358542442322
Loss at iteration 150 : 1.1382821798324585
Loss at iteration 200 : 0.4909058213233948
Loss at iteration 250 : 0.6656008362770081
Loss at iteration 300 : 0.45933765172958374
Loss at iteration 350 : 0.3496234118938446
Loss at iteration 400 : 0.8590946197509766
Mean training loss eporch  503 :  0.6001112805554151
Loss at iteration 50 : 0.6848454475402832
Loss at iteration 100 : 0.8538911938667297
Loss at iteration 150 : 0.7643138766288757
Loss at iteration 200 : 0.4711822271347046
Loss at iteration 250 : 0.6762112975120544
Loss at iteration 300 : 0.2938035726547241
Loss at iteration 350 : 0.8508098125457764
Loss at iteration 400 : 0.4176834225654602
Mean training loss eporch  504 :  0.5998687626535048
Loss at iteration 50 : 0.5375502705574036
Loss at iteration 100 : 1.110094428062439
Loss at iteration 150 : 0.7561360001564026
Loss at iteration 200 : 0.30996400117874146
Loss at iteration 250 : 0.43003594875335693
Loss at iteration 300 : 0.9429619312286377
Loss at iteration 350 : 1.1293938159942627
Loss at iteration 400 : 0.7906588315963745
Mean training loss eporch  505 :  0.6013054584107057
Loss at iteration 50 : 0.5748306512832642
Loss at iteration 100 : 0.2857232689857483
Loss at iteration 150 : 0.6571168899536133
Loss at iteration 200 : 0.6374251842498779
Loss at iteration 250 : 0.7532479763031006
Loss at iteration 300 : 0.8260136842727661
Loss at iteration 350 : 0.22508370876312256
Loss at iteration 400 : 0.7184844017028809
Mean training loss eporch  506 :  0.6016327130687611
Loss at iteration 50 : 0.7271377444267273
Loss at iteration 100 : 0.3472664952278137
Loss at iteration 150 : 0.5773258805274963
Loss at iteration 200 : 0.9015942811965942
Loss at iteration 250 : 0.6708626747131348
Loss at iteration 300 : 0.3922165036201477
Loss at iteration 350 : 0.5266609191894531
Loss at iteration 400 : 0.6630653142929077
Mean training loss eporch  507 :  0.6001412281140084
Loss at iteration 50 : 0.7507637739181519
Loss at iteration 100 : 0.6061471700668335
Loss at iteration 150 : 0.41349953413009644
Loss at iteration 200 : 0.6439290046691895
Loss at iteration 250 : 0.6559163331985474
Loss at iteration 300 : 1.3209532499313354
Loss at iteration 350 : 0.8033217787742615
Loss at iteration 400 : 0.3075908422470093
Mean training loss eporch  508 :  0.6008355070590439
Loss at iteration 50 : 0.7392178177833557
Loss at iteration 100 : 0.49238795042037964
Loss at iteration 150 : 0.36108189821243286
Loss at iteration 200 : 0.9045774936676025
Loss at iteration 250 : 0.6115974187850952
Loss at iteration 300 : 0.340925931930542
Loss at iteration 350 : 0.6793175935745239
Loss at iteration 400 : 0.37077534198760986
Mean training loss eporch  509 :  0.6008551658335822
Loss at iteration 50 : 0.37031179666519165
Loss at iteration 100 : 0.664068341255188
Loss at iteration 150 : 0.8860143423080444
Loss at iteration 200 : 0.39292046427726746
Loss at iteration 250 : 0.46873050928115845
Loss at iteration 300 : 0.3120940923690796
Loss at iteration 350 : 0.6138323545455933
Loss at iteration 400 : 1.2049391269683838
Mean training loss eporch  510 :  0.6003971197412687
Loss at iteration 50 : 0.8526788353919983
Loss at iteration 100 : 0.6992616653442383
Loss at iteration 150 : 0.509659469127655
Loss at iteration 200 : 0.6379715204238892
Loss at iteration 250 : 0.43441855907440186
Loss at iteration 300 : 0.40158379077911377
Loss at iteration 350 : 0.5888773202896118
Loss at iteration 400 : 0.40190210938453674
Mean training loss eporch  511 :  0.6001725145051832
Loss at iteration 50 : 0.6207830905914307
Loss at iteration 100 : 0.5439712405204773
Loss at iteration 150 : 0.8748428821563721
Loss at iteration 200 : 0.4737074673175812
Loss at iteration 250 : 0.3324250876903534
Loss at iteration 300 : 0.6089571118354797
Loss at iteration 350 : 1.416109561920166
Loss at iteration 400 : 0.4898498058319092
Mean training loss eporch  512 :  0.6002516094010507
Loss at iteration 50 : 0.6315593719482422
Loss at iteration 100 : 0.7572254538536072
Loss at iteration 150 : 0.8068059682846069
Loss at iteration 200 : 0.8053454756736755
Loss at iteration 250 : 0.5170149803161621
Loss at iteration 300 : 0.39889147877693176
Loss at iteration 350 : 0.47614550590515137
Loss at iteration 400 : 0.7426731586456299
Mean training loss eporch  513 :  0.6006110436846858
Loss at iteration 50 : 0.45009535551071167
Loss at iteration 100 : 0.7710892558097839
Loss at iteration 150 : 0.38792848587036133
Loss at iteration 200 : 0.36175596714019775
Loss at iteration 250 : 0.7719777226448059
Loss at iteration 300 : 0.30592358112335205
Loss at iteration 350 : 0.4590711295604706
Loss at iteration 400 : 0.3699999451637268
Mean training loss eporch  514 :  0.6007066137520721
Loss at iteration 50 : 0.4662441611289978
Loss at iteration 100 : 0.7747083306312561
Loss at iteration 150 : 0.6103471517562866
Loss at iteration 200 : 0.6351863741874695
Loss at iteration 250 : 1.013805627822876
Loss at iteration 300 : 0.3335365653038025
Loss at iteration 350 : 0.8160847425460815
Loss at iteration 400 : 0.35477423667907715
Mean training loss eporch  515 :  0.6001292661901547
Loss at iteration 50 : 0.8565431833267212
Loss at iteration 100 : 0.5794843435287476
Loss at iteration 150 : 0.44564753770828247
Loss at iteration 200 : 0.19865678250789642
Loss at iteration 250 : 0.485333651304245
Loss at iteration 300 : 0.777836799621582
Loss at iteration 350 : 0.7672325372695923
Loss at iteration 400 : 0.7352510094642639
Mean training loss eporch  516 :  0.6007142876830336
Loss at iteration 50 : 0.4285704493522644
Loss at iteration 100 : 0.29027342796325684
Loss at iteration 150 : 0.9864953756332397
Loss at iteration 200 : 1.09285569190979
Loss at iteration 250 : 0.6039708256721497
Loss at iteration 300 : 0.34928250312805176
Loss at iteration 350 : 0.5062398314476013
Loss at iteration 400 : 0.2615511119365692
Mean training loss eporch  517 :  0.6002645560043275
Loss at iteration 50 : 0.7586770057678223
Loss at iteration 100 : 0.5169023871421814
Loss at iteration 150 : 0.7590717077255249
Loss at iteration 200 : 0.8550640344619751
Loss at iteration 250 : 0.5038881897926331
Loss at iteration 300 : 0.32888263463974
Loss at iteration 350 : 0.869866669178009
Loss at iteration 400 : 0.6801097393035889
Mean training loss eporch  518 :  0.6002560698932596
Loss at iteration 50 : 0.7202702164649963
Loss at iteration 100 : 1.052208423614502
Loss at iteration 150 : 0.4421502947807312
Loss at iteration 200 : 0.8049176931381226
Loss at iteration 250 : 0.5712890028953552
Loss at iteration 300 : 0.6758685111999512
Loss at iteration 350 : 0.47692593932151794
Loss at iteration 400 : 0.32109057903289795
Mean training loss eporch  519 :  0.600345255180592
Loss at iteration 50 : 0.8047059774398804
Loss at iteration 100 : 0.7058612108230591
Loss at iteration 150 : 0.9816420078277588
Loss at iteration 200 : 1.0647037029266357
Loss at iteration 250 : 0.5150270462036133
Loss at iteration 300 : 0.3274102210998535
Loss at iteration 350 : 0.35896605253219604
Loss at iteration 400 : 0.6559249758720398
Mean training loss eporch  520 :  0.6010892822870759
Loss at iteration 50 : 0.7315830588340759
Loss at iteration 100 : 0.46238791942596436
Loss at iteration 150 : 0.7691339254379272
Loss at iteration 200 : 0.744896411895752
Loss at iteration 250 : 0.4465481638908386
Loss at iteration 300 : 0.5957880616188049
Loss at iteration 350 : 0.7510138750076294
Loss at iteration 400 : 0.5347303152084351
Mean training loss eporch  521 :  0.5998145741130739
Loss at iteration 50 : 0.48822078108787537
Loss at iteration 100 : 0.3906104564666748
Loss at iteration 150 : 0.7970765233039856
Loss at iteration 200 : 0.5366467237472534
Loss at iteration 250 : 0.748528242111206
Loss at iteration 300 : 0.3559470474720001
Loss at iteration 350 : 0.7653379440307617
Loss at iteration 400 : 0.676850438117981
Mean training loss eporch  522 :  0.6002976869988869
Loss at iteration 50 : 0.5349291563034058
Loss at iteration 100 : 0.4576026499271393
Loss at iteration 150 : 0.6755501627922058
Loss at iteration 200 : 0.5949963331222534
Loss at iteration 250 : 0.44787517189979553
Loss at iteration 300 : 0.346535861492157
Loss at iteration 350 : 0.7008183002471924
Loss at iteration 400 : 0.9051116704940796
Mean training loss eporch  523 :  0.5997700491761413
Loss at iteration 50 : 0.28591060638427734
Loss at iteration 100 : 0.3653869926929474
Loss at iteration 150 : 0.847245991230011
Loss at iteration 200 : 0.5657012462615967
Loss at iteration 250 : 0.5484423637390137
Loss at iteration 300 : 0.5493527054786682
Loss at iteration 350 : 0.7487307786941528
Loss at iteration 400 : 0.2874692678451538
Mean training loss eporch  524 :  0.6004733685153483
Loss at iteration 50 : 0.3820582926273346
Loss at iteration 100 : 0.32274994254112244
Loss at iteration 150 : 0.7165808081626892
Loss at iteration 200 : 0.5845677852630615
Loss at iteration 250 : 0.7626060843467712
Loss at iteration 300 : 0.5863258242607117
Loss at iteration 350 : 0.7607097625732422
Loss at iteration 400 : 0.5968196392059326
Mean training loss eporch  525 :  0.6008589224630941
Loss at iteration 50 : 0.4535020589828491
Loss at iteration 100 : 0.6424177289009094
Loss at iteration 150 : 0.6645199060440063
Loss at iteration 200 : 0.6683985590934753
Loss at iteration 250 : 0.8385320901870728
Loss at iteration 300 : 0.40073156356811523
Loss at iteration 350 : 0.458631694316864
Loss at iteration 400 : 0.5736565589904785
Mean training loss eporch  526 :  0.6005911259105922
Loss at iteration 50 : 0.6793891191482544
Loss at iteration 100 : 0.8704960942268372
Loss at iteration 150 : 0.7128316760063171
Loss at iteration 200 : 0.43626752495765686
Loss at iteration 250 : 1.0785350799560547
Loss at iteration 300 : 0.48479002714157104
Loss at iteration 350 : 0.5385816097259521
Loss at iteration 400 : 0.44752007722854614
Mean training loss eporch  527 :  0.6017503832621425
Loss at iteration 50 : 0.46018338203430176
Loss at iteration 100 : 0.6167781352996826
Loss at iteration 150 : 0.8345011472702026
Loss at iteration 200 : 0.5657768845558167
Loss at iteration 250 : 0.8095591068267822
Loss at iteration 300 : 0.5844076871871948
Loss at iteration 350 : 0.4324580430984497
Loss at iteration 400 : 0.4205261170864105
Mean training loss eporch  528 :  0.6001767958827617
Loss at iteration 50 : 0.48595675826072693
Loss at iteration 100 : 0.3270554542541504
Loss at iteration 150 : 0.5516348481178284
Loss at iteration 200 : 0.7218562364578247
Loss at iteration 250 : 0.5608538389205933
Loss at iteration 300 : 0.6841968297958374
Loss at iteration 350 : 0.5092406868934631
Loss at iteration 400 : 0.555034339427948
Mean training loss eporch  529 :  0.6007645730881414
Loss at iteration 50 : 0.49593934416770935
Loss at iteration 100 : 0.2266550362110138
Loss at iteration 150 : 0.6294777393341064
Loss at iteration 200 : 0.7803412079811096
Loss at iteration 250 : 0.860393762588501
Loss at iteration 300 : 0.30701905488967896
Loss at iteration 350 : 0.4787716865539551
Loss at iteration 400 : 0.3182198405265808
Mean training loss eporch  530 :  0.6006051635728823
Loss at iteration 50 : 0.5382179021835327
Loss at iteration 100 : 1.5232726335525513
Loss at iteration 150 : 0.3229251503944397
Loss at iteration 200 : 0.5272389054298401
Loss at iteration 250 : 0.6532686948776245
Loss at iteration 300 : 0.7286376953125
Loss at iteration 350 : 0.44624942541122437
Loss at iteration 400 : 0.8793532252311707
Mean training loss eporch  531 :  0.6001643452609601
Loss at iteration 50 : 0.7265809774398804
Loss at iteration 100 : 0.44446080923080444
Loss at iteration 150 : 1.0108205080032349
Loss at iteration 200 : 0.661533534526825
Loss at iteration 250 : 0.4781482517719269
Loss at iteration 300 : 0.9117465615272522
Loss at iteration 350 : 0.6922179460525513
Loss at iteration 400 : 0.8724690675735474
Mean training loss eporch  532 :  0.6004976761848937
Loss at iteration 50 : 0.4127429723739624
Loss at iteration 100 : 0.20548498630523682
Loss at iteration 150 : 0.492290735244751
Loss at iteration 200 : 0.8960133790969849
Loss at iteration 250 : 1.0228726863861084
Loss at iteration 300 : 0.638163149356842
Loss at iteration 350 : 0.7483140230178833
Loss at iteration 400 : 0.530251681804657
Mean training loss eporch  533 :  0.5999249741769158
Loss at iteration 50 : 0.2690812945365906
Loss at iteration 100 : 0.8839044570922852
Loss at iteration 150 : 1.1112775802612305
Loss at iteration 200 : 0.7491724491119385
Loss at iteration 250 : 0.3488485813140869
Loss at iteration 300 : 0.5295134782791138
Loss at iteration 350 : 1.5626659393310547
Loss at iteration 400 : 0.4091210961341858
Mean training loss eporch  534 :  0.6000484590639983
Loss at iteration 50 : 0.38612014055252075
Loss at iteration 100 : 1.1682243347167969
Loss at iteration 150 : 0.4436337947845459
Loss at iteration 200 : 0.5937248468399048
Loss at iteration 250 : 1.0089805126190186
Loss at iteration 300 : 0.925378680229187
Loss at iteration 350 : 0.7078385353088379
Loss at iteration 400 : 0.5458173751831055
Mean training loss eporch  535 :  0.600391635545968
Loss at iteration 50 : 0.7627822160720825
Loss at iteration 100 : 0.2995569109916687
Loss at iteration 150 : 0.8287402391433716
Loss at iteration 200 : 0.3385201394557953
Loss at iteration 250 : 0.7320611476898193
Loss at iteration 300 : 0.8766740560531616
Loss at iteration 350 : 0.5671172142028809
Loss at iteration 400 : 0.42788952589035034
Mean training loss eporch  536 :  0.5997255065983721
Loss at iteration 50 : 0.42724698781967163
Loss at iteration 100 : 0.5891120433807373
Loss at iteration 150 : 0.31105050444602966
Loss at iteration 200 : 0.6133432388305664
Loss at iteration 250 : 0.5212511420249939
Loss at iteration 300 : 0.7399743795394897
Loss at iteration 350 : 0.364719957113266
Loss at iteration 400 : 0.38955962657928467
Mean training loss eporch  537 :  0.5999951100001956
Loss at iteration 50 : 0.49957868456840515
Loss at iteration 100 : 0.6420029997825623
Loss at iteration 150 : 0.7869831323623657
Loss at iteration 200 : 1.2702984809875488
Loss at iteration 250 : 0.759152352809906
Loss at iteration 300 : 0.24222856760025024
Loss at iteration 350 : 0.7573359608650208
Loss at iteration 400 : 0.3167131543159485
Mean training loss eporch  538 :  0.599650039968202
Loss at iteration 50 : 0.5716934204101562
Loss at iteration 100 : 0.6189724206924438
Loss at iteration 150 : 0.2830258309841156
Loss at iteration 200 : 0.6488072872161865
Loss at iteration 250 : 0.6777498722076416
Loss at iteration 300 : 0.3156184256076813
Loss at iteration 350 : 0.2920418083667755
Loss at iteration 400 : 0.9045060873031616
Mean training loss eporch  539 :  0.5991629662468294
Loss at iteration 50 : 0.4676746726036072
Loss at iteration 100 : 0.45603513717651367
Loss at iteration 150 : 0.6803202629089355
Loss at iteration 200 : 0.3684470057487488
Loss at iteration 250 : 0.6607046127319336
Loss at iteration 300 : 0.8338429927825928
Loss at iteration 350 : 0.8223706483840942
Loss at iteration 400 : 0.42683327198028564
Mean training loss eporch  540 :  0.5999319571417009
Loss at iteration 50 : 0.6944586038589478
Loss at iteration 100 : 0.8842966556549072
Loss at iteration 150 : 0.6894962787628174
Loss at iteration 200 : 0.3216545283794403
Loss at iteration 250 : 0.4902018904685974
Loss at iteration 300 : 0.888229489326477
Loss at iteration 350 : 0.4321969151496887
Loss at iteration 400 : 0.926173746585846
Mean training loss eporch  541 :  0.6001810045587108
Loss at iteration 50 : 0.8282321095466614
Loss at iteration 100 : 0.8175231218338013
Loss at iteration 150 : 1.0035063028335571
Loss at iteration 200 : 0.7026948928833008
Loss at iteration 250 : 0.2510920763015747
Loss at iteration 300 : 0.2782781422138214
Loss at iteration 350 : 0.353645384311676
Loss at iteration 400 : 1.0123625993728638
Mean training loss eporch  542 :  0.5996085142407717
Loss at iteration 50 : 0.46191439032554626
Loss at iteration 100 : 0.4263937771320343
Loss at iteration 150 : 0.8243536353111267
Loss at iteration 200 : 0.5707487463951111
Loss at iteration 250 : 0.20739172399044037
Loss at iteration 300 : 0.4706568717956543
Loss at iteration 350 : 0.6721804141998291
Loss at iteration 400 : 0.5498637557029724
Mean training loss eporch  543 :  0.5997871353620906
Loss at iteration 50 : 0.3787420988082886
Loss at iteration 100 : 0.7278956174850464
Loss at iteration 150 : 0.39364027976989746
Loss at iteration 200 : 0.6067441701889038
Loss at iteration 250 : 0.2802262306213379
Loss at iteration 300 : 0.33856767416000366
Loss at iteration 350 : 0.5564473867416382
Loss at iteration 400 : 0.5870606303215027
Mean training loss eporch  544 :  0.5998189963500596
Loss at iteration 50 : 0.34706801176071167
Loss at iteration 100 : 0.5009350180625916
Loss at iteration 150 : 0.723663330078125
Loss at iteration 200 : 0.46758970618247986
Loss at iteration 250 : 0.9078874588012695
Loss at iteration 300 : 0.8586475253105164
Loss at iteration 350 : 0.6631293296813965
Loss at iteration 400 : 1.2979575395584106
Mean training loss eporch  545 :  0.5996753094809739
Loss at iteration 50 : 0.6568285226821899
Loss at iteration 100 : 0.3107169270515442
Loss at iteration 150 : 0.6641161441802979
Loss at iteration 200 : 1.480255126953125
Loss at iteration 250 : 0.3935489356517792
Loss at iteration 300 : 0.37199580669403076
Loss at iteration 350 : 0.8647168874740601
Loss at iteration 400 : 0.5560524463653564
Mean training loss eporch  546 :  0.5998911504454142
Loss at iteration 50 : 0.9509223699569702
Loss at iteration 100 : 0.4114828109741211
Loss at iteration 150 : 0.42479896545410156
Loss at iteration 200 : 1.0413224697113037
Loss at iteration 250 : 0.8423460125923157
Loss at iteration 300 : 0.37843769788742065
Loss at iteration 350 : 0.2874046266078949
Loss at iteration 400 : 0.7361490726470947
Mean training loss eporch  547 :  0.5993268539285446
Loss at iteration 50 : 0.6661258935928345
Loss at iteration 100 : 0.7885087728500366
Loss at iteration 150 : 0.4211876094341278
Loss at iteration 200 : 0.4575358033180237
Loss at iteration 250 : 0.2558742165565491
Loss at iteration 300 : 0.7901091575622559
Loss at iteration 350 : 0.40336140990257263
Loss at iteration 400 : 0.4604630470275879
Mean training loss eporch  548 :  0.5995373844164904
Loss at iteration 50 : 0.3802831172943115
Loss at iteration 100 : 0.43782269954681396
Loss at iteration 150 : 0.9293028712272644
Loss at iteration 200 : 0.47452107071876526
Loss at iteration 250 : 0.4816269278526306
Loss at iteration 300 : 0.4011976420879364
Loss at iteration 350 : 0.9372836947441101
Loss at iteration 400 : 0.35077786445617676
Mean training loss eporch  549 :  0.6000032506782912
Loss at iteration 50 : 0.5386434197425842
Loss at iteration 100 : 0.4245234429836273
Loss at iteration 150 : 0.24528780579566956
Loss at iteration 200 : 0.9544855356216431
Loss at iteration 250 : 1.1658351421356201
Loss at iteration 300 : 0.304985910654068
Loss at iteration 350 : 0.7603294849395752
Loss at iteration 400 : 0.6038112640380859
Mean training loss eporch  550 :  0.5996247856071711
Loss at iteration 50 : 1.0965797901153564
Loss at iteration 100 : 0.4145887494087219
Loss at iteration 150 : 0.6219773292541504
Loss at iteration 200 : 0.34598690271377563
Loss at iteration 250 : 0.5398987531661987
Loss at iteration 300 : 0.6613590717315674
Loss at iteration 350 : 0.5464128255844116
Loss at iteration 400 : 0.42639848589897156
Mean training loss eporch  551 :  0.6000465568271988
Loss at iteration 50 : 0.44240811467170715
Loss at iteration 100 : 0.5175180435180664
Loss at iteration 150 : 0.33224886655807495
Loss at iteration 200 : 0.7169046401977539
Loss at iteration 250 : 0.6388944387435913
Loss at iteration 300 : 0.4789605140686035
Loss at iteration 350 : 0.4488213062286377
Loss at iteration 400 : 0.9966780543327332
Mean training loss eporch  552 :  0.5996633100202265
Loss at iteration 50 : 0.909509539604187
Loss at iteration 100 : 0.4707288444042206
Loss at iteration 150 : 0.3065868616104126
Loss at iteration 200 : 0.47110849618911743
Loss at iteration 250 : 0.27735501527786255
Loss at iteration 300 : 1.1124978065490723
Loss at iteration 350 : 0.7980732321739197
Loss at iteration 400 : 0.7139049768447876
Mean training loss eporch  553 :  0.6004237867056521
Loss at iteration 50 : 0.39661043882369995
Loss at iteration 100 : 0.6190780401229858
Loss at iteration 150 : 0.19974753260612488
Loss at iteration 200 : 0.8946022987365723
Loss at iteration 250 : 0.6624044179916382
Loss at iteration 300 : 0.4224545955657959
Loss at iteration 350 : 0.9345133900642395
Loss at iteration 400 : 0.8637872934341431
Mean training loss eporch  554 :  0.5996800910664781
Loss at iteration 50 : 0.48555517196655273
Loss at iteration 100 : 0.34389787912368774
Loss at iteration 150 : 0.6789698600769043
Loss at iteration 200 : 0.671721875667572
Loss at iteration 250 : 0.4071102738380432
Loss at iteration 300 : 0.5696018934249878
Loss at iteration 350 : 0.5226129293441772
Loss at iteration 400 : 0.7148609161376953
Mean training loss eporch  555 :  0.5993377032434993
Loss at iteration 50 : 0.6899794340133667
Loss at iteration 100 : 0.27077004313468933
Loss at iteration 150 : 0.6652422547340393
Loss at iteration 200 : 0.6107096076011658
Loss at iteration 250 : 0.6729532480239868
Loss at iteration 300 : 0.9244763851165771
Loss at iteration 350 : 0.34081435203552246
Loss at iteration 400 : 0.5684337615966797
Mean training loss eporch  556 :  0.599328932239603
Loss at iteration 50 : 0.3850874900817871
Loss at iteration 100 : 0.916328489780426
Loss at iteration 150 : 0.46261775493621826
Loss at iteration 200 : 0.950516939163208
Loss at iteration 250 : 0.7297737002372742
Loss at iteration 300 : 0.4339514970779419
Loss at iteration 350 : 0.8318469524383545
Loss at iteration 400 : 0.27415725588798523
Mean training loss eporch  557 :  0.603888438889264
Loss at iteration 50 : 0.2722153961658478
Loss at iteration 100 : 0.392628014087677
Loss at iteration 150 : 0.824884831905365
Loss at iteration 200 : 0.6570712327957153
Loss at iteration 250 : 0.43289685249328613
Loss at iteration 300 : 0.842769980430603
Loss at iteration 350 : 0.3580179214477539
Loss at iteration 400 : 0.7718319892883301
Mean training loss eporch  558 :  0.5999124531588212
Loss at iteration 50 : 0.5109303593635559
Loss at iteration 100 : 0.38030046224594116
Loss at iteration 150 : 0.790995717048645
Loss at iteration 200 : 0.4488937258720398
Loss at iteration 250 : 0.9347745180130005
Loss at iteration 300 : 0.975097119808197
Loss at iteration 350 : 1.2440686225891113
Loss at iteration 400 : 0.7934856414794922
Mean training loss eporch  559 :  0.6005330393132607
Loss at iteration 50 : 0.4983832538127899
Loss at iteration 100 : 0.27320596575737
Loss at iteration 150 : 0.851690411567688
Loss at iteration 200 : 1.0112578868865967
Loss at iteration 250 : 0.671357274055481
Loss at iteration 300 : 0.818600058555603
Loss at iteration 350 : 0.3059808015823364
Loss at iteration 400 : 0.5728340148925781
Mean training loss eporch  560 :  0.59957668429132
Loss at iteration 50 : 0.35679981112480164
Loss at iteration 100 : 0.2980097532272339
Loss at iteration 150 : 0.22227364778518677
Loss at iteration 200 : 0.8371681571006775
Loss at iteration 250 : 0.7779743671417236
Loss at iteration 300 : 0.28894150257110596
Loss at iteration 350 : 0.4009222984313965
Loss at iteration 400 : 0.7694100141525269
Mean training loss eporch  561 :  0.60015463852428
Loss at iteration 50 : 0.758836567401886
Loss at iteration 100 : 0.6519783735275269
Loss at iteration 150 : 0.31813400983810425
Loss at iteration 200 : 1.1032829284667969
Loss at iteration 250 : 0.4693219065666199
Loss at iteration 300 : 1.146850824356079
Loss at iteration 350 : 0.2956368923187256
Loss at iteration 400 : 0.5601521730422974
Mean training loss eporch  562 :  0.6000227327103572
Loss at iteration 50 : 1.0464798212051392
Loss at iteration 100 : 0.4919974207878113
Loss at iteration 150 : 0.6375004649162292
Loss at iteration 200 : 0.7649812698364258
Loss at iteration 250 : 0.5860874652862549
Loss at iteration 300 : 0.3097151219844818
Loss at iteration 350 : 0.38922131061553955
Loss at iteration 400 : 0.7844289541244507
Mean training loss eporch  563 :  0.5994137784992366
Loss at iteration 50 : 0.4065231680870056
Loss at iteration 100 : 0.5354040861129761
Loss at iteration 150 : 0.4550362825393677
Loss at iteration 200 : 0.20856493711471558
Loss at iteration 250 : 0.700957179069519
Loss at iteration 300 : 0.5350787043571472
Loss at iteration 350 : 0.3703557848930359
Loss at iteration 400 : 0.46684956550598145
Mean training loss eporch  564 :  0.6000799429122643
Loss at iteration 50 : 0.3305051326751709
Loss at iteration 100 : 0.8858237266540527
Loss at iteration 150 : 0.6101596355438232
Loss at iteration 200 : 0.9694522619247437
Loss at iteration 250 : 0.3333227336406708
Loss at iteration 300 : 0.9053891897201538
Loss at iteration 350 : 0.3025254011154175
Loss at iteration 400 : 0.44181060791015625
Mean training loss eporch  565 :  0.5994828778064304
Loss at iteration 50 : 0.36190247535705566
Loss at iteration 100 : 0.7093765735626221
Loss at iteration 150 : 0.5036782026290894
Loss at iteration 200 : 0.6120470762252808
Loss at iteration 250 : 0.8082599639892578
Loss at iteration 300 : 0.8301007151603699
Loss at iteration 350 : 0.44867953658103943
Loss at iteration 400 : 0.37053626775741577
Mean training loss eporch  566 :  0.6031536917314936
Loss at iteration 50 : 0.3134378492832184
Loss at iteration 100 : 0.7500118017196655
Loss at iteration 150 : 0.7764471769332886
Loss at iteration 200 : 0.6017478108406067
Loss at iteration 250 : 0.6741281747817993
Loss at iteration 300 : 0.7678637504577637
Loss at iteration 350 : 0.7000350952148438
Loss at iteration 400 : 0.6018475890159607
Mean training loss eporch  567 :  0.6002596667529222
Loss at iteration 50 : 0.6895919442176819
Loss at iteration 100 : 0.42879804968833923
Loss at iteration 150 : 0.3548743724822998
Loss at iteration 200 : 0.3446877896785736
Loss at iteration 250 : 0.8490031361579895
Loss at iteration 300 : 1.0197603702545166
Loss at iteration 350 : 0.48331162333488464
Loss at iteration 400 : 1.0441405773162842
Mean training loss eporch  568 :  0.6032116619594429
Loss at iteration 50 : 0.7921637296676636
Loss at iteration 100 : 0.5882776379585266
Loss at iteration 150 : 0.7804043889045715
Loss at iteration 200 : 0.5818175077438354
Loss at iteration 250 : 0.9159274697303772
Loss at iteration 300 : 0.5098787546157837
Loss at iteration 350 : 0.7818700671195984
Loss at iteration 400 : 0.41730797290802
Mean training loss eporch  569 :  0.5996776129966894
Loss at iteration 50 : 0.28360119462013245
Loss at iteration 100 : 0.48729464411735535
Loss at iteration 150 : 0.4401966333389282
Loss at iteration 200 : 0.4697137773036957
Loss at iteration 250 : 0.6554964780807495
Loss at iteration 300 : 0.7361479997634888
Loss at iteration 350 : 0.4112163186073303
Loss at iteration 400 : 0.8564327955245972
Mean training loss eporch  570 :  0.5993979283245155
Loss at iteration 50 : 0.24909524619579315
Loss at iteration 100 : 0.6380531787872314
Loss at iteration 150 : 0.5972604751586914
Loss at iteration 200 : 0.6500493884086609
Loss at iteration 250 : 0.2983928620815277
Loss at iteration 300 : 0.41478124260902405
Loss at iteration 350 : 0.3163580298423767
Loss at iteration 400 : 0.5609867572784424
Mean training loss eporch  571 :  0.5995753169594324
Loss at iteration 50 : 0.4403644800186157
Loss at iteration 100 : 0.8158944845199585
Loss at iteration 150 : 0.7939277291297913
Loss at iteration 200 : 0.6678076982498169
Loss at iteration 250 : 0.4371144771575928
Loss at iteration 300 : 0.6444060206413269
Loss at iteration 350 : 0.8086742758750916
Loss at iteration 400 : 1.1234732866287231
Mean training loss eporch  572 :  0.5995539273448589
Loss at iteration 50 : 0.37429895997047424
Loss at iteration 100 : 0.435711145401001
Loss at iteration 150 : 0.35550999641418457
Loss at iteration 200 : 0.6913570165634155
Loss at iteration 250 : 0.39828774333000183
Loss at iteration 300 : 0.6498723030090332
Loss at iteration 350 : 0.570442795753479
Loss at iteration 400 : 0.47064948081970215
Mean training loss eporch  573 :  0.5992246765660063
Loss at iteration 50 : 0.8382489681243896
Loss at iteration 100 : 0.42300945520401
Loss at iteration 150 : 0.44171959161758423
Loss at iteration 200 : 0.4911961853504181
Loss at iteration 250 : 0.2812514007091522
Loss at iteration 300 : 0.35828280448913574
Loss at iteration 350 : 0.6532680988311768
Loss at iteration 400 : 0.33084940910339355
Mean training loss eporch  574 :  0.5997066851955892
Loss at iteration 50 : 0.5361136794090271
Loss at iteration 100 : 0.33562594652175903
Loss at iteration 150 : 0.34468138217926025
Loss at iteration 200 : 0.31377947330474854
Loss at iteration 250 : 0.32944321632385254
Loss at iteration 300 : 0.8993406295776367
Loss at iteration 350 : 0.6551648378372192
Loss at iteration 400 : 0.46539318561553955
Mean training loss eporch  575 :  0.5999641061631019
Loss at iteration 50 : 1.0818082094192505
Loss at iteration 100 : 0.7454057335853577
Loss at iteration 150 : 0.6048398017883301
Loss at iteration 200 : 0.3844330906867981
Loss at iteration 250 : 0.3615708351135254
Loss at iteration 300 : 0.2951282858848572
Loss at iteration 350 : 0.5047444105148315
Loss at iteration 400 : 0.7398884296417236
Mean training loss eporch  576 :  0.5994898142301448
Loss at iteration 50 : 0.7893420457839966
Loss at iteration 100 : 0.2867482006549835
Loss at iteration 150 : 0.5211406946182251
Loss at iteration 200 : 0.7470604777336121
Loss at iteration 250 : 0.625287652015686
Loss at iteration 300 : 1.158555269241333
Loss at iteration 350 : 0.5451541543006897
Loss at iteration 400 : 0.33673110604286194
Mean training loss eporch  577 :  0.6001701122442169
Loss at iteration 50 : 0.687599778175354
Loss at iteration 100 : 0.2801024317741394
Loss at iteration 150 : 0.45373135805130005
Loss at iteration 200 : 0.3542671203613281
Loss at iteration 250 : 0.3744317293167114
Loss at iteration 300 : 0.46429821848869324
Loss at iteration 350 : 1.0021257400512695
Loss at iteration 400 : 0.26540452241897583
Mean training loss eporch  578 :  0.6001168850625577
Loss at iteration 50 : 1.171602725982666
Loss at iteration 100 : 0.23642399907112122
Loss at iteration 150 : 0.7952696681022644
Loss at iteration 200 : 0.3662329912185669
Loss at iteration 250 : 0.3304629921913147
Loss at iteration 300 : 0.5116006731987
Loss at iteration 350 : 0.6457297801971436
Loss at iteration 400 : 0.3454914689064026
Mean training loss eporch  579 :  0.6007855343444465
Loss at iteration 50 : 0.41016173362731934
Loss at iteration 100 : 0.3238072991371155
Loss at iteration 150 : 0.6382285356521606
Loss at iteration 200 : 0.45369476079940796
Loss at iteration 250 : 0.5749468803405762
Loss at iteration 300 : 0.7920869588851929
Loss at iteration 350 : 0.3316434323787689
Loss at iteration 400 : 0.3859021067619324
Mean training loss eporch  580 :  0.5993256743312416
Loss at iteration 50 : 0.6914083361625671
Loss at iteration 100 : 1.0544922351837158
Loss at iteration 150 : 0.5928477048873901
Loss at iteration 200 : 0.5922058820724487
Loss at iteration 250 : 0.620470404624939
Loss at iteration 300 : 0.38025128841400146
Loss at iteration 350 : 0.3393379747867584
Loss at iteration 400 : 0.6012470126152039
Mean training loss eporch  581 :  0.5992531683546545
Loss at iteration 50 : 0.7548667788505554
Loss at iteration 100 : 0.6840436458587646
Loss at iteration 150 : 0.35444414615631104
Loss at iteration 200 : 0.8376185894012451
Loss at iteration 250 : 0.5899787545204163
Loss at iteration 300 : 0.9311976432800293
Loss at iteration 350 : 0.7900763750076294
Loss at iteration 400 : 1.0126774311065674
Mean training loss eporch  582 :  0.5997380218030092
Loss at iteration 50 : 1.0742281675338745
Loss at iteration 100 : 0.6631088852882385
Loss at iteration 150 : 0.3659297823905945
Loss at iteration 200 : 0.39092737436294556
Loss at iteration 250 : 0.4192468523979187
Loss at iteration 300 : 0.6059648990631104
Loss at iteration 350 : 0.5025835037231445
Loss at iteration 400 : 0.3253208100795746
Mean training loss eporch  583 :  0.6032482070572708
Loss at iteration 50 : 0.49441662430763245
Loss at iteration 100 : 0.7634889483451843
Loss at iteration 150 : 0.9432463645935059
Loss at iteration 200 : 0.6654406785964966
Loss at iteration 250 : 0.2751540541648865
Loss at iteration 300 : 1.0107760429382324
Loss at iteration 350 : 1.0136947631835938
Loss at iteration 400 : 0.60843825340271
Mean training loss eporch  584 :  0.5992741205900773
Loss at iteration 50 : 0.8121271133422852
Loss at iteration 100 : 0.7763724327087402
Loss at iteration 150 : 0.47530597448349
Loss at iteration 200 : 0.38497063517570496
Loss at iteration 250 : 0.6967228651046753
Loss at iteration 300 : 0.46156829595565796
Loss at iteration 350 : 0.4241858720779419
Loss at iteration 400 : 0.4750576913356781
Mean training loss eporch  585 :  0.599275171890387
Loss at iteration 50 : 0.9381712079048157
Loss at iteration 100 : 0.5484802722930908
Loss at iteration 150 : 0.6250085234642029
Loss at iteration 200 : 1.074040412902832
Loss at iteration 250 : 0.7842387557029724
Loss at iteration 300 : 0.5350247621536255
Loss at iteration 350 : 0.647097647190094
Loss at iteration 400 : 0.2836337983608246
Mean training loss eporch  586 :  0.5998468662992187
Loss at iteration 50 : 1.1059620380401611
Loss at iteration 100 : 0.3562582731246948
Loss at iteration 150 : 1.3212885856628418
Loss at iteration 200 : 0.5196301937103271
Loss at iteration 250 : 0.542128324508667
Loss at iteration 300 : 0.7994794845581055
Loss at iteration 350 : 0.3738238513469696
Loss at iteration 400 : 0.7090451717376709
Mean training loss eporch  587 :  0.5994880675200389
Loss at iteration 50 : 0.38383352756500244
Loss at iteration 100 : 0.5923346281051636
Loss at iteration 150 : 0.567936897277832
Loss at iteration 200 : 0.23888441920280457
Loss at iteration 250 : 0.3641473054885864
Loss at iteration 300 : 0.4722050726413727
Loss at iteration 350 : 0.4411085844039917
Loss at iteration 400 : 0.4082114100456238
Mean training loss eporch  588 :  0.601313705735677
Loss at iteration 50 : 0.28848743438720703
Loss at iteration 100 : 0.7158239483833313
Loss at iteration 150 : 0.2656000554561615
Loss at iteration 200 : 0.3366539478302002
Loss at iteration 250 : 1.021277666091919
Loss at iteration 300 : 0.3281342387199402
Loss at iteration 350 : 0.9502636194229126
Loss at iteration 400 : 0.4190554618835449
Mean training loss eporch  589 :  0.6002521285361239
Loss at iteration 50 : 0.4926948845386505
Loss at iteration 100 : 0.9881515502929688
Loss at iteration 150 : 0.3893035352230072
Loss at iteration 200 : 0.26839950680732727
Loss at iteration 250 : 0.5216286182403564
Loss at iteration 300 : 0.5657391548156738
Loss at iteration 350 : 0.38289088010787964
Loss at iteration 400 : 0.7487360239028931
Mean training loss eporch  590 :  0.6002348474322947
Loss at iteration 50 : 0.252986878156662
Loss at iteration 100 : 0.7800034284591675
Loss at iteration 150 : 0.33181679248809814
Loss at iteration 200 : 0.5836082696914673
Loss at iteration 250 : 0.48069024085998535
Loss at iteration 300 : 0.6760848164558411
Loss at iteration 350 : 0.4217226505279541
Loss at iteration 400 : 0.43980640172958374
Mean training loss eporch  591 :  0.5993155031527639
Loss at iteration 50 : 0.4920133352279663
Loss at iteration 100 : 0.34155037999153137
Loss at iteration 150 : 1.110290288925171
Loss at iteration 200 : 0.7694369554519653
Loss at iteration 250 : 0.4924798607826233
Loss at iteration 300 : 0.8230695128440857
Loss at iteration 350 : 0.33049604296684265
Loss at iteration 400 : 0.5678713321685791
Mean training loss eporch  592 :  0.6005117175557688
Loss at iteration 50 : 0.3103611171245575
Loss at iteration 100 : 0.6715527176856995
Loss at iteration 150 : 0.41970038414001465
Loss at iteration 200 : 0.48685523867607117
Loss at iteration 250 : 1.134966492652893
Loss at iteration 300 : 0.5369372367858887
Loss at iteration 350 : 0.34708738327026367
Loss at iteration 400 : 0.4119296073913574
Mean training loss eporch  593 :  0.5991964740469852
Loss at iteration 50 : 0.5303308963775635
Loss at iteration 100 : 1.1171236038208008
Loss at iteration 150 : 0.6635140180587769
Loss at iteration 200 : 0.6824902296066284
Loss at iteration 250 : 0.6091850399971008
Loss at iteration 300 : 0.23657825589179993
Loss at iteration 350 : 0.44538748264312744
Loss at iteration 400 : 0.4614662528038025
Mean training loss eporch  594 :  0.6002976904735972
Loss at iteration 50 : 0.5436704158782959
Loss at iteration 100 : 0.3194734454154968
Loss at iteration 150 : 0.4041154980659485
Loss at iteration 200 : 0.7654581069946289
Loss at iteration 250 : 0.3463919758796692
Loss at iteration 300 : 0.8575942516326904
Loss at iteration 350 : 0.32063549757003784
Loss at iteration 400 : 0.241754412651062
Mean training loss eporch  595 :  0.605617927963691
Loss at iteration 50 : 0.7178380489349365
Loss at iteration 100 : 0.27023613452911377
Loss at iteration 150 : 0.5180663466453552
Loss at iteration 200 : 0.45226967334747314
Loss at iteration 250 : 0.7527344226837158
Loss at iteration 300 : 0.6630856394767761
Loss at iteration 350 : 0.7851874232292175
Loss at iteration 400 : 0.9457376003265381
Mean training loss eporch  596 :  0.5992500519405032
Loss at iteration 50 : 0.7969628572463989
Loss at iteration 100 : 0.48610466718673706
Loss at iteration 150 : 0.4129449427127838
Loss at iteration 200 : 0.5542783737182617
Loss at iteration 250 : 0.37332165241241455
Loss at iteration 300 : 0.6578693985939026
Loss at iteration 350 : 0.27832427620887756
Loss at iteration 400 : 0.4025363028049469
Mean training loss eporch  597 :  0.5993144274626612
Loss at iteration 50 : 0.6265507340431213
Loss at iteration 100 : 0.36093974113464355
Loss at iteration 150 : 0.6808123588562012
Loss at iteration 200 : 0.8394860029220581
Loss at iteration 250 : 0.8983500599861145
Loss at iteration 300 : 0.3739031255245209
Loss at iteration 350 : 0.5408198833465576
Loss at iteration 400 : 0.2911273241043091
Mean training loss eporch  598 :  0.599777678837958
Loss at iteration 50 : 0.775871753692627
Loss at iteration 100 : 0.26215264201164246
Loss at iteration 150 : 0.6094849109649658
Loss at iteration 200 : 0.7065067291259766
Loss at iteration 250 : 0.6904247403144836
Loss at iteration 300 : 0.5757076740264893
Loss at iteration 350 : 0.4885673522949219
Loss at iteration 400 : 0.9263049960136414
Mean training loss eporch  599 :  0.6025325089292142
Loss at iteration 50 : 0.9433531165122986
Loss at iteration 100 : 0.41696780920028687
Loss at iteration 150 : 0.36548852920532227
Loss at iteration 200 : 0.8163468837738037
Loss at iteration 250 : 0.5182878375053406
Loss at iteration 300 : 0.34434401988983154
Loss at iteration 350 : 0.6264615654945374
Loss at iteration 400 : 0.4128286838531494
Mean training loss eporch  600 :  0.5993722911105562
Loss at iteration 50 : 0.4106411933898926
Loss at iteration 100 : 0.4212105870246887
Loss at iteration 150 : 0.7038514614105225
Loss at iteration 200 : 1.0091311931610107
Loss at iteration 250 : 0.6731960773468018
Loss at iteration 300 : 0.6500833034515381
Loss at iteration 350 : 0.440746545791626
Loss at iteration 400 : 0.8714430928230286
Mean training loss eporch  601 :  0.6032920114595793
Loss at iteration 50 : 0.9111529588699341
Loss at iteration 100 : 0.46109968423843384
Loss at iteration 150 : 0.4135681986808777
Loss at iteration 200 : 0.34125033020973206
Loss at iteration 250 : 0.6164613962173462
Loss at iteration 300 : 0.2396164834499359
Loss at iteration 350 : 0.30032432079315186
Loss at iteration 400 : 0.6934984922409058
Mean training loss eporch  602 :  0.5998517442578157
Loss at iteration 50 : 0.3811529576778412
Loss at iteration 100 : 0.31846749782562256
Loss at iteration 150 : 0.5769633650779724
Loss at iteration 200 : 0.6810612082481384
Loss at iteration 250 : 0.6208465099334717
Loss at iteration 300 : 0.5126409530639648
Loss at iteration 350 : 0.7504773139953613
Loss at iteration 400 : 0.7181151509284973
Mean training loss eporch  603 :  0.599629993395955
Loss at iteration 50 : 0.6793884038925171
Loss at iteration 100 : 0.6195793151855469
Loss at iteration 150 : 0.4749132990837097
Loss at iteration 200 : 0.32319238781929016
Loss at iteration 250 : 0.43770819902420044
Loss at iteration 300 : 0.9080291986465454
Loss at iteration 350 : 0.6280481815338135
Loss at iteration 400 : 0.28350311517715454
Mean training loss eporch  604 :  0.5991128562477672
Loss at iteration 50 : 0.7200154662132263
Loss at iteration 100 : 0.5333755016326904
Loss at iteration 150 : 0.44047725200653076
Loss at iteration 200 : 0.309605211019516
Loss at iteration 250 : 0.8723920583724976
Loss at iteration 300 : 0.6144325733184814
Loss at iteration 350 : 1.1917914152145386
Loss at iteration 400 : 1.2482624053955078
Mean training loss eporch  605 :  0.602223886703162
Loss at iteration 50 : 0.32488954067230225
Loss at iteration 100 : 1.0023813247680664
Loss at iteration 150 : 0.47401922941207886
Loss at iteration 200 : 0.8451527953147888
Loss at iteration 250 : 0.7510203719139099
Loss at iteration 300 : 0.9219110012054443
Loss at iteration 350 : 0.6824408769607544
Loss at iteration 400 : 0.515627920627594
Mean training loss eporch  606 :  0.6003035346689246
Loss at iteration 50 : 0.47352510690689087
Loss at iteration 100 : 0.6487941741943359
Loss at iteration 150 : 1.2252085208892822
Loss at iteration 200 : 0.3888058662414551
Loss at iteration 250 : 0.7966563701629639
Loss at iteration 300 : 0.8045663833618164
Loss at iteration 350 : 1.0573104619979858
Loss at iteration 400 : 0.7225660085678101
Mean training loss eporch  607 :  0.5995236569054992
Loss at iteration 50 : 0.4927799701690674
Loss at iteration 100 : 0.4419688880443573
Loss at iteration 150 : 0.65998774766922
Loss at iteration 200 : 0.6691104173660278
Loss at iteration 250 : 0.5003618001937866
Loss at iteration 300 : 1.41082763671875
Loss at iteration 350 : 0.3803415298461914
Loss at iteration 400 : 0.6034726500511169
Mean training loss eporch  608 :  0.5993644066255189
Loss at iteration 50 : 0.5449895858764648
Loss at iteration 100 : 0.6529037952423096
Loss at iteration 150 : 0.6068286895751953
Loss at iteration 200 : 0.5235066413879395
Loss at iteration 250 : 0.6215566396713257
Loss at iteration 300 : 0.49513566493988037
Loss at iteration 350 : 0.5296475887298584
Loss at iteration 400 : 0.6836326122283936
Mean training loss eporch  609 :  0.5994539451305108
Loss at iteration 50 : 0.3103824257850647
Loss at iteration 100 : 0.47718876600265503
Loss at iteration 150 : 0.4166720509529114
Loss at iteration 200 : 0.8659878373146057
Loss at iteration 250 : 0.29759135842323303
Loss at iteration 300 : 1.1140269041061401
Loss at iteration 350 : 0.6721948385238647
Loss at iteration 400 : 0.780146062374115
Mean training loss eporch  610 :  0.5998282594597928
Loss at iteration 50 : 0.4764746129512787
Loss at iteration 100 : 0.45421266555786133
Loss at iteration 150 : 0.5842641592025757
Loss at iteration 200 : 0.4560912251472473
Loss at iteration 250 : 0.958592414855957
Loss at iteration 300 : 0.512806236743927
Loss at iteration 350 : 0.5334233641624451
Loss at iteration 400 : 0.4186796545982361
Mean training loss eporch  611 :  0.598821014499985
Loss at iteration 50 : 1.169032335281372
Loss at iteration 100 : 0.34134137630462646
Loss at iteration 150 : 1.1001218557357788
Loss at iteration 200 : 0.7535308599472046
Loss at iteration 250 : 0.6085869073867798
Loss at iteration 300 : 0.368847131729126
Loss at iteration 350 : 0.6462010741233826
Loss at iteration 400 : 0.4930700659751892
Mean training loss eporch  612 :  0.6000807136297226
Loss at iteration 50 : 0.27217066287994385
Loss at iteration 100 : 0.45726311206817627
Loss at iteration 150 : 0.8159465193748474
Loss at iteration 200 : 0.5947115421295166
Loss at iteration 250 : 0.5557093024253845
Loss at iteration 300 : 0.2538125216960907
Loss at iteration 350 : 0.7538121938705444
Loss at iteration 400 : 0.6431667804718018
Mean training loss eporch  613 :  0.5998933594590345
Loss at iteration 50 : 0.3968861699104309
Loss at iteration 100 : 0.388645201921463
Loss at iteration 150 : 0.49298346042633057
Loss at iteration 200 : 0.4847767949104309
Loss at iteration 250 : 0.563206672668457
Loss at iteration 300 : 0.48672953248023987
Loss at iteration 350 : 1.2213935852050781
Loss at iteration 400 : 0.8562669157981873
Mean training loss eporch  614 :  0.5987848343469637
Loss at iteration 50 : 0.9979652762413025
Loss at iteration 100 : 0.31282132863998413
Loss at iteration 150 : 0.3047502040863037
Loss at iteration 200 : 0.4964878559112549
Loss at iteration 250 : 0.55454021692276
Loss at iteration 300 : 0.5575703978538513
Loss at iteration 350 : 0.7216871976852417
Loss at iteration 400 : 0.6587608456611633
Mean training loss eporch  615 :  0.5988420649627934
Loss at iteration 50 : 0.6720680594444275
Loss at iteration 100 : 0.48168694972991943
Loss at iteration 150 : 0.3909910023212433
Loss at iteration 200 : 0.39880895614624023
Loss at iteration 250 : 0.3816092014312744
Loss at iteration 300 : 0.46740928292274475
Loss at iteration 350 : 0.7646294832229614
Loss at iteration 400 : 0.48503926396369934
Mean training loss eporch  616 :  0.6002716183528772
Loss at iteration 50 : 0.9284908771514893
Loss at iteration 100 : 0.3247588574886322
Loss at iteration 150 : 0.6017894744873047
Loss at iteration 200 : 0.3754904866218567
Loss at iteration 250 : 0.7023252844810486
Loss at iteration 300 : 0.6181566119194031
Loss at iteration 350 : 0.3712839186191559
Loss at iteration 400 : 0.4079951047897339
Mean training loss eporch  617 :  0.5998384379552084
Loss at iteration 50 : 0.3494189381599426
Loss at iteration 100 : 0.38767385482788086
Loss at iteration 150 : 0.5819442272186279
Loss at iteration 200 : 0.7149836421012878
Loss at iteration 250 : 0.3928109407424927
Loss at iteration 300 : 0.6002882122993469
Loss at iteration 350 : 0.4800720512866974
Loss at iteration 400 : 1.2480285167694092
Mean training loss eporch  618 :  0.5991589313063921
Loss at iteration 50 : 0.3306126892566681
Loss at iteration 100 : 0.9126636981964111
Loss at iteration 150 : 0.8718408346176147
Loss at iteration 200 : 0.24198642373085022
Loss at iteration 250 : 0.6794770956039429
Loss at iteration 300 : 0.9414187669754028
Loss at iteration 350 : 0.44567710161209106
Loss at iteration 400 : 0.3897218406200409
Mean training loss eporch  619 :  0.5996981275188549
Loss at iteration 50 : 0.8098684549331665
Loss at iteration 100 : 0.3934691548347473
Loss at iteration 150 : 0.48386386036872864
Loss at iteration 200 : 0.7268375158309937
Loss at iteration 250 : 0.45153841376304626
Loss at iteration 300 : 0.3750624358654022
Loss at iteration 350 : 0.4048900902271271
Loss at iteration 400 : 0.915082573890686
Mean training loss eporch  620 :  0.5989882267109482
Loss at iteration 50 : 0.2420244812965393
Loss at iteration 100 : 0.47800275683403015
Loss at iteration 150 : 0.7403011322021484
Loss at iteration 200 : 0.7857415676116943
Loss at iteration 250 : 0.2372736632823944
Loss at iteration 300 : 0.7307088375091553
Loss at iteration 350 : 0.6780611872673035
Loss at iteration 400 : 0.640781044960022
Mean training loss eporch  621 :  0.5988732977632449
Loss at iteration 50 : 1.0153660774230957
Loss at iteration 100 : 0.21988801658153534
Loss at iteration 150 : 0.971725344657898
Loss at iteration 200 : 0.6627573370933533
Loss at iteration 250 : 0.5097900032997131
Loss at iteration 300 : 0.5633703470230103
Loss at iteration 350 : 0.37058812379837036
Loss at iteration 400 : 0.9794470071792603
Mean training loss eporch  622 :  0.5991710051894188
Loss at iteration 50 : 0.5515353083610535
Loss at iteration 100 : 0.29075777530670166
Loss at iteration 150 : 0.4433383345603943
Loss at iteration 200 : 0.3228350877761841
Loss at iteration 250 : 0.6130026578903198
Loss at iteration 300 : 0.34989264607429504
Loss at iteration 350 : 0.5578117370605469
Loss at iteration 400 : 0.9336475729942322
Mean training loss eporch  623 :  0.5994996448536091
Loss at iteration 50 : 0.7031053900718689
Loss at iteration 100 : 0.49210089445114136
Loss at iteration 150 : 0.7469496130943298
Loss at iteration 200 : 0.43734976649284363
Loss at iteration 250 : 0.5364726781845093
Loss at iteration 300 : 0.35059911012649536
Loss at iteration 350 : 0.5489343404769897
Loss at iteration 400 : 0.7057801485061646
Mean training loss eporch  624 :  0.599283000445954
Loss at iteration 50 : 0.2900840640068054
Loss at iteration 100 : 0.4230749309062958
Loss at iteration 150 : 1.186328649520874
Loss at iteration 200 : 0.7487037181854248
Loss at iteration 250 : 0.7747117280960083
Loss at iteration 300 : 0.6275262832641602
Loss at iteration 350 : 0.7014713287353516
Loss at iteration 400 : 0.7420619130134583
Mean training loss eporch  625 :  0.599349134605829
Loss at iteration 50 : 0.654627799987793
Loss at iteration 100 : 0.42179083824157715
Loss at iteration 150 : 0.5267249345779419
Loss at iteration 200 : 0.36881107091903687
Loss at iteration 250 : 0.6839499473571777
Loss at iteration 300 : 1.040935754776001
Loss at iteration 350 : 0.7612241506576538
Loss at iteration 400 : 0.5354744791984558
Mean training loss eporch  626 :  0.5991266608371862
Loss at iteration 50 : 0.39354294538497925
Loss at iteration 100 : 0.43111729621887207
Loss at iteration 150 : 1.099241852760315
Loss at iteration 200 : 0.5762121081352234
Loss at iteration 250 : 1.0158125162124634
Loss at iteration 300 : 0.6131997108459473
Loss at iteration 350 : 0.41708409786224365
Loss at iteration 400 : 0.9293155074119568
Mean training loss eporch  627 :  0.599351015793903
Loss at iteration 50 : 0.5526261925697327
Loss at iteration 100 : 0.7597360610961914
Loss at iteration 150 : 0.6365096569061279
Loss at iteration 200 : 0.8418007493019104
Loss at iteration 250 : 1.193188190460205
Loss at iteration 300 : 0.2639884352684021
Loss at iteration 350 : 0.5695513486862183
Loss at iteration 400 : 0.2973695397377014
Mean training loss eporch  628 :  0.6000035779692667
Loss at iteration 50 : 0.461029589176178
Loss at iteration 100 : 0.29382702708244324
Loss at iteration 150 : 0.4921286702156067
Loss at iteration 200 : 0.548213541507721
Loss at iteration 250 : 0.5301555395126343
Loss at iteration 300 : 0.5881503224372864
Loss at iteration 350 : 0.2866508662700653
Loss at iteration 400 : 0.43274223804473877
Mean training loss eporch  629 :  0.5991277777560623
Loss at iteration 50 : 0.6300396919250488
Loss at iteration 100 : 0.6141036748886108
Loss at iteration 150 : 0.4810624420642853
Loss at iteration 200 : 0.39151817560195923
Loss at iteration 250 : 0.7097803354263306
Loss at iteration 300 : 0.8248873949050903
Loss at iteration 350 : 0.37243345379829407
Loss at iteration 400 : 0.8617240190505981
Mean training loss eporch  630 :  0.5989393904938826
Loss at iteration 50 : 0.8291863799095154
Loss at iteration 100 : 0.6570128202438354
Loss at iteration 150 : 0.5559817552566528
Loss at iteration 200 : 0.6738855838775635
Loss at iteration 250 : 0.4425491988658905
Loss at iteration 300 : 0.9439025521278381
Loss at iteration 350 : 0.5411514043807983
Loss at iteration 400 : 0.4278455674648285
Mean training loss eporch  631 :  0.5993242851153617
Loss at iteration 50 : 1.173396348953247
Loss at iteration 100 : 0.30108049511909485
Loss at iteration 150 : 0.3133389353752136
Loss at iteration 200 : 0.9758993983268738
Loss at iteration 250 : 0.4069210886955261
Loss at iteration 300 : 0.37132006883621216
Loss at iteration 350 : 0.5867841243743896
Loss at iteration 400 : 0.2508232295513153
Mean training loss eporch  632 :  0.5994950481794875
Loss at iteration 50 : 0.6756288409233093
Loss at iteration 100 : 0.29960960149765015
Loss at iteration 150 : 0.40797507762908936
Loss at iteration 200 : 0.45168548822402954
Loss at iteration 250 : 0.6256356239318848
Loss at iteration 300 : 0.42375046014785767
Loss at iteration 350 : 0.2494281530380249
Loss at iteration 400 : 0.651591956615448
Mean training loss eporch  633 :  0.5990392993383878
Loss at iteration 50 : 0.24771104753017426
Loss at iteration 100 : 0.6363142728805542
Loss at iteration 150 : 0.24786096811294556
Loss at iteration 200 : 0.7191059589385986
Loss at iteration 250 : 0.49238064885139465
Loss at iteration 300 : 0.5334932804107666
Loss at iteration 350 : 0.7622255682945251
Loss at iteration 400 : 0.6841585636138916
Mean training loss eporch  634 :  0.5989144992400712
Loss at iteration 50 : 0.5232741832733154
Loss at iteration 100 : 0.9170184135437012
Loss at iteration 150 : 0.292622834444046
Loss at iteration 200 : 0.6633273959159851
Loss at iteration 250 : 0.92913419008255
Loss at iteration 300 : 0.45947203040122986
Loss at iteration 350 : 0.3729361891746521
Loss at iteration 400 : 0.6610957384109497
Mean training loss eporch  635 :  0.5990764561856808
Loss at iteration 50 : 0.38663798570632935
Loss at iteration 100 : 0.6215551495552063
Loss at iteration 150 : 0.3123047351837158
Loss at iteration 200 : 0.5297582149505615
Loss at iteration 250 : 0.35934916138648987
Loss at iteration 300 : 0.7241494655609131
Loss at iteration 350 : 0.9875866770744324
Loss at iteration 400 : 0.4563596248626709
Mean training loss eporch  636 :  0.5991787202155109
Loss at iteration 50 : 0.9352722764015198
Loss at iteration 100 : 0.5422759056091309
Loss at iteration 150 : 0.25690531730651855
Loss at iteration 200 : 0.26232948899269104
Loss at iteration 250 : 0.3505634665489197
Loss at iteration 300 : 0.3275017738342285
Loss at iteration 350 : 0.5283477306365967
Loss at iteration 400 : 0.6612916588783264
Mean training loss eporch  637 :  0.5991602473863037
Loss at iteration 50 : 0.45011740922927856
Loss at iteration 100 : 0.4277912378311157
Loss at iteration 150 : 0.597798228263855
Loss at iteration 200 : 0.7927554845809937
Loss at iteration 250 : 0.5526123642921448
Loss at iteration 300 : 0.40867000818252563
Loss at iteration 350 : 0.871638298034668
Loss at iteration 400 : 0.48639872670173645
Mean training loss eporch  638 :  0.5991401008159056
Loss at iteration 50 : 0.906132698059082
Loss at iteration 100 : 0.5656430125236511
Loss at iteration 150 : 0.8421055674552917
Loss at iteration 200 : 0.5945749282836914
Loss at iteration 250 : 0.3338463306427002
Loss at iteration 300 : 0.3262111246585846
Loss at iteration 350 : 0.36288589239120483
Loss at iteration 400 : 0.6758051514625549
Mean training loss eporch  639 :  0.598757423727769
Loss at iteration 50 : 0.6534954309463501
Loss at iteration 100 : 0.3996613323688507
Loss at iteration 150 : 0.8185795545578003
Loss at iteration 200 : 0.5749680995941162
Loss at iteration 250 : 0.9067450761795044
Loss at iteration 300 : 0.503011167049408
Loss at iteration 350 : 0.492423415184021
Loss at iteration 400 : 1.2253923416137695
Mean training loss eporch  640 :  0.5995698939390781
Loss at iteration 50 : 0.3128214478492737
Loss at iteration 100 : 0.60782790184021
Loss at iteration 150 : 0.49079352617263794
Loss at iteration 200 : 0.4056127369403839
Loss at iteration 250 : 1.3088431358337402
Loss at iteration 300 : 1.190065860748291
Loss at iteration 350 : 0.4123924672603607
Loss at iteration 400 : 0.2772284150123596
Mean training loss eporch  641 :  0.5992053069608628
Loss at iteration 50 : 0.42856675386428833
Loss at iteration 100 : 1.0146900415420532
Loss at iteration 150 : 0.3658612072467804
Loss at iteration 200 : 0.7616859078407288
Loss at iteration 250 : 0.2724352777004242
Loss at iteration 300 : 0.8113060593605042
Loss at iteration 350 : 0.368826687335968
Loss at iteration 400 : 0.6745225191116333
Mean training loss eporch  642 :  0.5986588165297636
Loss at iteration 50 : 0.95025235414505
Loss at iteration 100 : 0.417927622795105
Loss at iteration 150 : 0.5204694271087646
Loss at iteration 200 : 0.5198946595191956
Loss at iteration 250 : 0.4765428304672241
Loss at iteration 300 : 0.457233190536499
Loss at iteration 350 : 0.9787787199020386
Loss at iteration 400 : 0.4610927104949951
Mean training loss eporch  643 :  0.6005630160727843
Loss at iteration 50 : 0.3666467070579529
Loss at iteration 100 : 0.38031628727912903
Loss at iteration 150 : 1.1791739463806152
Loss at iteration 200 : 0.5787500143051147
Loss at iteration 250 : 0.7712489366531372
Loss at iteration 300 : 0.8932410478591919
Loss at iteration 350 : 0.4657129943370819
Loss at iteration 400 : 0.3983493447303772
Mean training loss eporch  644 :  0.5988737552890329
Loss at iteration 50 : 0.5671882033348083
Loss at iteration 100 : 0.5121936798095703
Loss at iteration 150 : 0.8418401479721069
Loss at iteration 200 : 0.39693668484687805
Loss at iteration 250 : 1.2828428745269775
Loss at iteration 300 : 0.31700125336647034
Loss at iteration 350 : 0.7109665274620056
Loss at iteration 400 : 0.6660748720169067
Mean training loss eporch  645 :  0.5992072019809564
Loss at iteration 50 : 1.0476570129394531
Loss at iteration 100 : 0.9801086187362671
Loss at iteration 150 : 0.8923947811126709
Loss at iteration 200 : 1.0045667886734009
Loss at iteration 250 : 1.0327526330947876
Loss at iteration 300 : 0.6724516749382019
Loss at iteration 350 : 0.8930978178977966
Loss at iteration 400 : 0.365612655878067
Mean training loss eporch  646 :  0.5990385601940177
Loss at iteration 50 : 0.6682717204093933
Loss at iteration 100 : 0.68416428565979
Loss at iteration 150 : 0.4639182984828949
Loss at iteration 200 : 0.7875028848648071
Loss at iteration 250 : 0.6248626112937927
Loss at iteration 300 : 0.3791468143463135
Loss at iteration 350 : 0.7184596061706543
Loss at iteration 400 : 0.6791887283325195
Mean training loss eporch  647 :  0.598864068999686
Loss at iteration 50 : 0.5362215638160706
Loss at iteration 100 : 0.3578454256057739
Loss at iteration 150 : 0.4721986949443817
Loss at iteration 200 : 0.6108422875404358
Loss at iteration 250 : 0.4318379759788513
Loss at iteration 300 : 0.7388026118278503
Loss at iteration 350 : 0.29156291484832764
Loss at iteration 400 : 0.5825148224830627
Mean training loss eporch  648 :  0.5984600582264464
Loss at iteration 50 : 1.077078938484192
Loss at iteration 100 : 0.7902340888977051
Loss at iteration 150 : 0.5489428639411926
Loss at iteration 200 : 0.4763094186782837
Loss at iteration 250 : 0.9450713396072388
Loss at iteration 300 : 1.1000492572784424
Loss at iteration 350 : 0.6295976638793945
Loss at iteration 400 : 0.32386013865470886
Mean training loss eporch  649 :  0.6012720603646184
Loss at iteration 50 : 0.26223379373550415
Loss at iteration 100 : 0.32236284017562866
Loss at iteration 150 : 0.6624265909194946
Loss at iteration 200 : 0.6621072292327881
Loss at iteration 250 : 0.49351486563682556
Loss at iteration 300 : 0.6349679231643677
Loss at iteration 350 : 0.2751806080341339
Loss at iteration 400 : 1.1278128623962402
Mean training loss eporch  650 :  0.600360789073155
Loss at iteration 50 : 1.4787399768829346
Loss at iteration 100 : 0.6055525541305542
Loss at iteration 150 : 0.25545433163642883
Loss at iteration 200 : 0.7666491866111755
Loss at iteration 250 : 0.45794859528541565
Loss at iteration 300 : 0.4796270430088043
Loss at iteration 350 : 0.34548550844192505
Loss at iteration 400 : 0.851304292678833
Mean training loss eporch  651 :  0.5988933173730769
Loss at iteration 50 : 0.8810971975326538
Loss at iteration 100 : 0.6355223655700684
Loss at iteration 150 : 0.7025015354156494
Loss at iteration 200 : 0.25960373878479004
Loss at iteration 250 : 0.3733808398246765
Loss at iteration 300 : 0.743683934211731
Loss at iteration 350 : 0.855919599533081
Loss at iteration 400 : 0.33195751905441284
Mean training loss eporch  652 :  0.5990820475290174
Loss at iteration 50 : 0.555334210395813
Loss at iteration 100 : 0.7060772180557251
Loss at iteration 150 : 0.9431051015853882
Loss at iteration 200 : 0.7344248294830322
Loss at iteration 250 : 0.5290657877922058
Loss at iteration 300 : 0.5475304126739502
Loss at iteration 350 : 0.5311960577964783
Loss at iteration 400 : 0.4896717369556427
Mean training loss eporch  653 :  0.5992216899189179
Loss at iteration 50 : 0.47337451577186584
Loss at iteration 100 : 0.3796754479408264
Loss at iteration 150 : 0.34120023250579834
Loss at iteration 200 : 0.5206311941146851
Loss at iteration 250 : 0.5385921001434326
Loss at iteration 300 : 0.790859043598175
Loss at iteration 350 : 0.3695358633995056
Loss at iteration 400 : 0.4764868915081024
Mean training loss eporch  654 :  0.5994786742089041
Loss at iteration 50 : 0.7736383676528931
Loss at iteration 100 : 0.512358546257019
Loss at iteration 150 : 0.419752836227417
Loss at iteration 200 : 0.42684683203697205
Loss at iteration 250 : 0.32194119691848755
Loss at iteration 300 : 0.5025761127471924
Loss at iteration 350 : 0.9031201601028442
Loss at iteration 400 : 1.0515289306640625
Mean training loss eporch  655 :  0.5990268653885131
Loss at iteration 50 : 0.6285120248794556
Loss at iteration 100 : 0.8793812394142151
Loss at iteration 150 : 0.4018937945365906
Loss at iteration 200 : 0.9190853834152222
Loss at iteration 250 : 0.2785923480987549
Loss at iteration 300 : 0.2825060486793518
Loss at iteration 350 : 0.32907891273498535
Loss at iteration 400 : 1.0923998355865479
Mean training loss eporch  656 :  0.5992356887245927
Loss at iteration 50 : 0.6821802854537964
Loss at iteration 100 : 0.8284153938293457
Loss at iteration 150 : 1.0507965087890625
Loss at iteration 200 : 0.5469822287559509
Loss at iteration 250 : 0.7142618298530579
Loss at iteration 300 : 0.6742733120918274
Loss at iteration 350 : 0.5797815322875977
Loss at iteration 400 : 0.39642810821533203
Mean training loss eporch  657 :  0.5991415709524411
Loss at iteration 50 : 0.282572478055954
Loss at iteration 100 : 1.0013788938522339
Loss at iteration 150 : 0.6629831790924072
Loss at iteration 200 : 0.34465914964675903
Loss at iteration 250 : 0.34924131631851196
Loss at iteration 300 : 1.071035623550415
Loss at iteration 350 : 0.9578598737716675
Loss at iteration 400 : 0.31027770042419434
Mean training loss eporch  658 :  0.5997498741732584
Loss at iteration 50 : 0.9288309216499329
Loss at iteration 100 : 0.5104241371154785
Loss at iteration 150 : 0.7692334651947021
Loss at iteration 200 : 0.7133162021636963
Loss at iteration 250 : 0.5734987258911133
Loss at iteration 300 : 0.2710522413253784
Loss at iteration 350 : 0.8683983087539673
Loss at iteration 400 : 0.886544406414032
Mean training loss eporch  659 :  0.5986737351981514
Loss at iteration 50 : 0.3824702799320221
Loss at iteration 100 : 0.3491917550563812
Loss at iteration 150 : 0.3383826017379761
Loss at iteration 200 : 0.4826120138168335
Loss at iteration 250 : 0.6151107549667358
Loss at iteration 300 : 0.39551007747650146
Loss at iteration 350 : 0.5289182662963867
Loss at iteration 400 : 0.6045740842819214
Mean training loss eporch  660 :  0.5988135594757683
Loss at iteration 50 : 0.6448063254356384
Loss at iteration 100 : 0.34939566254615784
Loss at iteration 150 : 0.2722534239292145
Loss at iteration 200 : 0.6170216798782349
Loss at iteration 250 : 0.6498304009437561
Loss at iteration 300 : 0.6340378522872925
Loss at iteration 350 : 0.323714017868042
Loss at iteration 400 : 0.3063182830810547
Mean training loss eporch  661 :  0.5987702370224512
Loss at iteration 50 : 0.4312683939933777
Loss at iteration 100 : 0.5084525346755981
Loss at iteration 150 : 0.6728307008743286
Loss at iteration 200 : 0.26263904571533203
Loss at iteration 250 : 0.5445501804351807
Loss at iteration 300 : 0.4247989058494568
Loss at iteration 350 : 0.5852813124656677
Loss at iteration 400 : 0.26319313049316406
Mean training loss eporch  662 :  0.598505447564371
Loss at iteration 50 : 0.5818312168121338
Loss at iteration 100 : 0.5616870522499084
Loss at iteration 150 : 1.199425458908081
Loss at iteration 200 : 0.49742504954338074
Loss at iteration 250 : 0.8799129128456116
Loss at iteration 300 : 0.7686975598335266
Loss at iteration 350 : 0.448538601398468
Loss at iteration 400 : 0.4159458875656128
Mean training loss eporch  663 :  0.5993517634579953
Loss at iteration 50 : 0.3341517448425293
Loss at iteration 100 : 0.2646407186985016
Loss at iteration 150 : 0.7319485545158386
Loss at iteration 200 : 0.51607745885849
Loss at iteration 250 : 0.2691255807876587
Loss at iteration 300 : 0.24488848447799683
Loss at iteration 350 : 0.4103693664073944
Loss at iteration 400 : 0.6230582594871521
Mean training loss eporch  664 :  0.6051415255451951
Loss at iteration 50 : 1.0126903057098389
Loss at iteration 100 : 0.8245868682861328
Loss at iteration 150 : 0.7426833510398865
Loss at iteration 200 : 0.349150687456131
Loss at iteration 250 : 0.6347341537475586
Loss at iteration 300 : 0.3658439517021179
Loss at iteration 350 : 0.28989431262016296
Loss at iteration 400 : 0.39032888412475586
Mean training loss eporch  665 :  0.5985180737926821
Loss at iteration 50 : 0.3616364598274231
Loss at iteration 100 : 0.43678802251815796
Loss at iteration 150 : 0.4975586533546448
Loss at iteration 200 : 0.7537146806716919
Loss at iteration 250 : 0.6243016123771667
Loss at iteration 300 : 0.315231055021286
Loss at iteration 350 : 0.9462649822235107
Loss at iteration 400 : 0.8005859851837158
Mean training loss eporch  666 :  0.5987907019130585
Loss at iteration 50 : 0.4324460029602051
Loss at iteration 100 : 1.0521252155303955
Loss at iteration 150 : 0.5043055415153503
Loss at iteration 200 : 1.234513759613037
Loss at iteration 250 : 0.285054087638855
Loss at iteration 300 : 0.9876511693000793
Loss at iteration 350 : 0.5419707298278809
Loss at iteration 400 : 0.21876752376556396
Mean training loss eporch  667 :  0.5985535126630501
Loss at iteration 50 : 0.6572566628456116
Loss at iteration 100 : 0.40766221284866333
Loss at iteration 150 : 0.5686824917793274
Loss at iteration 200 : 0.6478997468948364
Loss at iteration 250 : 0.6495133638381958
Loss at iteration 300 : 0.8774436712265015
Loss at iteration 350 : 0.6224994659423828
Loss at iteration 400 : 0.6528180837631226
Mean training loss eporch  668 :  0.5993667615435583
Loss at iteration 50 : 0.28590458631515503
Loss at iteration 100 : 0.3611360490322113
Loss at iteration 150 : 0.5043187141418457
Loss at iteration 200 : 0.4716654419898987
Loss at iteration 250 : 0.5835733413696289
Loss at iteration 300 : 0.2838083505630493
Loss at iteration 350 : 0.46222031116485596
Loss at iteration 400 : 0.24302062392234802
Mean training loss eporch  669 :  0.5988039032226186
Loss at iteration 50 : 0.30486780405044556
Loss at iteration 100 : 0.6889423131942749
Loss at iteration 150 : 0.6143267154693604
Loss at iteration 200 : 0.7422958612442017
Loss at iteration 250 : 0.3947431743144989
Loss at iteration 300 : 0.5935611128807068
Loss at iteration 350 : 0.7906771898269653
Loss at iteration 400 : 0.504073977470398
Mean training loss eporch  670 :  0.5993069824282364
Loss at iteration 50 : 1.111664056777954
Loss at iteration 100 : 0.3806363642215729
Loss at iteration 150 : 0.4916394352912903
Loss at iteration 200 : 0.4571913480758667
Loss at iteration 250 : 1.2760066986083984
Loss at iteration 300 : 0.986213207244873
Loss at iteration 350 : 0.8772821426391602
Loss at iteration 400 : 0.8623980283737183
Mean training loss eporch  671 :  0.5993487903422304
Loss at iteration 50 : 0.3730846047401428
Loss at iteration 100 : 0.9424639940261841
Loss at iteration 150 : 0.696601390838623
Loss at iteration 200 : 0.9671660661697388
Loss at iteration 250 : 1.0410631895065308
Loss at iteration 300 : 0.7831208109855652
Loss at iteration 350 : 0.5016535520553589
Loss at iteration 400 : 0.34113526344299316
Mean training loss eporch  672 :  0.5987056355773066
Loss at iteration 50 : 0.4553447365760803
Loss at iteration 100 : 0.5549089312553406
Loss at iteration 150 : 0.7602716088294983
Loss at iteration 200 : 0.33983826637268066
Loss at iteration 250 : 0.8032008409500122
Loss at iteration 300 : 0.22545233368873596
Loss at iteration 350 : 0.36209797859191895
Loss at iteration 400 : 0.4338509738445282
Mean training loss eporch  673 :  0.5992101134874361
Loss at iteration 50 : 0.575664758682251
Loss at iteration 100 : 0.3747859299182892
Loss at iteration 150 : 0.5116411447525024
Loss at iteration 200 : 0.4828650951385498
Loss at iteration 250 : 0.5971982479095459
Loss at iteration 300 : 0.34498360753059387
Loss at iteration 350 : 0.48744189739227295
Loss at iteration 400 : 0.6013607382774353
Mean training loss eporch  674 :  0.5990419213413658
Loss at iteration 50 : 0.7715134024620056
Loss at iteration 100 : 0.5200111865997314
Loss at iteration 150 : 0.5666868686676025
Loss at iteration 200 : 0.2952144145965576
Loss at iteration 250 : 1.0945793390274048
Loss at iteration 300 : 1.2632540464401245
Loss at iteration 350 : 1.1098206043243408
Loss at iteration 400 : 0.36022406816482544
Mean training loss eporch  675 :  0.5988322453982627
Loss at iteration 50 : 0.46336162090301514
Loss at iteration 100 : 0.4179512858390808
Loss at iteration 150 : 1.7537181377410889
Loss at iteration 200 : 0.40659278631210327
Loss at iteration 250 : 0.4797072112560272
Loss at iteration 300 : 0.6584610342979431
Loss at iteration 350 : 0.7693262100219727
Loss at iteration 400 : 0.5501924753189087
Mean training loss eporch  676 :  0.603847560192972
Loss at iteration 50 : 0.3025484085083008
Loss at iteration 100 : 0.2666575610637665
Loss at iteration 150 : 0.6278358697891235
Loss at iteration 200 : 0.55898118019104
Loss at iteration 250 : 0.688478946685791
Loss at iteration 300 : 0.5126552581787109
Loss at iteration 350 : 0.3649895191192627
Loss at iteration 400 : 1.084737777709961
Mean training loss eporch  677 :  0.5987203633370956
Loss at iteration 50 : 0.9592748880386353
Loss at iteration 100 : 0.31133943796157837
Loss at iteration 150 : 0.6396729946136475
Loss at iteration 200 : 0.6953260898590088
Loss at iteration 250 : 0.5077154636383057
Loss at iteration 300 : 0.5203580856323242
Loss at iteration 350 : 0.2963254749774933
Loss at iteration 400 : 0.8587895035743713
Mean training loss eporch  678 :  0.5983710565975964
Loss at iteration 50 : 0.4517635107040405
Loss at iteration 100 : 0.39085012674331665
Loss at iteration 150 : 0.5304951667785645
Loss at iteration 200 : 0.7767242193222046
Loss at iteration 250 : 0.9399318695068359
Loss at iteration 300 : 0.7030636072158813
Loss at iteration 350 : 0.634594738483429
Loss at iteration 400 : 0.7641564607620239
Mean training loss eporch  679 :  0.6016078498465063
Loss at iteration 50 : 0.4360005259513855
Loss at iteration 100 : 0.2741815745830536
Loss at iteration 150 : 0.6728115081787109
Loss at iteration 200 : 1.1029831171035767
Loss at iteration 250 : 0.562095046043396
Loss at iteration 300 : 1.0519918203353882
Loss at iteration 350 : 0.4799434542655945
Loss at iteration 400 : 0.9369577169418335
Mean training loss eporch  680 :  0.5983557385045851
Loss at iteration 50 : 0.3887789249420166
Loss at iteration 100 : 0.3178683817386627
Loss at iteration 150 : 0.5288671255111694
Loss at iteration 200 : 0.7486366629600525
Loss at iteration 250 : 0.2517263889312744
Loss at iteration 300 : 0.429829865694046
Loss at iteration 350 : 0.732575535774231
Loss at iteration 400 : 0.770820677280426
Mean training loss eporch  681 :  0.6010017889497526
Loss at iteration 50 : 0.4468178153038025
Loss at iteration 100 : 0.876709520816803
Loss at iteration 150 : 0.28655701875686646
Loss at iteration 200 : 0.7678810358047485
Loss at iteration 250 : 0.3626226782798767
Loss at iteration 300 : 0.2754158675670624
Loss at iteration 350 : 0.9630565047264099
Loss at iteration 400 : 0.41323864459991455
Mean training loss eporch  682 :  0.5986514411565969
Loss at iteration 50 : 0.6796185970306396
Loss at iteration 100 : 0.5352526903152466
Loss at iteration 150 : 0.3435344099998474
Loss at iteration 200 : 0.6380763053894043
Loss at iteration 250 : 0.6433662176132202
Loss at iteration 300 : 0.23395943641662598
Loss at iteration 350 : 0.4182788133621216
Loss at iteration 400 : 0.45868536829948425
Mean training loss eporch  683 :  0.5996460994797437
Loss at iteration 50 : 0.5177607536315918
Loss at iteration 100 : 0.4633059501647949
Loss at iteration 150 : 0.9160751104354858
Loss at iteration 200 : 0.44940391182899475
Loss at iteration 250 : 0.3537892699241638
Loss at iteration 300 : 0.5821957588195801
Loss at iteration 350 : 0.2488688826560974
Loss at iteration 400 : 0.453563928604126
Mean training loss eporch  684 :  0.5991626711303343
Loss at iteration 50 : 0.571374237537384
Loss at iteration 100 : 0.7970154881477356
Loss at iteration 150 : 0.8111430406570435
Loss at iteration 200 : 0.33412307500839233
Loss at iteration 250 : 0.285220742225647
Loss at iteration 300 : 0.8327844142913818
Loss at iteration 350 : 0.7233721613883972
Loss at iteration 400 : 0.6823761463165283
Mean training loss eporch  685 :  0.6002712493319683
Loss at iteration 50 : 0.7789319753646851
Loss at iteration 100 : 0.5405322909355164
Loss at iteration 150 : 0.351691335439682
Loss at iteration 200 : 0.6465274691581726
Loss at iteration 250 : 0.3431217074394226
Loss at iteration 300 : 0.43819698691368103
Loss at iteration 350 : 0.3534066677093506
Loss at iteration 400 : 0.23754283785820007
Mean training loss eporch  686 :  0.5994350453462836
Loss at iteration 50 : 0.33161473274230957
Loss at iteration 100 : 1.16654634475708
Loss at iteration 150 : 0.5105855464935303
Loss at iteration 200 : 0.8638792037963867
Loss at iteration 250 : 0.4646475315093994
Loss at iteration 300 : 0.30989357829093933
Loss at iteration 350 : 0.5817232131958008
Loss at iteration 400 : 0.6610341668128967
Mean training loss eporch  687 :  0.5985576431580188
Loss at iteration 50 : 0.39883509278297424
Loss at iteration 100 : 0.631525993347168
Loss at iteration 150 : 0.571398138999939
Loss at iteration 200 : 1.0026189088821411
Loss at iteration 250 : 0.7426629066467285
Loss at iteration 300 : 0.2765803933143616
Loss at iteration 350 : 0.6710916757583618
Loss at iteration 400 : 0.23135191202163696
Mean training loss eporch  688 :  0.5987857319247563
Loss at iteration 50 : 0.6812829375267029
Loss at iteration 100 : 0.4486036002635956
Loss at iteration 150 : 0.4682943820953369
Loss at iteration 200 : 0.7696963548660278
Loss at iteration 250 : 0.5876778364181519
Loss at iteration 300 : 0.7018293142318726
Loss at iteration 350 : 1.0275465250015259
Loss at iteration 400 : 0.28019753098487854
Mean training loss eporch  689 :  0.5988663303010132
Loss at iteration 50 : 0.49542486667633057
Loss at iteration 100 : 0.25694435834884644
Loss at iteration 150 : 0.7170231938362122
Loss at iteration 200 : 0.33120015263557434
Loss at iteration 250 : 0.5850555300712585
Loss at iteration 300 : 0.3623241186141968
Loss at iteration 350 : 0.7069566249847412
Loss at iteration 400 : 0.4274924099445343
Mean training loss eporch  690 :  0.5991144841829223
Loss at iteration 50 : 0.4631214737892151
Loss at iteration 100 : 0.4335399270057678
Loss at iteration 150 : 0.43132588267326355
Loss at iteration 200 : 0.9242402911186218
Loss at iteration 250 : 0.4685526490211487
Loss at iteration 300 : 0.6663129329681396
Loss at iteration 350 : 0.44967690110206604
Loss at iteration 400 : 0.8091322183609009
Mean training loss eporch  691 :  0.5989608277186684
Loss at iteration 50 : 0.8012647032737732
Loss at iteration 100 : 0.6975178718566895
Loss at iteration 150 : 0.6072274446487427
Loss at iteration 200 : 0.9094198346138
Loss at iteration 250 : 0.697574257850647
Loss at iteration 300 : 0.8797143697738647
Loss at iteration 350 : 0.7940300703048706
Loss at iteration 400 : 0.44679075479507446
Mean training loss eporch  692 :  0.5991099118317724
Loss at iteration 50 : 1.0733773708343506
Loss at iteration 100 : 0.34326308965682983
Loss at iteration 150 : 0.9745723605155945
Loss at iteration 200 : 0.5721656084060669
Loss at iteration 250 : 0.46964719891548157
Loss at iteration 300 : 0.5386373996734619
Loss at iteration 350 : 0.45968306064605713
Loss at iteration 400 : 0.3671029210090637
Mean training loss eporch  693 :  0.5989503745875017
Loss at iteration 50 : 0.62319016456604
Loss at iteration 100 : 0.43775078654289246
Loss at iteration 150 : 0.4271407723426819
Loss at iteration 200 : 0.7276483774185181
Loss at iteration 250 : 1.4969043731689453
Loss at iteration 300 : 0.940345823764801
Loss at iteration 350 : 0.7881234884262085
Loss at iteration 400 : 0.905739426612854
Mean training loss eporch  694 :  0.604829705249301
Loss at iteration 50 : 0.8186244964599609
Loss at iteration 100 : 0.495572566986084
Loss at iteration 150 : 0.5855284333229065
Loss at iteration 200 : 0.6844386458396912
Loss at iteration 250 : 0.7282779216766357
Loss at iteration 300 : 0.6925569176673889
Loss at iteration 350 : 0.7514289617538452
Loss at iteration 400 : 0.7757335901260376
Mean training loss eporch  695 :  0.5989323137453318
Loss at iteration 50 : 0.43655019998550415
Loss at iteration 100 : 0.4320361018180847
Loss at iteration 150 : 0.47131550312042236
Loss at iteration 200 : 0.3117598295211792
Loss at iteration 250 : 0.4453614354133606
Loss at iteration 300 : 0.47185131907463074
Loss at iteration 350 : 0.4252670407295227
Loss at iteration 400 : 0.4554252028465271
Mean training loss eporch  696 :  0.5988928132059863
Loss at iteration 50 : 0.6909363269805908
Loss at iteration 100 : 0.6468740701675415
Loss at iteration 150 : 0.4431944489479065
Loss at iteration 200 : 0.6091492176055908
Loss at iteration 250 : 0.3456480801105499
Loss at iteration 300 : 0.7338142395019531
Loss at iteration 350 : 0.486776202917099
Loss at iteration 400 : 0.5387258529663086
Mean training loss eporch  697 :  0.5992980642837259
Loss at iteration 50 : 0.47026970982551575
Loss at iteration 100 : 0.6946548819541931
Loss at iteration 150 : 0.6775848269462585
Loss at iteration 200 : 0.994777262210846
Loss at iteration 250 : 0.6639647483825684
Loss at iteration 300 : 0.9544230103492737
Loss at iteration 350 : 0.7721028327941895
Loss at iteration 400 : 1.0303022861480713
Mean training loss eporch  698 :  0.5992761253308287
Loss at iteration 50 : 0.7674674987792969
Loss at iteration 100 : 0.6227961778640747
Loss at iteration 150 : 0.6849960088729858
Loss at iteration 200 : 0.5556285381317139
Loss at iteration 250 : 0.5347537994384766
Loss at iteration 300 : 0.7561750411987305
Loss at iteration 350 : 0.8676255941390991
Loss at iteration 400 : 0.9972918033599854
Mean training loss eporch  699 :  0.5997094774927794
Loss at iteration 50 : 0.3785957992076874
Loss at iteration 100 : 0.4610998034477234
Loss at iteration 150 : 0.9566813111305237
Loss at iteration 200 : 1.0956814289093018
Loss at iteration 250 : 0.4257141947746277
Loss at iteration 300 : 0.6881961226463318
Loss at iteration 350 : 0.8444826602935791
Loss at iteration 400 : 0.5341722965240479
Mean training loss eporch  700 :  0.5990092737324569
Loss at iteration 50 : 0.5215065479278564
Loss at iteration 100 : 1.278303623199463
Loss at iteration 150 : 0.4988372325897217
Loss at iteration 200 : 0.35871461033821106
Loss at iteration 250 : 0.6337360143661499
Loss at iteration 300 : 0.5671688318252563
Loss at iteration 350 : 0.3724351227283478
Loss at iteration 400 : 0.4575030207633972
Mean training loss eporch  701 :  0.5988464466392192
Loss at iteration 50 : 0.2969083786010742
Loss at iteration 100 : 0.4733738899230957
Loss at iteration 150 : 0.7391711473464966
Loss at iteration 200 : 1.0348858833312988
Loss at iteration 250 : 0.33115968108177185
Loss at iteration 300 : 0.8338717818260193
Loss at iteration 350 : 0.30218636989593506
Loss at iteration 400 : 0.8405888676643372
Mean training loss eporch  702 :  0.5985032987233769
Loss at iteration 50 : 0.62636399269104
Loss at iteration 100 : 0.6983867883682251
Loss at iteration 150 : 0.7027297019958496
Loss at iteration 200 : 0.7584896087646484
Loss at iteration 250 : 0.48496711254119873
Loss at iteration 300 : 0.42036205530166626
Loss at iteration 350 : 0.5486241579055786
Loss at iteration 400 : 1.0707303285598755
Mean training loss eporch  703 :  0.5985051662464848
Loss at iteration 50 : 0.8613054752349854
Loss at iteration 100 : 0.668876588344574
Loss at iteration 150 : 0.4341481328010559
Loss at iteration 200 : 0.48674291372299194
Loss at iteration 250 : 0.3428764045238495
Loss at iteration 300 : 0.5259624719619751
Loss at iteration 350 : 0.7036082148551941
Loss at iteration 400 : 0.6088496446609497
Mean training loss eporch  704 :  0.5991571090547493
Loss at iteration 50 : 1.143692970275879
Loss at iteration 100 : 0.3440951108932495
Loss at iteration 150 : 0.6888937950134277
Loss at iteration 200 : 0.527733564376831
Loss at iteration 250 : 0.4236324429512024
Loss at iteration 300 : 0.5652989745140076
Loss at iteration 350 : 0.5944160223007202
Loss at iteration 400 : 0.482742577791214
Mean training loss eporch  705 :  0.5984917167791337
Loss at iteration 50 : 0.8176987171173096
Loss at iteration 100 : 0.32917869091033936
Loss at iteration 150 : 0.6702957153320312
Loss at iteration 200 : 1.056210994720459
Loss at iteration 250 : 0.7980532646179199
Loss at iteration 300 : 0.7878005504608154
Loss at iteration 350 : 0.3319653272628784
Loss at iteration 400 : 0.9777798652648926
Mean training loss eporch  706 :  0.5982788244404333
Loss at iteration 50 : 0.8459048271179199
Loss at iteration 100 : 0.756859302520752
Loss at iteration 150 : 0.31931331753730774
Loss at iteration 200 : 0.5410720109939575
Loss at iteration 250 : 0.47238022089004517
Loss at iteration 300 : 0.6447147130966187
Loss at iteration 350 : 0.4001094698905945
Loss at iteration 400 : 0.6051404476165771
Mean training loss eporch  707 :  0.5984140495548333
Loss at iteration 50 : 0.716096818447113
Loss at iteration 100 : 1.4476878643035889
Loss at iteration 150 : 1.235457420349121
Loss at iteration 200 : 0.40998736023902893
Loss at iteration 250 : 0.5645620822906494
Loss at iteration 300 : 0.7443946003913879
Loss at iteration 350 : 0.5520339608192444
Loss at iteration 400 : 0.6757888793945312
Mean training loss eporch  708 :  0.5985380263806993
Loss at iteration 50 : 0.29851096868515015
Loss at iteration 100 : 0.7918606996536255
Loss at iteration 150 : 0.502570629119873
Loss at iteration 200 : 0.3232000470161438
Loss at iteration 250 : 0.5858516693115234
Loss at iteration 300 : 0.4648444950580597
Loss at iteration 350 : 0.30054622888565063
Loss at iteration 400 : 0.7556338310241699
Mean training loss eporch  709 :  0.5981486174758239
Loss at iteration 50 : 0.2849465012550354
Loss at iteration 100 : 1.0854694843292236
Loss at iteration 150 : 0.964911162853241
Loss at iteration 200 : 0.49985814094543457
Loss at iteration 250 : 0.267486035823822
Loss at iteration 300 : 0.326718270778656
Loss at iteration 350 : 0.569572925567627
Loss at iteration 400 : 0.5881553888320923
Mean training loss eporch  710 :  0.5987880247323503
Loss at iteration 50 : 0.40906965732574463
Loss at iteration 100 : 0.31318068504333496
Loss at iteration 150 : 0.6104418039321899
Loss at iteration 200 : 0.9466044902801514
Loss at iteration 250 : 1.23236882686615
Loss at iteration 300 : 0.9184674024581909
Loss at iteration 350 : 0.3736359775066376
Loss at iteration 400 : 0.7417954802513123
Mean training loss eporch  711 :  0.5992052888856875
Loss at iteration 50 : 0.7475490570068359
Loss at iteration 100 : 0.7533485889434814
Loss at iteration 150 : 0.3222111463546753
Loss at iteration 200 : 0.5003175735473633
Loss at iteration 250 : 0.594136655330658
Loss at iteration 300 : 0.40251100063323975
Loss at iteration 350 : 0.2624129056930542
Loss at iteration 400 : 0.4473697543144226
Mean training loss eporch  712 :  0.5984511105134883
Loss at iteration 50 : 0.7638425230979919
Loss at iteration 100 : 0.8961518406867981
Loss at iteration 150 : 0.8779194951057434
Loss at iteration 200 : 0.500745952129364
Loss at iteration 250 : 0.29942089319229126
Loss at iteration 300 : 0.3454541862010956
Loss at iteration 350 : 0.6424033641815186
Loss at iteration 400 : 0.3024393320083618
Mean training loss eporch  713 :  0.598487259761635
Loss at iteration 50 : 0.6488085389137268
Loss at iteration 100 : 0.6459708213806152
Loss at iteration 150 : 1.1713175773620605
Loss at iteration 200 : 0.6847525835037231
Loss at iteration 250 : 0.6237941980361938
Loss at iteration 300 : 0.36981451511383057
Loss at iteration 350 : 1.119748830795288
Loss at iteration 400 : 0.29975706338882446
Mean training loss eporch  714 :  0.5985611245503875
Loss at iteration 50 : 0.9731746315956116
Loss at iteration 100 : 0.2648782730102539
Loss at iteration 150 : 0.21694812178611755
Loss at iteration 200 : 0.8356250524520874
Loss at iteration 250 : 0.9867507815361023
Loss at iteration 300 : 1.2646223306655884
Loss at iteration 350 : 0.5286079049110413
Loss at iteration 400 : 0.7399317622184753
Mean training loss eporch  715 :  0.5989355782355963
Loss at iteration 50 : 0.31870269775390625
Loss at iteration 100 : 0.3708803355693817
Loss at iteration 150 : 0.48174309730529785
Loss at iteration 200 : 1.365681529045105
Loss at iteration 250 : 0.718711256980896
Loss at iteration 300 : 0.321760892868042
Loss at iteration 350 : 0.7128522396087646
Loss at iteration 400 : 0.26023048162460327
Mean training loss eporch  716 :  0.5993182617719932
Loss at iteration 50 : 0.3435697555541992
Loss at iteration 100 : 0.5103981494903564
Loss at iteration 150 : 1.0178043842315674
Loss at iteration 200 : 0.5727272033691406
Loss at iteration 250 : 0.7267642021179199
Loss at iteration 300 : 0.46809011697769165
Loss at iteration 350 : 0.8541706204414368
Loss at iteration 400 : 1.053576946258545
Mean training loss eporch  717 :  0.5983308742465995
Loss at iteration 50 : 0.33998945355415344
Loss at iteration 100 : 0.6798062324523926
Loss at iteration 150 : 0.6346666812896729
Loss at iteration 200 : 0.8785867691040039
Loss at iteration 250 : 0.6970779895782471
Loss at iteration 300 : 0.4454275965690613
Loss at iteration 350 : 0.6860489845275879
Loss at iteration 400 : 0.5221585035324097
Mean training loss eporch  718 :  0.5993322615465776
Loss at iteration 50 : 0.3125467896461487
Loss at iteration 100 : 0.5490246415138245
Loss at iteration 150 : 0.3508601188659668
Loss at iteration 200 : 1.4485032558441162
Loss at iteration 250 : 0.3957000970840454
Loss at iteration 300 : 0.444304496049881
Loss at iteration 350 : 0.748343825340271
Loss at iteration 400 : 0.4078269600868225
Mean training loss eporch  719 :  0.5986538052625721
Loss at iteration 50 : 0.45461559295654297
Loss at iteration 100 : 0.4632440507411957
Loss at iteration 150 : 0.4838547110557556
Loss at iteration 200 : 0.2934289574623108
Loss at iteration 250 : 0.27041923999786377
Loss at iteration 300 : 0.6367876529693604
Loss at iteration 350 : 0.40836620330810547
Loss at iteration 400 : 0.6986238956451416
Mean training loss eporch  720 :  0.5986602331089866
Loss at iteration 50 : 0.9838156700134277
Loss at iteration 100 : 0.5566298961639404
Loss at iteration 150 : 0.2272014170885086
Loss at iteration 200 : 0.5547078847885132
Loss at iteration 250 : 1.1239402294158936
Loss at iteration 300 : 1.3668043613433838
Loss at iteration 350 : 0.3483625054359436
Loss at iteration 400 : 0.5565211772918701
Mean training loss eporch  721 :  0.5983427136535068
Loss at iteration 50 : 0.48383834958076477
Loss at iteration 100 : 0.7336797714233398
Loss at iteration 150 : 1.2535994052886963
Loss at iteration 200 : 0.29334163665771484
Loss at iteration 250 : 0.31026938557624817
Loss at iteration 300 : 0.5816090106964111
Loss at iteration 350 : 0.6986875534057617
Loss at iteration 400 : 0.5031453967094421
Mean training loss eporch  722 :  0.5986547356830584
Loss at iteration 50 : 0.8295766115188599
Loss at iteration 100 : 0.9622969627380371
Loss at iteration 150 : 0.5384924411773682
Loss at iteration 200 : 0.5653957724571228
Loss at iteration 250 : 0.58921217918396
Loss at iteration 300 : 1.0652104616165161
Loss at iteration 350 : 0.8129606246948242
Loss at iteration 400 : 0.6237586736679077
Mean training loss eporch  723 :  0.5985533005720831
Loss at iteration 50 : 0.40107986330986023
Loss at iteration 100 : 0.5745900869369507
Loss at iteration 150 : 0.4046684503555298
Loss at iteration 200 : 0.7154704332351685
Loss at iteration 250 : 0.3075636625289917
Loss at iteration 300 : 0.6743932962417603
Loss at iteration 350 : 0.5889232754707336
Loss at iteration 400 : 0.9235005974769592
Mean training loss eporch  724 :  0.5977694110552292
Loss at iteration 50 : 0.4180656671524048
Loss at iteration 100 : 0.28629642724990845
Loss at iteration 150 : 0.6533358097076416
Loss at iteration 200 : 0.606286883354187
Loss at iteration 250 : 0.49859240651130676
Loss at iteration 300 : 0.37611687183380127
Loss at iteration 350 : 0.8638412952423096
Loss at iteration 400 : 0.5876233577728271
Mean training loss eporch  725 :  0.599319456103404
Loss at iteration 50 : 0.7118805646896362
Loss at iteration 100 : 0.40942227840423584
Loss at iteration 150 : 0.3535112142562866
Loss at iteration 200 : 0.5636032819747925
Loss at iteration 250 : 0.5718319416046143
Loss at iteration 300 : 0.3411335349082947
Loss at iteration 350 : 0.2802334427833557
Loss at iteration 400 : 0.6848247051239014
Mean training loss eporch  726 :  0.5983757277775238
Loss at iteration 50 : 0.7092984914779663
Loss at iteration 100 : 0.43643802404403687
Loss at iteration 150 : 0.4897615313529968
Loss at iteration 200 : 0.30961236357688904
Loss at iteration 250 : 0.5622059106826782
Loss at iteration 300 : 0.5819237232208252
Loss at iteration 350 : 0.4942026138305664
Loss at iteration 400 : 0.31030166149139404
Mean training loss eporch  727 :  0.598490508248186
Loss at iteration 50 : 0.7379320859909058
Loss at iteration 100 : 0.5844001770019531
Loss at iteration 150 : 0.5175639390945435
Loss at iteration 200 : 0.5299227237701416
Loss at iteration 250 : 0.47800177335739136
Loss at iteration 300 : 0.7106317281723022
Loss at iteration 350 : 0.5923833847045898
Loss at iteration 400 : 0.34669357538223267
Mean training loss eporch  728 :  0.5981360348150334
Loss at iteration 50 : 0.731431782245636
Loss at iteration 100 : 0.502700686454773
Loss at iteration 150 : 0.32964587211608887
Loss at iteration 200 : 0.45829668641090393
Loss at iteration 250 : 0.4812495708465576
Loss at iteration 300 : 0.38430067896842957
Loss at iteration 350 : 0.24440929293632507
Loss at iteration 400 : 0.4857119023799896
Mean training loss eporch  729 :  0.5987102252351864
Loss at iteration 50 : 0.33751410245895386
Loss at iteration 100 : 0.44405078887939453
Loss at iteration 150 : 0.445998877286911
Loss at iteration 200 : 0.41159966588020325
Loss at iteration 250 : 0.4204649031162262
Loss at iteration 300 : 0.49192675948143005
Loss at iteration 350 : 0.7464933395385742
Loss at iteration 400 : 0.581652045249939
Mean training loss eporch  730 :  0.601419264960182
Loss at iteration 50 : 0.3430570065975189
Loss at iteration 100 : 0.4162294268608093
Loss at iteration 150 : 0.6353415250778198
Loss at iteration 200 : 0.4529961347579956
Loss at iteration 250 : 1.0160770416259766
Loss at iteration 300 : 0.3917336165904999
Loss at iteration 350 : 0.7656968832015991
Loss at iteration 400 : 0.4142269492149353
Mean training loss eporch  731 :  0.6007466162868145
Loss at iteration 50 : 0.42887893319129944
Loss at iteration 100 : 0.8236747980117798
Loss at iteration 150 : 0.35527312755584717
Loss at iteration 200 : 0.7098554372787476
Loss at iteration 250 : 0.696814775466919
Loss at iteration 300 : 0.7638907432556152
Loss at iteration 350 : 0.34386563301086426
Loss at iteration 400 : 1.4819259643554688
Mean training loss eporch  732 :  0.5999195612799961
Loss at iteration 50 : 0.9752178192138672
Loss at iteration 100 : 0.4250703454017639
Loss at iteration 150 : 0.3673018515110016
Loss at iteration 200 : 0.881773829460144
Loss at iteration 250 : 0.3429969549179077
Loss at iteration 300 : 0.616655707359314
Loss at iteration 350 : 0.6218744516372681
Loss at iteration 400 : 0.3018816113471985
Mean training loss eporch  733 :  0.5985585875775782
Loss at iteration 50 : 0.5431029796600342
Loss at iteration 100 : 0.2926378548145294
Loss at iteration 150 : 0.27729669213294983
Loss at iteration 200 : 0.7226765155792236
Loss at iteration 250 : 0.40707463026046753
Loss at iteration 300 : 0.41965252161026
Loss at iteration 350 : 0.8039329648017883
Loss at iteration 400 : 0.5782443881034851
Mean training loss eporch  734 :  0.5987952644247645
Loss at iteration 50 : 0.49710172414779663
Loss at iteration 100 : 0.4547017812728882
Loss at iteration 150 : 0.9735358357429504
Loss at iteration 200 : 0.6476826071739197
Loss at iteration 250 : 0.505530834197998
Loss at iteration 300 : 0.9909102320671082
Loss at iteration 350 : 0.7625070810317993
Loss at iteration 400 : 0.30592045187950134
Mean training loss eporch  735 :  0.5989591090067086
Loss at iteration 50 : 0.811124324798584
Loss at iteration 100 : 0.3353557586669922
Loss at iteration 150 : 0.7850702404975891
Loss at iteration 200 : 0.2562854588031769
Loss at iteration 250 : 0.6937885284423828
Loss at iteration 300 : 0.6035921573638916
Loss at iteration 350 : 0.5933376550674438
Loss at iteration 400 : 0.3890113830566406
Mean training loss eporch  736 :  0.5982481985616043
Loss at iteration 50 : 0.7950341701507568
Loss at iteration 100 : 0.3740667998790741
Loss at iteration 150 : 0.33807629346847534
Loss at iteration 200 : 0.4649820625782013
Loss at iteration 250 : 0.645475447177887
Loss at iteration 300 : 0.4940372109413147
Loss at iteration 350 : 0.530424952507019
Loss at iteration 400 : 0.5667068362236023
Mean training loss eporch  737 :  0.5984514212327687
Loss at iteration 50 : 0.6661754250526428
Loss at iteration 100 : 0.575330376625061
Loss at iteration 150 : 0.2787604033946991
Loss at iteration 200 : 0.6294251680374146
Loss at iteration 250 : 0.6410994529724121
Loss at iteration 300 : 0.5124410390853882
Loss at iteration 350 : 0.6639163494110107
Loss at iteration 400 : 0.6927961111068726
Mean training loss eporch  738 :  0.5988796137841293
Loss at iteration 50 : 0.6096321940422058
Loss at iteration 100 : 0.3329584002494812
Loss at iteration 150 : 0.5400682687759399
Loss at iteration 200 : 0.3491511344909668
Loss at iteration 250 : 0.730838418006897
Loss at iteration 300 : 0.28774645924568176
Loss at iteration 350 : 0.8032558560371399
Loss at iteration 400 : 0.41626793146133423
Mean training loss eporch  739 :  0.5990237965511634
Loss at iteration 50 : 0.5458617210388184
Loss at iteration 100 : 0.42161065340042114
Loss at iteration 150 : 0.20672285556793213
Loss at iteration 200 : 0.6738641858100891
Loss at iteration 250 : 0.4415573179721832
Loss at iteration 300 : 0.5290612578392029
Loss at iteration 350 : 0.21572846174240112
Loss at iteration 400 : 0.629541277885437
Mean training loss eporch  740 :  0.5983490110646449
Loss at iteration 50 : 0.4772745966911316
Loss at iteration 100 : 0.8770221471786499
Loss at iteration 150 : 0.5425584316253662
Loss at iteration 200 : 0.448628693819046
Loss at iteration 250 : 0.5118441581726074
Loss at iteration 300 : 0.38449999690055847
Loss at iteration 350 : 0.5795041918754578
Loss at iteration 400 : 1.0015383958816528
Mean training loss eporch  741 :  0.5985893055863445
Loss at iteration 50 : 0.41042089462280273
Loss at iteration 100 : 0.5552085638046265
Loss at iteration 150 : 0.77491295337677
Loss at iteration 200 : 0.3286166489124298
Loss at iteration 250 : 0.677071213722229
Loss at iteration 300 : 0.6888236999511719
Loss at iteration 350 : 0.856401801109314
Loss at iteration 400 : 0.6789730787277222
Mean training loss eporch  742 :  0.598936094530762
Loss at iteration 50 : 0.7675493955612183
Loss at iteration 100 : 0.6301299333572388
Loss at iteration 150 : 0.4994710087776184
Loss at iteration 200 : 0.7439376711845398
Loss at iteration 250 : 0.7360536456108093
Loss at iteration 300 : 0.33049917221069336
Loss at iteration 350 : 0.5514209270477295
Loss at iteration 400 : 0.2848277986049652
Mean training loss eporch  743 :  0.5993563968504491
Loss at iteration 50 : 1.3853893280029297
Loss at iteration 100 : 0.3252224624156952
Loss at iteration 150 : 0.6343979239463806
Loss at iteration 200 : 0.5633959770202637
Loss at iteration 250 : 1.0603888034820557
Loss at iteration 300 : 0.24817833304405212
Loss at iteration 350 : 0.42814603447914124
Loss at iteration 400 : 0.4055558145046234
Mean training loss eporch  744 :  0.59840454351608
Loss at iteration 50 : 0.5632147192955017
Loss at iteration 100 : 0.7126604318618774
Loss at iteration 150 : 0.5507345199584961
Loss at iteration 200 : 0.6375231742858887
Loss at iteration 250 : 0.38793423771858215
Loss at iteration 300 : 0.24890373647212982
Loss at iteration 350 : 0.41536998748779297
Loss at iteration 400 : 0.44109046459198
Mean training loss eporch  745 :  0.6046577252347373
Loss at iteration 50 : 0.3665938973426819
Loss at iteration 100 : 0.41279977560043335
Loss at iteration 150 : 0.4775921702384949
Loss at iteration 200 : 0.3678320646286011
Loss at iteration 250 : 0.40345531702041626
Loss at iteration 300 : 0.5844650864601135
Loss at iteration 350 : 0.6918529272079468
Loss at iteration 400 : 0.7452251315116882
Mean training loss eporch  746 :  0.6008652920747017
Loss at iteration 50 : 0.7115486860275269
Loss at iteration 100 : 0.6765553951263428
Loss at iteration 150 : 0.6682083606719971
Loss at iteration 200 : 0.6898434162139893
Loss at iteration 250 : 0.4075213074684143
Loss at iteration 300 : 0.5895153284072876
Loss at iteration 350 : 0.7341947555541992
Loss at iteration 400 : 0.8205322027206421
Mean training loss eporch  747 :  0.6018343125490865
Loss at iteration 50 : 0.5676811933517456
Loss at iteration 100 : 0.6726392507553101
Loss at iteration 150 : 0.3123467266559601
Loss at iteration 200 : 0.42592936754226685
Loss at iteration 250 : 0.4293924570083618
Loss at iteration 300 : 0.47950270771980286
Loss at iteration 350 : 0.6888992190361023
Loss at iteration 400 : 0.24228210747241974
Mean training loss eporch  748 :  0.5984174077286314
Loss at iteration 50 : 0.4786216914653778
Loss at iteration 100 : 0.412068247795105
Loss at iteration 150 : 0.38057106733322144
Loss at iteration 200 : 0.7528987526893616
Loss at iteration 250 : 0.30153346061706543
Loss at iteration 300 : 0.8564241528511047
Loss at iteration 350 : 0.7576261758804321
Loss at iteration 400 : 0.5948254466056824
Mean training loss eporch  749 :  0.5996042411156299
Loss at iteration 50 : 0.2655271887779236
Loss at iteration 100 : 0.2525591254234314
Loss at iteration 150 : 0.7918362617492676
Loss at iteration 200 : 0.3611404299736023
Loss at iteration 250 : 0.45014846324920654
Loss at iteration 300 : 0.68323814868927
Loss at iteration 350 : 0.6541700959205627
Loss at iteration 400 : 0.4055204391479492
Mean training loss eporch  750 :  0.598640565814726
Loss at iteration 50 : 0.6198557615280151
Loss at iteration 100 : 0.4701356887817383
Loss at iteration 150 : 0.3774939179420471
Loss at iteration 200 : 0.2296009659767151
Loss at iteration 250 : 0.8613277077674866
Loss at iteration 300 : 0.5897707343101501
Loss at iteration 350 : 0.8026819229125977
Loss at iteration 400 : 0.9046571850776672
Mean training loss eporch  751 :  0.5985504006591078
Loss at iteration 50 : 0.5535097718238831
Loss at iteration 100 : 0.6267821788787842
Loss at iteration 150 : 0.5306248664855957
Loss at iteration 200 : 0.6852983236312866
Loss at iteration 250 : 0.7203035354614258
Loss at iteration 300 : 0.8965774774551392
Loss at iteration 350 : 0.46655964851379395
Loss at iteration 400 : 0.2488030642271042
Mean training loss eporch  752 :  0.6024464907066169
Loss at iteration 50 : 0.8935970664024353
Loss at iteration 100 : 0.6940832138061523
Loss at iteration 150 : 0.5006890296936035
Loss at iteration 200 : 0.4732569456100464
Loss at iteration 250 : 0.4461224675178528
Loss at iteration 300 : 0.46444207429885864
Loss at iteration 350 : 0.28994280099868774
Loss at iteration 400 : 0.5411111116409302
Mean training loss eporch  753 :  0.5983153330705091
Loss at iteration 50 : 0.8529046773910522
Loss at iteration 100 : 0.5050557851791382
Loss at iteration 150 : 0.5951189398765564
Loss at iteration 200 : 0.532795786857605
Loss at iteration 250 : 0.36273396015167236
Loss at iteration 300 : 0.2913399040699005
Loss at iteration 350 : 0.3074026107788086
Loss at iteration 400 : 0.41662096977233887
Mean training loss eporch  754 :  0.5990703621119127
Loss at iteration 50 : 0.5695933103561401
Loss at iteration 100 : 0.8979206681251526
Loss at iteration 150 : 0.29184454679489136
Loss at iteration 200 : 0.6998001933097839
Loss at iteration 250 : 0.4758794605731964
Loss at iteration 300 : 0.8632073402404785
Loss at iteration 350 : 0.8566539287567139
Loss at iteration 400 : 0.6819831132888794
Mean training loss eporch  755 :  0.6041102618513621
Loss at iteration 50 : 0.3229309916496277
Loss at iteration 100 : 0.7835192680358887
Loss at iteration 150 : 0.3153075575828552
Loss at iteration 200 : 0.6194792985916138
Loss at iteration 250 : 0.43665045499801636
Loss at iteration 300 : 0.5311702489852905
Loss at iteration 350 : 0.881585955619812
Loss at iteration 400 : 0.3495616316795349
Mean training loss eporch  756 :  0.5992634084259448
Loss at iteration 50 : 0.7566342949867249
Loss at iteration 100 : 1.1712818145751953
Loss at iteration 150 : 0.8339998126029968
Loss at iteration 200 : 0.7458573579788208
Loss at iteration 250 : 0.5645995140075684
Loss at iteration 300 : 0.930625319480896
Loss at iteration 350 : 0.851779580116272
Loss at iteration 400 : 1.227797269821167
Mean training loss eporch  757 :  0.5990265573420867
Loss at iteration 50 : 0.8931838870048523
Loss at iteration 100 : 0.8029708862304688
Loss at iteration 150 : 0.47903358936309814
Loss at iteration 200 : 0.5381478071212769
Loss at iteration 250 : 0.832538366317749
Loss at iteration 300 : 0.25753265619277954
Loss at iteration 350 : 0.47392863035202026
Loss at iteration 400 : 0.5201348066329956
Mean training loss eporch  758 :  0.6028113440013253
Loss at iteration 50 : 0.37334543466567993
Loss at iteration 100 : 0.3287140130996704
Loss at iteration 150 : 0.33705490827560425
Loss at iteration 200 : 0.5847433805465698
Loss at iteration 250 : 0.6658200025558472
Loss at iteration 300 : 0.6252622008323669
Loss at iteration 350 : 0.2978968024253845
Loss at iteration 400 : 0.5646438598632812
Mean training loss eporch  759 :  0.5985563021871542
Loss at iteration 50 : 0.7201259732246399
Loss at iteration 100 : 0.8413068652153015
Loss at iteration 150 : 0.2875770330429077
Loss at iteration 200 : 0.5859702229499817
Loss at iteration 250 : 0.6740988492965698
Loss at iteration 300 : 0.5579204559326172
Loss at iteration 350 : 0.8322676420211792
Loss at iteration 400 : 0.7328128814697266
Mean training loss eporch  760 :  0.5980859638730507
Loss at iteration 50 : 0.3754715025424957
Loss at iteration 100 : 0.9501966834068298
Loss at iteration 150 : 0.9236835837364197
Loss at iteration 200 : 0.38228657841682434
Loss at iteration 250 : 0.7640337944030762
Loss at iteration 300 : 1.315200686454773
Loss at iteration 350 : 0.8260421752929688
Loss at iteration 400 : 1.0132558345794678
Mean training loss eporch  761 :  0.5985598657029627
Loss at iteration 50 : 1.1183505058288574
Loss at iteration 100 : 1.116147756576538
Loss at iteration 150 : 0.6380008459091187
Loss at iteration 200 : 0.3075535297393799
Loss at iteration 250 : 0.8633370399475098
Loss at iteration 300 : 0.33261990547180176
Loss at iteration 350 : 0.8192645311355591
Loss at iteration 400 : 0.5594738721847534
Mean training loss eporch  762 :  0.6009969419294409
Loss at iteration 50 : 0.7075339555740356
Loss at iteration 100 : 0.3728265166282654
Loss at iteration 150 : 0.4258759021759033
Loss at iteration 200 : 0.381218284368515
Loss at iteration 250 : 0.6106385588645935
Loss at iteration 300 : 0.49599990248680115
Loss at iteration 350 : 0.7495259046554565
Loss at iteration 400 : 0.8875988721847534
Mean training loss eporch  763 :  0.6041701044469671
Loss at iteration 50 : 0.41000497341156006
Loss at iteration 100 : 0.6612498760223389
Loss at iteration 150 : 0.4775826632976532
Loss at iteration 200 : 1.4048359394073486
Loss at iteration 250 : 0.8300474882125854
Loss at iteration 300 : 0.3493167757987976
Loss at iteration 350 : 0.4408470392227173
Loss at iteration 400 : 1.0758098363876343
Mean training loss eporch  764 :  0.5982807419960274
Loss at iteration 50 : 0.33743274211883545
Loss at iteration 100 : 0.3033764958381653
Loss at iteration 150 : 0.6095762848854065
Loss at iteration 200 : 0.5934668779373169
Loss at iteration 250 : 1.0120811462402344
Loss at iteration 300 : 0.8979380130767822
Loss at iteration 350 : 0.6576187610626221
Loss at iteration 400 : 0.23188388347625732
Mean training loss eporch  765 :  0.597594141960144
Loss at iteration 50 : 0.16895315051078796
Loss at iteration 100 : 0.680431604385376
Loss at iteration 150 : 0.5891872644424438
Loss at iteration 200 : 0.778409481048584
Loss at iteration 250 : 0.7754814624786377
Loss at iteration 300 : 0.741790235042572
Loss at iteration 350 : 0.8040702939033508
Loss at iteration 400 : 0.3459330201148987
Mean training loss eporch  766 :  0.599184165888303
Loss at iteration 50 : 0.359587699174881
Loss at iteration 100 : 0.39349624514579773
Loss at iteration 150 : 0.5330897569656372
Loss at iteration 200 : 0.45576977729797363
Loss at iteration 250 : 0.34788402915000916
Loss at iteration 300 : 0.5429328680038452
Loss at iteration 350 : 0.2984659969806671
Loss at iteration 400 : 0.6770216226577759
Mean training loss eporch  767 :  0.5984163872580224
Loss at iteration 50 : 0.4642891585826874
Loss at iteration 100 : 0.3291066586971283
Loss at iteration 150 : 0.7703733444213867
Loss at iteration 200 : 0.7362672090530396
Loss at iteration 250 : 0.4738807678222656
Loss at iteration 300 : 0.6959749460220337
Loss at iteration 350 : 0.7459322214126587
Loss at iteration 400 : 0.5594898462295532
Mean training loss eporch  768 :  0.6001737485952976
Loss at iteration 50 : 0.9319039583206177
Loss at iteration 100 : 1.2562031745910645
Loss at iteration 150 : 0.4524344503879547
Loss at iteration 200 : 0.6354258060455322
Loss at iteration 250 : 0.35561513900756836
Loss at iteration 300 : 0.6721818447113037
Loss at iteration 350 : 0.7978860139846802
Loss at iteration 400 : 0.4748408794403076
Mean training loss eporch  769 :  0.5983007654814977
Loss at iteration 50 : 0.2747699022293091
Loss at iteration 100 : 0.39036136865615845
Loss at iteration 150 : 0.6164761781692505
Loss at iteration 200 : 0.4866304099559784
Loss at iteration 250 : 0.2928788661956787
Loss at iteration 300 : 0.7423040270805359
Loss at iteration 350 : 0.6710937023162842
Loss at iteration 400 : 0.6086283922195435
Mean training loss eporch  770 :  0.5981027439838033
Loss at iteration 50 : 0.39693400263786316
Loss at iteration 100 : 0.31807073950767517
Loss at iteration 150 : 1.079012155532837
Loss at iteration 200 : 0.2992006540298462
Loss at iteration 250 : 0.37938663363456726
Loss at iteration 300 : 0.6653894782066345
Loss at iteration 350 : 0.23958474397659302
Loss at iteration 400 : 0.5283584594726562
Mean training loss eporch  771 :  0.5980914390808798
Loss at iteration 50 : 0.3099364638328552
Loss at iteration 100 : 1.0763014554977417
Loss at iteration 150 : 0.5889779329299927
Loss at iteration 200 : 0.46284615993499756
Loss at iteration 250 : 0.7252668142318726
Loss at iteration 300 : 0.4831259548664093
Loss at iteration 350 : 0.7431555986404419
Loss at iteration 400 : 0.5386168956756592
Mean training loss eporch  772 :  0.6007639630426206
Loss at iteration 50 : 0.7727539539337158
Loss at iteration 100 : 1.285218358039856
Loss at iteration 150 : 0.29589152336120605
Loss at iteration 200 : 0.8744370341300964
Loss at iteration 250 : 1.1868175268173218
Loss at iteration 300 : 1.0876836776733398
Loss at iteration 350 : 0.31643593311309814
Loss at iteration 400 : 0.31862032413482666
Mean training loss eporch  773 :  0.6043847663653805
Loss at iteration 50 : 0.5598859786987305
Loss at iteration 100 : 0.6803780198097229
Loss at iteration 150 : 0.5201725959777832
Loss at iteration 200 : 0.7320261597633362
Loss at iteration 250 : 0.5940460562705994
Loss at iteration 300 : 0.9173169732093811
Loss at iteration 350 : 0.3532828092575073
Loss at iteration 400 : 0.9393182396888733
Mean training loss eporch  774 :  0.5981873250127908
Loss at iteration 50 : 0.7122005820274353
Loss at iteration 100 : 0.6559306979179382
Loss at iteration 150 : 0.566940188407898
Loss at iteration 200 : 0.6412999629974365
Loss at iteration 250 : 0.48717963695526123
Loss at iteration 300 : 0.5825014114379883
Loss at iteration 350 : 0.48587310314178467
Loss at iteration 400 : 1.0707859992980957
Mean training loss eporch  775 :  0.5990744866597812
Loss at iteration 50 : 0.5265604257583618
Loss at iteration 100 : 0.3418729603290558
Loss at iteration 150 : 0.8108791708946228
Loss at iteration 200 : 1.0285001993179321
Loss at iteration 250 : 0.726335883140564
Loss at iteration 300 : 0.46778425574302673
Loss at iteration 350 : 1.095650553703308
Loss at iteration 400 : 0.2834874391555786
Mean training loss eporch  776 :  0.5985361054339217
Loss at iteration 50 : 1.30592942237854
Loss at iteration 100 : 0.4011673927307129
Loss at iteration 150 : 0.9270198941230774
Loss at iteration 200 : 0.7211068272590637
Loss at iteration 250 : 0.29250994324684143
Loss at iteration 300 : 0.5221068859100342
Loss at iteration 350 : 0.29095005989074707
Loss at iteration 400 : 0.6190739870071411
Mean training loss eporch  777 :  0.5983636073200157
Loss at iteration 50 : 0.38472670316696167
Loss at iteration 100 : 0.3166336417198181
Loss at iteration 150 : 0.2860103249549866
Loss at iteration 200 : 0.44794225692749023
Loss at iteration 250 : 0.7533177733421326
Loss at iteration 300 : 0.7779492139816284
Loss at iteration 350 : 0.3867933452129364
Loss at iteration 400 : 0.8002424240112305
Mean training loss eporch  778 :  0.5986204002336536
Loss at iteration 50 : 0.5187304019927979
Loss at iteration 100 : 0.3401259779930115
Loss at iteration 150 : 0.3427804112434387
Loss at iteration 200 : 0.5782366991043091
Loss at iteration 250 : 0.5022786855697632
Loss at iteration 300 : 0.6450052261352539
Loss at iteration 350 : 0.7371877431869507
Loss at iteration 400 : 0.2682476043701172
Mean training loss eporch  779 :  0.5985687794372639
Loss at iteration 50 : 0.7340744733810425
Loss at iteration 100 : 0.561968982219696
Loss at iteration 150 : 1.0193227529525757
Loss at iteration 200 : 0.8725681900978088
Loss at iteration 250 : 0.7667871713638306
Loss at iteration 300 : 0.702813446521759
Loss at iteration 350 : 0.3584524393081665
Loss at iteration 400 : 0.47096896171569824
Mean training loss eporch  780 :  0.5990057275901995
Loss at iteration 50 : 0.5072808265686035
Loss at iteration 100 : 0.30754390358924866
Loss at iteration 150 : 0.7401981949806213
Loss at iteration 200 : 0.717561662197113
Loss at iteration 250 : 0.5446223020553589
Loss at iteration 300 : 0.4369218945503235
Loss at iteration 350 : 0.9587937593460083
Loss at iteration 400 : 0.25539156794548035
Mean training loss eporch  781 :  0.5978176161780485
Loss at iteration 50 : 0.2807999849319458
Loss at iteration 100 : 0.6216906309127808
Loss at iteration 150 : 0.5282934904098511
Loss at iteration 200 : 0.8877225518226624
Loss at iteration 250 : 0.36856594681739807
Loss at iteration 300 : 0.3344067335128784
Loss at iteration 350 : 0.8333263397216797
Loss at iteration 400 : 1.0419243574142456
Mean training loss eporch  782 :  0.598283366404574
Loss at iteration 50 : 0.6686739325523376
Loss at iteration 100 : 0.9279575347900391
Loss at iteration 150 : 0.4533386826515198
Loss at iteration 200 : 0.7456606030464172
Loss at iteration 250 : 0.8340365290641785
Loss at iteration 300 : 0.46364346146583557
Loss at iteration 350 : 0.5931807160377502
Loss at iteration 400 : 0.9061968326568604
Mean training loss eporch  783 :  0.5995866226642121
Loss at iteration 50 : 0.40128955245018005
Loss at iteration 100 : 0.38251787424087524
Loss at iteration 150 : 0.9392727613449097
Loss at iteration 200 : 0.30906346440315247
Loss at iteration 250 : 0.9132563471794128
Loss at iteration 300 : 0.8456576466560364
Loss at iteration 350 : 0.6991723775863647
Loss at iteration 400 : 0.2866300344467163
Mean training loss eporch  784 :  0.598123478181159
Loss at iteration 50 : 1.125898838043213
Loss at iteration 100 : 1.3067212104797363
Loss at iteration 150 : 0.639609694480896
Loss at iteration 200 : 0.9877838492393494
Loss at iteration 250 : 0.5225732326507568
Loss at iteration 300 : 0.3520883321762085
Loss at iteration 350 : 0.6297422051429749
Loss at iteration 400 : 0.47581443190574646
Mean training loss eporch  785 :  0.597814474872944
Loss at iteration 50 : 0.28735262155532837
Loss at iteration 100 : 0.6047921180725098
Loss at iteration 150 : 0.28090983629226685
Loss at iteration 200 : 0.754462718963623
Loss at iteration 250 : 0.3920930027961731
Loss at iteration 300 : 0.7715619802474976
Loss at iteration 350 : 0.920041561126709
Loss at iteration 400 : 0.920903205871582
Mean training loss eporch  786 :  0.5994442924523032
Loss at iteration 50 : 0.431024968624115
Loss at iteration 100 : 0.3591044843196869
Loss at iteration 150 : 0.6779086589813232
Loss at iteration 200 : 0.4114234149456024
Loss at iteration 250 : 0.44421273469924927
Loss at iteration 300 : 0.6810629367828369
Loss at iteration 350 : 0.24640439450740814
Loss at iteration 400 : 0.5484737753868103
Mean training loss eporch  787 :  0.5980032829759901
Loss at iteration 50 : 0.8075881600379944
Loss at iteration 100 : 0.29530906677246094
Loss at iteration 150 : 0.26012617349624634
Loss at iteration 200 : 0.5532275438308716
Loss at iteration 250 : 0.7321969270706177
Loss at iteration 300 : 0.7977695465087891
Loss at iteration 350 : 0.5437562465667725
Loss at iteration 400 : 0.4521235227584839
Mean training loss eporch  788 :  0.5988135533950254
Loss at iteration 50 : 0.26697492599487305
Loss at iteration 100 : 0.896346926689148
Loss at iteration 150 : 0.9226902723312378
Loss at iteration 200 : 0.7179099321365356
Loss at iteration 250 : 1.0157768726348877
Loss at iteration 300 : 0.7682053446769714
Loss at iteration 350 : 0.7815268039703369
Loss at iteration 400 : 0.6652998328208923
Mean training loss eporch  789 :  0.598301765162314
Loss at iteration 50 : 0.7720650434494019
Loss at iteration 100 : 0.9504489898681641
Loss at iteration 150 : 0.3354088366031647
Loss at iteration 200 : 0.70304274559021
Loss at iteration 250 : 0.4363607168197632
Loss at iteration 300 : 1.2611186504364014
Loss at iteration 350 : 0.6566606760025024
Loss at iteration 400 : 0.4016377329826355
Mean training loss eporch  790 :  0.5984672331689719
Loss at iteration 50 : 0.6965969800949097
Loss at iteration 100 : 0.4281810522079468
Loss at iteration 150 : 0.6662845015525818
Loss at iteration 200 : 0.9759812355041504
Loss at iteration 250 : 0.42127421498298645
Loss at iteration 300 : 0.9506127238273621
Loss at iteration 350 : 0.5767749547958374
Loss at iteration 400 : 0.3780696988105774
Mean training loss eporch  791 :  0.5990833474381623
Loss at iteration 50 : 0.5298736691474915
Loss at iteration 100 : 0.5922090411186218
Loss at iteration 150 : 0.5682671666145325
Loss at iteration 200 : 0.48405730724334717
Loss at iteration 250 : 0.43064048886299133
Loss at iteration 300 : 0.33393436670303345
Loss at iteration 350 : 0.8801672458648682
Loss at iteration 400 : 0.5769906640052795
Mean training loss eporch  792 :  0.5986092321474455
Loss at iteration 50 : 0.670346736907959
Loss at iteration 100 : 0.524631679058075
Loss at iteration 150 : 0.2685677409172058
Loss at iteration 200 : 0.5919824838638306
Loss at iteration 250 : 0.887066125869751
Loss at iteration 300 : 0.4463614821434021
Loss at iteration 350 : 0.6989279985427856
Loss at iteration 400 : 0.23476828634738922
Mean training loss eporch  793 :  0.5982418435838724
Loss at iteration 50 : 0.44486939907073975
Loss at iteration 100 : 0.6582492589950562
Loss at iteration 150 : 0.4887997508049011
Loss at iteration 200 : 0.7905350923538208
Loss at iteration 250 : 1.036858320236206
Loss at iteration 300 : 0.7743349075317383
Loss at iteration 350 : 0.3260379433631897
Loss at iteration 400 : 0.35530635714530945
Mean training loss eporch  794 :  0.5978029021500472
Loss at iteration 50 : 0.4480803608894348
Loss at iteration 100 : 0.4957798719406128
Loss at iteration 150 : 0.33000168204307556
Loss at iteration 200 : 0.38340848684310913
Loss at iteration 250 : 0.560343325138092
Loss at iteration 300 : 0.7410231828689575
Loss at iteration 350 : 0.2185860276222229
Loss at iteration 400 : 0.9253366589546204
Mean training loss eporch  795 :  0.6001318810766588
Loss at iteration 50 : 0.48637109994888306
Loss at iteration 100 : 1.0131779909133911
Loss at iteration 150 : 0.8437035083770752
Loss at iteration 200 : 0.77102130651474
Loss at iteration 250 : 0.46748971939086914
Loss at iteration 300 : 0.7236239910125732
Loss at iteration 350 : 0.8969542980194092
Loss at iteration 400 : 0.46190494298934937
Mean training loss eporch  796 :  0.5982932286679478
Loss at iteration 50 : 0.7009149789810181
Loss at iteration 100 : 0.6601822376251221
Loss at iteration 150 : 0.49082791805267334
Loss at iteration 200 : 0.44198888540267944
Loss at iteration 250 : 0.38985106348991394
Loss at iteration 300 : 0.32481929659843445
Loss at iteration 350 : 1.0311822891235352
Loss at iteration 400 : 0.25374695658683777
Mean training loss eporch  797 :  0.599195404370804
Loss at iteration 50 : 0.636018693447113
Loss at iteration 100 : 0.92624831199646
Loss at iteration 150 : 0.6197813153266907
Loss at iteration 200 : 0.5716179609298706
Loss at iteration 250 : 0.8282226920127869
Loss at iteration 300 : 0.6087417602539062
Loss at iteration 350 : 0.5378680229187012
Loss at iteration 400 : 0.5857366919517517
Mean training loss eporch  798 :  0.5992454033461921
Loss at iteration 50 : 0.5845988988876343
Loss at iteration 100 : 1.0738985538482666
Loss at iteration 150 : 0.6812698245048523
Loss at iteration 200 : 0.5356020927429199
Loss at iteration 250 : 0.5047324895858765
Loss at iteration 300 : 1.4818155765533447
Loss at iteration 350 : 0.4934943914413452
Loss at iteration 400 : 0.6198495030403137
Mean training loss eporch  799 :  0.5986195179245397
Loss at iteration 50 : 0.2132309079170227
Loss at iteration 100 : 0.7087580561637878
Loss at iteration 150 : 0.7061850428581238
Loss at iteration 200 : 0.7073484659194946
Loss at iteration 250 : 0.615150511264801
Loss at iteration 300 : 0.6221301555633545
Loss at iteration 350 : 0.41117486357688904
Loss at iteration 400 : 0.2991602420806885
Mean training loss eporch  800 :  0.5986421752069563
Loss at iteration 50 : 0.3596067428588867
Loss at iteration 100 : 0.9534881114959717
Loss at iteration 150 : 0.30865246057510376
Loss at iteration 200 : 0.5303082466125488
Loss at iteration 250 : 0.607079803943634
Loss at iteration 300 : 0.5016593337059021
Loss at iteration 350 : 0.6446787118911743
Loss at iteration 400 : 0.17949123680591583
Mean training loss eporch  801 :  0.5982634756931275
Loss at iteration 50 : 0.7232675552368164
Loss at iteration 100 : 0.43920713663101196
Loss at iteration 150 : 1.135658860206604
Loss at iteration 200 : 0.629407525062561
Loss at iteration 250 : 0.766653299331665
Loss at iteration 300 : 0.2766370177268982
Loss at iteration 350 : 0.30489465594291687
Loss at iteration 400 : 0.4095768928527832
Mean training loss eporch  802 :  0.5984429155096345
Loss at iteration 50 : 0.4488070011138916
Loss at iteration 100 : 0.467454195022583
Loss at iteration 150 : 0.5878559350967407
Loss at iteration 200 : 1.012034296989441
Loss at iteration 250 : 0.7146351337432861
Loss at iteration 300 : 0.6270332932472229
Loss at iteration 350 : 0.34128743410110474
Loss at iteration 400 : 0.6269641518592834
Mean training loss eporch  803 :  0.5976775503245437
Loss at iteration 50 : 0.438804566860199
Loss at iteration 100 : 0.5327041149139404
Loss at iteration 150 : 0.4156661629676819
Loss at iteration 200 : 0.499585896730423
Loss at iteration 250 : 0.23952937126159668
Loss at iteration 300 : 0.6128949522972107
Loss at iteration 350 : 0.2871105372905731
Loss at iteration 400 : 0.7555179595947266
Mean training loss eporch  804 :  0.598112069404446
Loss at iteration 50 : 0.5751734375953674
Loss at iteration 100 : 0.45100322365760803
Loss at iteration 150 : 0.619498610496521
Loss at iteration 200 : 0.46647804975509644
Loss at iteration 250 : 1.1413137912750244
Loss at iteration 300 : 1.102313756942749
Loss at iteration 350 : 0.5946537256240845
Loss at iteration 400 : 0.5301801562309265
Mean training loss eporch  805 :  0.5980630193590584
Loss at iteration 50 : 0.6373810768127441
Loss at iteration 100 : 0.6576221585273743
Loss at iteration 150 : 0.6300815343856812
Loss at iteration 200 : 0.6125838756561279
Loss at iteration 250 : 0.6821898221969604
Loss at iteration 300 : 0.7862453460693359
Loss at iteration 350 : 0.9232627749443054
Loss at iteration 400 : 0.6320151090621948
Mean training loss eporch  806 :  0.598211607290223
Loss at iteration 50 : 0.32556524872779846
Loss at iteration 100 : 0.48554688692092896
Loss at iteration 150 : 0.5343090295791626
Loss at iteration 200 : 0.31544801592826843
Loss at iteration 250 : 0.5586932897567749
Loss at iteration 300 : 0.7540323138237
Loss at iteration 350 : 0.25616443157196045
Loss at iteration 400 : 0.37293264269828796
Mean training loss eporch  807 :  0.5986089733817652
Loss at iteration 50 : 0.5379852056503296
Loss at iteration 100 : 0.458159863948822
Loss at iteration 150 : 0.39040741324424744
Loss at iteration 200 : 0.36548030376434326
Loss at iteration 250 : 1.393694281578064
Loss at iteration 300 : 0.5278957486152649
Loss at iteration 350 : 0.6380113363265991
Loss at iteration 400 : 0.4739917516708374
Mean training loss eporch  808 :  0.5977596515630927
Loss at iteration 50 : 0.6285886764526367
Loss at iteration 100 : 0.9701196551322937
Loss at iteration 150 : 0.5879608988761902
Loss at iteration 200 : 0.3063329756259918
Loss at iteration 250 : 0.5494756102561951
Loss at iteration 300 : 0.334689736366272
Loss at iteration 350 : 0.34421098232269287
Loss at iteration 400 : 1.119664192199707
Mean training loss eporch  809 :  0.5975730198314372
Loss at iteration 50 : 0.44805610179901123
Loss at iteration 100 : 0.44261637330055237
Loss at iteration 150 : 0.7435793876647949
Loss at iteration 200 : 0.9306551814079285
Loss at iteration 250 : 0.4195885956287384
Loss at iteration 300 : 0.27516940236091614
Loss at iteration 350 : 0.5241065621376038
Loss at iteration 400 : 0.30950281023979187
Mean training loss eporch  810 :  0.598343597466101
Loss at iteration 50 : 0.5299063324928284
Loss at iteration 100 : 0.382347047328949
Loss at iteration 150 : 0.7379403114318848
Loss at iteration 200 : 0.6386556029319763
Loss at iteration 250 : 0.8115341663360596
Loss at iteration 300 : 0.38964715600013733
Loss at iteration 350 : 0.4583911895751953
Loss at iteration 400 : 0.3269006013870239
Mean training loss eporch  811 :  0.5980432207942543
Loss at iteration 50 : 0.3658270537853241
Loss at iteration 100 : 0.7600231170654297
Loss at iteration 150 : 0.696374773979187
Loss at iteration 200 : 0.48552119731903076
Loss at iteration 250 : 0.5602874159812927
Loss at iteration 300 : 0.4104416072368622
Loss at iteration 350 : 0.39652547240257263
Loss at iteration 400 : 0.7915796637535095
Mean training loss eporch  812 :  0.5981330821800125
Loss at iteration 50 : 0.27620869874954224
Loss at iteration 100 : 0.3736717700958252
Loss at iteration 150 : 0.7397845983505249
Loss at iteration 200 : 0.7556738257408142
Loss at iteration 250 : 0.9314830303192139
Loss at iteration 300 : 0.38427767157554626
Loss at iteration 350 : 0.42102980613708496
Loss at iteration 400 : 0.43166297674179077
Mean training loss eporch  813 :  0.5980407691188991
Loss at iteration 50 : 0.3724713921546936
Loss at iteration 100 : 1.0547471046447754
Loss at iteration 150 : 0.8388258218765259
Loss at iteration 200 : 0.49643805623054504
Loss at iteration 250 : 0.38824039697647095
Loss at iteration 300 : 0.33145827054977417
Loss at iteration 350 : 0.25224679708480835
Loss at iteration 400 : 0.26700955629348755
Mean training loss eporch  814 :  0.5981987933607379
Loss at iteration 50 : 0.29365062713623047
Loss at iteration 100 : 0.6213599443435669
Loss at iteration 150 : 0.6393328309059143
Loss at iteration 200 : 0.6874932050704956
Loss at iteration 250 : 0.7037950754165649
Loss at iteration 300 : 0.4212571680545807
Loss at iteration 350 : 0.9493985772132874
Loss at iteration 400 : 0.3194665014743805
Mean training loss eporch  815 :  0.5983380243649932
Loss at iteration 50 : 0.3016144037246704
Loss at iteration 100 : 0.9400894641876221
Loss at iteration 150 : 0.533722460269928
Loss at iteration 200 : 0.5802305936813354
Loss at iteration 250 : 0.5166059136390686
Loss at iteration 300 : 0.3272475004196167
Loss at iteration 350 : 0.4529527425765991
Loss at iteration 400 : 0.5531477928161621
Mean training loss eporch  816 :  0.6001295306690605
Loss at iteration 50 : 0.5920237302780151
Loss at iteration 100 : 0.962031364440918
Loss at iteration 150 : 0.21844007074832916
Loss at iteration 200 : 0.5512762665748596
Loss at iteration 250 : 0.4221581816673279
Loss at iteration 300 : 0.7153416872024536
Loss at iteration 350 : 0.4453738033771515
Loss at iteration 400 : 0.6511171460151672
Mean training loss eporch  817 :  0.5979330562422628
Loss at iteration 50 : 0.3653724789619446
Loss at iteration 100 : 0.5390589237213135
Loss at iteration 150 : 0.5539876818656921
Loss at iteration 200 : 0.3249485492706299
Loss at iteration 250 : 0.3919341564178467
Loss at iteration 300 : 0.7900651693344116
Loss at iteration 350 : 0.47988244891166687
Loss at iteration 400 : 1.2552504539489746
Mean training loss eporch  818 :  0.5988480917140507
Loss at iteration 50 : 0.521361231803894
Loss at iteration 100 : 1.024416208267212
Loss at iteration 150 : 0.6259249448776245
Loss at iteration 200 : 1.1312562227249146
Loss at iteration 250 : 0.36906230449676514
Loss at iteration 300 : 0.4024483561515808
Loss at iteration 350 : 0.7201249003410339
Loss at iteration 400 : 0.3316574692726135
Mean training loss eporch  819 :  0.598283671510861
Loss at iteration 50 : 0.7909377217292786
Loss at iteration 100 : 0.4917483329772949
Loss at iteration 150 : 0.7880092263221741
Loss at iteration 200 : 0.654513418674469
Loss at iteration 250 : 0.7557806968688965
Loss at iteration 300 : 0.7898919582366943
Loss at iteration 350 : 0.6535530686378479
Loss at iteration 400 : 0.5303406715393066
Mean training loss eporch  820 :  0.5995404044074328
Loss at iteration 50 : 0.5039619207382202
Loss at iteration 100 : 0.28321555256843567
Loss at iteration 150 : 1.3751133680343628
Loss at iteration 200 : 0.5018439292907715
Loss at iteration 250 : 0.6262769103050232
Loss at iteration 300 : 0.38824039697647095
Loss at iteration 350 : 0.759194016456604
Loss at iteration 400 : 0.6455376148223877
Mean training loss eporch  821 :  0.5978211833690315
Loss at iteration 50 : 0.9165345430374146
Loss at iteration 100 : 0.5834167003631592
Loss at iteration 150 : 0.4895786941051483
Loss at iteration 200 : 0.9139626622200012
Loss at iteration 250 : 1.2633352279663086
Loss at iteration 300 : 0.36079710721969604
Loss at iteration 350 : 0.24410003423690796
Loss at iteration 400 : 0.7323280572891235
Mean training loss eporch  822 :  0.5980319091757851
Loss at iteration 50 : 0.9410516023635864
Loss at iteration 100 : 1.1440162658691406
Loss at iteration 150 : 0.2821432054042816
Loss at iteration 200 : 0.511552631855011
Loss at iteration 250 : 0.5759936571121216
Loss at iteration 300 : 0.8183064460754395
Loss at iteration 350 : 0.34066033363342285
Loss at iteration 400 : 0.34320831298828125
Mean training loss eporch  823 :  0.5977429255776341
Loss at iteration 50 : 0.8649065494537354
Loss at iteration 100 : 0.5515932440757751
Loss at iteration 150 : 0.8074023723602295
Loss at iteration 200 : 1.0004017353057861
Loss at iteration 250 : 0.9136545062065125
Loss at iteration 300 : 0.25836917757987976
Loss at iteration 350 : 1.0438904762268066
Loss at iteration 400 : 0.4130851924419403
Mean training loss eporch  824 :  0.5979500416282046
Loss at iteration 50 : 0.5305976867675781
Loss at iteration 100 : 0.46853402256965637
Loss at iteration 150 : 0.3433646857738495
Loss at iteration 200 : 0.48891007900238037
Loss at iteration 250 : 0.30568575859069824
Loss at iteration 300 : 0.5576937198638916
Loss at iteration 350 : 0.6612197756767273
Loss at iteration 400 : 0.7999119758605957
Mean training loss eporch  825 :  0.6000346539400083
Loss at iteration 50 : 0.4104584753513336
Loss at iteration 100 : 0.8507603406906128
Loss at iteration 150 : 0.3236149549484253
Loss at iteration 200 : 0.8764715194702148
Loss at iteration 250 : 0.6358118057250977
Loss at iteration 300 : 0.3329964280128479
Loss at iteration 350 : 0.9450150728225708
Loss at iteration 400 : 1.0433588027954102
Mean training loss eporch  826 :  0.5999495724191045
Loss at iteration 50 : 0.5786125659942627
Loss at iteration 100 : 0.8719189167022705
Loss at iteration 150 : 1.1454545259475708
Loss at iteration 200 : 1.114417552947998
Loss at iteration 250 : 1.2140824794769287
Loss at iteration 300 : 0.3525697886943817
Loss at iteration 350 : 0.8389254808425903
Loss at iteration 400 : 0.5653047561645508
Mean training loss eporch  827 :  0.5979917324377817
Loss at iteration 50 : 0.6947965621948242
Loss at iteration 100 : 0.45539426803588867
Loss at iteration 150 : 0.7536196708679199
Loss at iteration 200 : 0.8365169763565063
Loss at iteration 250 : 0.35034793615341187
Loss at iteration 300 : 0.7446486949920654
Loss at iteration 350 : 0.23847633600234985
Loss at iteration 400 : 0.7350897789001465
Mean training loss eporch  828 :  0.5988501864965721
Loss at iteration 50 : 0.4399254024028778
Loss at iteration 100 : 0.23731070756912231
Loss at iteration 150 : 0.7616689205169678
Loss at iteration 200 : 0.8691465258598328
Loss at iteration 250 : 0.892916738986969
Loss at iteration 300 : 0.38112080097198486
Loss at iteration 350 : 0.6411760449409485
Loss at iteration 400 : 0.46248841285705566
Mean training loss eporch  829 :  0.5978136424793791
Loss at iteration 50 : 0.4849792718887329
Loss at iteration 100 : 0.3696631193161011
Loss at iteration 150 : 0.787946343421936
Loss at iteration 200 : 0.6287840604782104
Loss at iteration 250 : 0.5313093662261963
Loss at iteration 300 : 0.689326822757721
Loss at iteration 350 : 0.3430885374546051
Loss at iteration 400 : 0.38504987955093384
Mean training loss eporch  830 :  0.5995539917940517
Loss at iteration 50 : 0.6503463983535767
Loss at iteration 100 : 0.4572952687740326
Loss at iteration 150 : 0.48134374618530273
Loss at iteration 200 : 0.4987531900405884
Loss at iteration 250 : 0.6395642757415771
Loss at iteration 300 : 1.0813164710998535
Loss at iteration 350 : 0.543448269367218
Loss at iteration 400 : 0.674405574798584
Mean training loss eporch  831 :  0.5981248869088733
Loss at iteration 50 : 0.5501839518547058
Loss at iteration 100 : 0.42631298303604126
Loss at iteration 150 : 0.8667364716529846
Loss at iteration 200 : 0.3519892692565918
Loss at iteration 250 : 0.2388702630996704
Loss at iteration 300 : 0.5099245309829712
Loss at iteration 350 : 0.5456578731536865
Loss at iteration 400 : 0.49248117208480835
Mean training loss eporch  832 :  0.6009201600880366
Loss at iteration 50 : 0.3037473261356354
Loss at iteration 100 : 0.39971688389778137
Loss at iteration 150 : 0.5790682435035706
Loss at iteration 200 : 0.4139982759952545
Loss at iteration 250 : 0.7081230878829956
Loss at iteration 300 : 0.43424779176712036
Loss at iteration 350 : 0.46943509578704834
Loss at iteration 400 : 0.5121443271636963
Mean training loss eporch  833 :  0.5977969463630642
Loss at iteration 50 : 0.7385666370391846
Loss at iteration 100 : 0.5417340993881226
Loss at iteration 150 : 0.5668678879737854
Loss at iteration 200 : 0.5455169677734375
Loss at iteration 250 : 0.5950298309326172
Loss at iteration 300 : 0.4766849875450134
Loss at iteration 350 : 0.5683463215827942
Loss at iteration 400 : 0.6601809859275818
Mean training loss eporch  834 :  0.5994633071692536
Loss at iteration 50 : 0.5102901458740234
Loss at iteration 100 : 0.36664026975631714
Loss at iteration 150 : 0.4813523292541504
Loss at iteration 200 : 0.40075284242630005
Loss at iteration 250 : 0.39086270332336426
Loss at iteration 300 : 1.009092092514038
Loss at iteration 350 : 0.7116957902908325
Loss at iteration 400 : 0.49464496970176697
Mean training loss eporch  835 :  0.598326657685731
Loss at iteration 50 : 0.905121386051178
Loss at iteration 100 : 0.7362080216407776
Loss at iteration 150 : 0.3197748064994812
Loss at iteration 200 : 0.6141339540481567
Loss at iteration 250 : 0.20701929926872253
Loss at iteration 300 : 0.34195876121520996
Loss at iteration 350 : 0.5663614869117737
Loss at iteration 400 : 0.3692196011543274
Mean training loss eporch  836 :  0.5976935079981126
Loss at iteration 50 : 0.5884297490119934
Loss at iteration 100 : 0.554189920425415
Loss at iteration 150 : 0.4849533140659332
Loss at iteration 200 : 0.7999488115310669
Loss at iteration 250 : 0.311577707529068
Loss at iteration 300 : 0.49289989471435547
Loss at iteration 350 : 0.6336372494697571
Loss at iteration 400 : 0.65901118516922
Mean training loss eporch  837 :  0.599094684515566
Loss at iteration 50 : 0.584839940071106
Loss at iteration 100 : 0.39538106322288513
Loss at iteration 150 : 1.3199583292007446
Loss at iteration 200 : 0.8850913643836975
Loss at iteration 250 : 0.9204690456390381
Loss at iteration 300 : 0.8409451246261597
Loss at iteration 350 : 1.0947890281677246
Loss at iteration 400 : 1.0262678861618042
Mean training loss eporch  838 :  0.5977613221636802
Loss at iteration 50 : 0.8375293016433716
Loss at iteration 100 : 0.36770132184028625
Loss at iteration 150 : 0.8158003091812134
Loss at iteration 200 : 1.054030418395996
Loss at iteration 250 : 1.1506608724594116
Loss at iteration 300 : 0.3297763466835022
Loss at iteration 350 : 0.70124351978302
Loss at iteration 400 : 0.6253215074539185
Mean training loss eporch  839 :  0.599030149490844
Loss at iteration 50 : 0.5894982814788818
Loss at iteration 100 : 0.8786138296127319
Loss at iteration 150 : 0.4711184501647949
Loss at iteration 200 : 0.42265430092811584
Loss at iteration 250 : 0.3710080087184906
Loss at iteration 300 : 1.0255169868469238
Loss at iteration 350 : 0.5800175070762634
Loss at iteration 400 : 0.3767508864402771
Mean training loss eporch  840 :  0.5988886493003421
Loss at iteration 50 : 0.4133926033973694
Loss at iteration 100 : 0.46427708864212036
Loss at iteration 150 : 0.35682111978530884
Loss at iteration 200 : 0.6989022493362427
Loss at iteration 250 : 0.6079866290092468
Loss at iteration 300 : 0.31926578283309937
Loss at iteration 350 : 0.5232591032981873
Loss at iteration 400 : 0.8129738569259644
Mean training loss eporch  841 :  0.5977452719425407
Loss at iteration 50 : 0.48822593688964844
Loss at iteration 100 : 0.6283068060874939
Loss at iteration 150 : 0.3805293142795563
Loss at iteration 200 : 0.30382147431373596
Loss at iteration 250 : 0.2940044403076172
Loss at iteration 300 : 0.6504651308059692
Loss at iteration 350 : 0.5461484789848328
Loss at iteration 400 : 1.140242576599121
Mean training loss eporch  842 :  0.5979250634732268
Loss at iteration 50 : 0.5799461007118225
Loss at iteration 100 : 0.929121732711792
Loss at iteration 150 : 0.5113404393196106
Loss at iteration 200 : 0.6809628009796143
Loss at iteration 250 : 0.6352519989013672
Loss at iteration 300 : 0.6994495391845703
Loss at iteration 350 : 0.7814672589302063
Loss at iteration 400 : 0.37245362997055054
Mean training loss eporch  843 :  0.5976568901819499
Loss at iteration 50 : 0.7113440036773682
Loss at iteration 100 : 0.7946066856384277
Loss at iteration 150 : 0.3216017484664917
Loss at iteration 200 : 0.3946463167667389
Loss at iteration 250 : 0.46148037910461426
Loss at iteration 300 : 0.5017908215522766
Loss at iteration 350 : 0.4098053574562073
Loss at iteration 400 : 0.40496373176574707
Mean training loss eporch  844 :  0.603626279218849
Loss at iteration 50 : 0.3347688317298889
Loss at iteration 100 : 0.28345787525177
Loss at iteration 150 : 0.3580247163772583
Loss at iteration 200 : 0.36891812086105347
Loss at iteration 250 : 0.8736076354980469
Loss at iteration 300 : 0.6600109934806824
Loss at iteration 350 : 0.8570223450660706
Loss at iteration 400 : 0.5823755860328674
Mean training loss eporch  845 :  0.5983501245756321
Loss at iteration 50 : 1.3535441160202026
Loss at iteration 100 : 0.7192847728729248
Loss at iteration 150 : 0.5339962244033813
Loss at iteration 200 : 0.8778489828109741
Loss at iteration 250 : 0.8387941122055054
Loss at iteration 300 : 0.358634352684021
Loss at iteration 350 : 1.352813959121704
Loss at iteration 400 : 0.5193464756011963
Mean training loss eporch  846 :  0.5974158455571786
Loss at iteration 50 : 0.29175812005996704
Loss at iteration 100 : 0.8018403649330139
Loss at iteration 150 : 1.2764581441879272
Loss at iteration 200 : 0.5667111873626709
Loss at iteration 250 : 1.0114381313323975
Loss at iteration 300 : 0.9020770788192749
Loss at iteration 350 : 0.5385779142379761
Loss at iteration 400 : 0.5280032753944397
Mean training loss eporch  847 :  0.5975974055149096
Loss at iteration 50 : 0.5412322282791138
Loss at iteration 100 : 0.5300918221473694
Loss at iteration 150 : 0.5114297866821289
Loss at iteration 200 : 0.3081515431404114
Loss at iteration 250 : 0.28936976194381714
Loss at iteration 300 : 0.48849236965179443
Loss at iteration 350 : 0.7956218123435974
Loss at iteration 400 : 0.6968764662742615
Mean training loss eporch  848 :  0.6018592559903726
Loss at iteration 50 : 0.40444087982177734
Loss at iteration 100 : 0.5219755172729492
Loss at iteration 150 : 0.5392981171607971
Loss at iteration 200 : 0.5120748281478882
Loss at iteration 250 : 0.2766144275665283
Loss at iteration 300 : 0.36333978176116943
Loss at iteration 350 : 0.7917914390563965
Loss at iteration 400 : 0.5626113414764404
Mean training loss eporch  849 :  0.6036053728896941
Loss at iteration 50 : 0.5156846046447754
Loss at iteration 100 : 0.3783421814441681
Loss at iteration 150 : 0.9715540409088135
Loss at iteration 200 : 0.272378146648407
Loss at iteration 250 : 0.3276206851005554
Loss at iteration 300 : 0.6589291095733643
Loss at iteration 350 : 0.2802537679672241
Loss at iteration 400 : 0.4479489326477051
Mean training loss eporch  850 :  0.5978352716484947
Loss at iteration 50 : 0.45596349239349365
Loss at iteration 100 : 0.2722097933292389
Loss at iteration 150 : 0.4730469882488251
Loss at iteration 200 : 0.5433322191238403
Loss at iteration 250 : 0.5309072732925415
Loss at iteration 300 : 0.5413224697113037
Loss at iteration 350 : 0.7017636895179749
Loss at iteration 400 : 0.781330943107605
Mean training loss eporch  851 :  0.5980481827138785
Loss at iteration 50 : 1.1283836364746094
Loss at iteration 100 : 0.30564457178115845
Loss at iteration 150 : 1.4206914901733398
Loss at iteration 200 : 0.533961296081543
Loss at iteration 250 : 0.2650967538356781
Loss at iteration 300 : 1.0484075546264648
Loss at iteration 350 : 0.7504006624221802
Loss at iteration 400 : 0.3946220874786377
Mean training loss eporch  852 :  0.6001051243043801
Loss at iteration 50 : 0.6541445255279541
Loss at iteration 100 : 0.640397846698761
Loss at iteration 150 : 0.5716285705566406
Loss at iteration 200 : 0.6301237344741821
Loss at iteration 250 : 0.5671910047531128
Loss at iteration 300 : 1.4948608875274658
Loss at iteration 350 : 0.4054911732673645
Loss at iteration 400 : 1.8799149990081787
Mean training loss eporch  853 :  0.5976154042800446
Loss at iteration 50 : 0.28428423404693604
Loss at iteration 100 : 0.7163183093070984
Loss at iteration 150 : 0.5721767544746399
Loss at iteration 200 : 0.7737807631492615
Loss at iteration 250 : 0.8171330690383911
Loss at iteration 300 : 0.31564971804618835
Loss at iteration 350 : 0.7426306009292603
Loss at iteration 400 : 0.21101507544517517
Mean training loss eporch  854 :  0.5974647399422299
Loss at iteration 50 : 0.6729453802108765
Loss at iteration 100 : 0.27720409631729126
Loss at iteration 150 : 0.2925262451171875
Loss at iteration 200 : 0.7164759635925293
Loss at iteration 250 : 0.37126317620277405
Loss at iteration 300 : 0.6385217308998108
Loss at iteration 350 : 0.6533188819885254
Loss at iteration 400 : 0.544074535369873
Mean training loss eporch  855 :  0.5981948093381698
Loss at iteration 50 : 0.40959471464157104
Loss at iteration 100 : 0.5500776767730713
Loss at iteration 150 : 0.36178386211395264
Loss at iteration 200 : 0.34709322452545166
Loss at iteration 250 : 0.33750396966934204
Loss at iteration 300 : 0.7847291231155396
Loss at iteration 350 : 0.3943004310131073
Loss at iteration 400 : 0.2978789508342743
Mean training loss eporch  856 :  0.5977117710583948
Loss at iteration 50 : 0.7853360772132874
Loss at iteration 100 : 0.588249683380127
Loss at iteration 150 : 0.8058433532714844
Loss at iteration 200 : 0.25038763880729675
Loss at iteration 250 : 0.8578413724899292
Loss at iteration 300 : 0.9657119512557983
Loss at iteration 350 : 1.1031478643417358
Loss at iteration 400 : 0.4018707275390625
Mean training loss eporch  857 :  0.5977943615461678
Loss at iteration 50 : 0.419472336769104
Loss at iteration 100 : 0.5553585290908813
Loss at iteration 150 : 0.3540318012237549
Loss at iteration 200 : 0.3135380744934082
Loss at iteration 250 : 0.8028764724731445
Loss at iteration 300 : 0.618383526802063
Loss at iteration 350 : 0.46761956810951233
Loss at iteration 400 : 0.4744488596916199
Mean training loss eporch  858 :  0.5984423236395211
Loss at iteration 50 : 0.42518579959869385
Loss at iteration 100 : 0.9468642473220825
Loss at iteration 150 : 0.6318573951721191
Loss at iteration 200 : 0.5560538172721863
Loss at iteration 250 : 1.417224407196045
Loss at iteration 300 : 0.3204852342605591
Loss at iteration 350 : 0.7172844409942627
Loss at iteration 400 : 0.6261123418807983
Mean training loss eporch  859 :  0.5983313798837597
Loss at iteration 50 : 0.4369381070137024
Loss at iteration 100 : 0.431138813495636
Loss at iteration 150 : 0.718389630317688
Loss at iteration 200 : 0.43758711218833923
Loss at iteration 250 : 0.5304296612739563
Loss at iteration 300 : 0.4350396990776062
Loss at iteration 350 : 0.3070293068885803
Loss at iteration 400 : 0.672855019569397
Mean training loss eporch  860 :  0.5985089445261143
Loss at iteration 50 : 0.550064206123352
Loss at iteration 100 : 0.8796213865280151
Loss at iteration 150 : 0.3415781855583191
Loss at iteration 200 : 0.8411321640014648
Loss at iteration 250 : 0.3961465656757355
Loss at iteration 300 : 0.744300365447998
Loss at iteration 350 : 1.0886774063110352
Loss at iteration 400 : 0.7343579530715942
Mean training loss eporch  861 :  0.5974176524065
Loss at iteration 50 : 0.2995419204235077
Loss at iteration 100 : 0.9674546718597412
Loss at iteration 150 : 1.0869784355163574
Loss at iteration 200 : 0.6214514970779419
Loss at iteration 250 : 0.9564101696014404
Loss at iteration 300 : 0.3958238363265991
Loss at iteration 350 : 0.38263821601867676
Loss at iteration 400 : 1.2214224338531494
Mean training loss eporch  862 :  0.603252823009352
Loss at iteration 50 : 0.7234876751899719
Loss at iteration 100 : 0.30751851201057434
Loss at iteration 150 : 0.5288518667221069
Loss at iteration 200 : 0.5920307636260986
Loss at iteration 250 : 0.36564740538597107
Loss at iteration 300 : 0.41848766803741455
Loss at iteration 350 : 0.8304627537727356
Loss at iteration 400 : 1.1775317192077637
Mean training loss eporch  863 :  0.5990638051666486
Loss at iteration 50 : 0.4017902612686157
Loss at iteration 100 : 0.46155279874801636
Loss at iteration 150 : 0.49899402260780334
Loss at iteration 200 : 0.6490037441253662
Loss at iteration 250 : 0.9281816482543945
Loss at iteration 300 : 0.7730177640914917
Loss at iteration 350 : 0.2545962929725647
Loss at iteration 400 : 0.8294708132743835
Mean training loss eporch  864 :  0.5976314799267081
Loss at iteration 50 : 0.5062039494514465
Loss at iteration 100 : 0.8984836935997009
Loss at iteration 150 : 0.2639150023460388
Loss at iteration 200 : 0.336669385433197
Loss at iteration 250 : 0.3993648886680603
Loss at iteration 300 : 0.33224743604660034
Loss at iteration 350 : 0.40530675649642944
Loss at iteration 400 : 0.569063127040863
Mean training loss eporch  865 :  0.5979387038491767
Loss at iteration 50 : 0.2776029706001282
Loss at iteration 100 : 0.7038409113883972
Loss at iteration 150 : 0.7552087306976318
Loss at iteration 200 : 0.4365381598472595
Loss at iteration 250 : 0.7558897733688354
Loss at iteration 300 : 0.3385992646217346
Loss at iteration 350 : 0.8990737199783325
Loss at iteration 400 : 0.6958492398262024
Mean training loss eporch  866 :  0.5988679800533393
Loss at iteration 50 : 0.5637657642364502
Loss at iteration 100 : 0.6852461099624634
Loss at iteration 150 : 0.6358818411827087
Loss at iteration 200 : 0.3263366222381592
Loss at iteration 250 : 0.45966118574142456
Loss at iteration 300 : 0.5675292015075684
Loss at iteration 350 : 0.27159303426742554
Loss at iteration 400 : 0.3487522304058075
Mean training loss eporch  867 :  0.598602238624887
Loss at iteration 50 : 0.41818299889564514
Loss at iteration 100 : 0.869118332862854
Loss at iteration 150 : 0.4426523447036743
Loss at iteration 200 : 1.7623635530471802
Loss at iteration 250 : 0.3435918092727661
Loss at iteration 300 : 0.5673093199729919
Loss at iteration 350 : 0.44345366954803467
Loss at iteration 400 : 0.3481399416923523
Mean training loss eporch  868 :  0.5978287517890802
Loss at iteration 50 : 0.8680137991905212
Loss at iteration 100 : 0.3324376940727234
Loss at iteration 150 : 0.3597966730594635
Loss at iteration 200 : 0.42100244760513306
Loss at iteration 250 : 0.7815637588500977
Loss at iteration 300 : 0.25900858640670776
Loss at iteration 350 : 0.35920387506484985
Loss at iteration 400 : 0.3735145926475525
Mean training loss eporch  869 :  0.5980401905462346
Loss at iteration 50 : 0.588336706161499
Loss at iteration 100 : 0.35459771752357483
Loss at iteration 150 : 0.6218076944351196
Loss at iteration 200 : 0.6298041343688965
Loss at iteration 250 : 0.6375079154968262
Loss at iteration 300 : 0.4297632575035095
Loss at iteration 350 : 0.8198467493057251
Loss at iteration 400 : 0.7910478115081787
Mean training loss eporch  870 :  0.5987604337623301
Loss at iteration 50 : 0.2825360894203186
Loss at iteration 100 : 0.34903019666671753
Loss at iteration 150 : 0.7529264688491821
Loss at iteration 200 : 0.8116349577903748
Loss at iteration 250 : 0.2544969320297241
Loss at iteration 300 : 0.42819714546203613
Loss at iteration 350 : 0.7476619482040405
Loss at iteration 400 : 0.36795079708099365
Mean training loss eporch  871 :  0.5978561738013152
Loss at iteration 50 : 0.960761547088623
Loss at iteration 100 : 0.732848584651947
Loss at iteration 150 : 0.5927280783653259
Loss at iteration 200 : 0.49605539441108704
Loss at iteration 250 : 0.5069332122802734
Loss at iteration 300 : 0.30314427614212036
Loss at iteration 350 : 0.755814790725708
Loss at iteration 400 : 0.9651997685432434
Mean training loss eporch  872 :  0.5977697975499213
Loss at iteration 50 : 0.6515814065933228
Loss at iteration 100 : 0.6241735219955444
Loss at iteration 150 : 0.7700745463371277
Loss at iteration 200 : 0.2737741768360138
Loss at iteration 250 : 0.2656853497028351
Loss at iteration 300 : 0.5480773448944092
Loss at iteration 350 : 0.26642364263534546
Loss at iteration 400 : 0.36563780903816223
Mean training loss eporch  873 :  0.6021324845841113
Loss at iteration 50 : 0.445665568113327
Loss at iteration 100 : 0.37776854634284973
Loss at iteration 150 : 0.3489100933074951
Loss at iteration 200 : 0.6368939280509949
Loss at iteration 250 : 0.42827916145324707
Loss at iteration 300 : 0.8151568174362183
Loss at iteration 350 : 0.36909204721450806
Loss at iteration 400 : 1.6770579814910889
Mean training loss eporch  874 :  0.5978226068642641
Loss at iteration 50 : 0.5437904000282288
Loss at iteration 100 : 0.48751741647720337
Loss at iteration 150 : 0.5109149217605591
Loss at iteration 200 : 0.9629697799682617
Loss at iteration 250 : 0.6171889901161194
Loss at iteration 300 : 0.7043120861053467
Loss at iteration 350 : 0.36114728450775146
Loss at iteration 400 : 0.27635061740875244
Mean training loss eporch  875 :  0.5993896372649702
Loss at iteration 50 : 0.8578182458877563
Loss at iteration 100 : 0.5674310922622681
Loss at iteration 150 : 0.3787117600440979
Loss at iteration 200 : 0.6560993194580078
Loss at iteration 250 : 0.9231181144714355
Loss at iteration 300 : 0.8488770127296448
Loss at iteration 350 : 0.3492084741592407
Loss at iteration 400 : 0.36426177620887756
Mean training loss eporch  876 :  0.5993610747125117
Loss at iteration 50 : 0.4898284375667572
Loss at iteration 100 : 0.5553710460662842
Loss at iteration 150 : 0.7114604711532593
Loss at iteration 200 : 0.2515914738178253
Loss at iteration 250 : 1.1125495433807373
Loss at iteration 300 : 0.6528373956680298
Loss at iteration 350 : 0.845379114151001
Loss at iteration 400 : 0.5101187825202942
Mean training loss eporch  877 :  0.5980603502804388
Loss at iteration 50 : 0.445445716381073
Loss at iteration 100 : 0.3315281867980957
Loss at iteration 150 : 0.26630449295043945
Loss at iteration 200 : 0.403922975063324
Loss at iteration 250 : 0.4153473377227783
Loss at iteration 300 : 0.5870665907859802
Loss at iteration 350 : 0.34607717394828796
Loss at iteration 400 : 1.0345287322998047
Mean training loss eporch  878 :  0.5978206882896445
Loss at iteration 50 : 0.6340471506118774
Loss at iteration 100 : 1.1332191228866577
Loss at iteration 150 : 0.6750627160072327
Loss at iteration 200 : 0.8451299667358398
Loss at iteration 250 : 0.6243149042129517
Loss at iteration 300 : 0.4733922779560089
Loss at iteration 350 : 0.9238924384117126
Loss at iteration 400 : 0.9169178009033203
Mean training loss eporch  879 :  0.5976971755380588
Loss at iteration 50 : 0.3021751344203949
Loss at iteration 100 : 0.2615867555141449
Loss at iteration 150 : 0.31413209438323975
Loss at iteration 200 : 0.8705736398696899
Loss at iteration 250 : 0.43170350790023804
Loss at iteration 300 : 0.7040137052536011
Loss at iteration 350 : 0.5850716829299927
Loss at iteration 400 : 0.5368719696998596
Mean training loss eporch  880 :  0.5991432574230994
Loss at iteration 50 : 0.5782068967819214
Loss at iteration 100 : 0.7907328605651855
Loss at iteration 150 : 0.6443556547164917
Loss at iteration 200 : 0.8779926300048828
Loss at iteration 250 : 0.49595919251441956
Loss at iteration 300 : 0.5373315215110779
Loss at iteration 350 : 0.6994158029556274
Loss at iteration 400 : 0.3574828505516052
Mean training loss eporch  881 :  0.5975371733508302
Loss at iteration 50 : 0.5865204930305481
Loss at iteration 100 : 0.7312034368515015
Loss at iteration 150 : 0.4203665256500244
Loss at iteration 200 : 1.0275657176971436
Loss at iteration 250 : 0.5332895517349243
Loss at iteration 300 : 0.7265467047691345
Loss at iteration 350 : 0.6716338396072388
Loss at iteration 400 : 0.6520267128944397
Mean training loss eporch  882 :  0.6015773396855513
Loss at iteration 50 : 0.47161126136779785
Loss at iteration 100 : 0.5621410608291626
Loss at iteration 150 : 0.6108291149139404
Loss at iteration 200 : 1.242281198501587
Loss at iteration 250 : 0.3509858250617981
Loss at iteration 300 : 0.8847149610519409
Loss at iteration 350 : 0.2338452786207199
Loss at iteration 400 : 0.3410458266735077
Mean training loss eporch  883 :  0.5978762941109226
Loss at iteration 50 : 0.4501974582672119
Loss at iteration 100 : 0.614997923374176
Loss at iteration 150 : 0.38971370458602905
Loss at iteration 200 : 0.27760082483291626
Loss at iteration 250 : 0.7039505839347839
Loss at iteration 300 : 0.7118516564369202
Loss at iteration 350 : 0.5423834919929504
Loss at iteration 400 : 0.8038589358329773
Mean training loss eporch  884 :  0.5987825980702324
Loss at iteration 50 : 0.6853975057601929
Loss at iteration 100 : 0.8721208572387695
Loss at iteration 150 : 0.43465352058410645
Loss at iteration 200 : 0.3989220857620239
Loss at iteration 250 : 0.6559175848960876
Loss at iteration 300 : 1.2670040130615234
Loss at iteration 350 : 0.3917883634567261
Loss at iteration 400 : 0.29808929562568665
Mean training loss eporch  885 :  0.5994568983669238
Loss at iteration 50 : 0.37157803773880005
Loss at iteration 100 : 0.7754034996032715
Loss at iteration 150 : 0.687973141670227
Loss at iteration 200 : 0.41313469409942627
Loss at iteration 250 : 0.6560631394386292
Loss at iteration 300 : 0.5214674472808838
Loss at iteration 350 : 0.7602857947349548
Loss at iteration 400 : 0.2415170967578888
Mean training loss eporch  886 :  0.5980638556616723
Loss at iteration 50 : 0.8198189735412598
Loss at iteration 100 : 0.4431907534599304
Loss at iteration 150 : 0.9561526775360107
Loss at iteration 200 : 0.8236069679260254
Loss at iteration 250 : 0.4974833130836487
Loss at iteration 300 : 0.5696302652359009
Loss at iteration 350 : 0.8866329193115234
Loss at iteration 400 : 0.4323703646659851
Mean training loss eporch  887 :  0.599442694887452
Loss at iteration 50 : 0.6785659790039062
Loss at iteration 100 : 0.7961502075195312
Loss at iteration 150 : 0.30222809314727783
Loss at iteration 200 : 1.0063316822052002
Loss at iteration 250 : 1.065181016921997
Loss at iteration 300 : 0.5070539116859436
Loss at iteration 350 : 0.3217131495475769
Loss at iteration 400 : 0.26333603262901306
Mean training loss eporch  888 :  0.5971510575357574
Loss at iteration 50 : 0.7106180191040039
Loss at iteration 100 : 0.20398664474487305
Loss at iteration 150 : 0.5924628973007202
Loss at iteration 200 : 0.6657123565673828
Loss at iteration 250 : 0.7578146457672119
Loss at iteration 300 : 0.9854745864868164
Loss at iteration 350 : 1.0460009574890137
Loss at iteration 400 : 0.4155634343624115
Mean training loss eporch  889 :  0.599533628455192
Loss at iteration 50 : 0.6283380389213562
Loss at iteration 100 : 0.4304734766483307
Loss at iteration 150 : 0.8225675821304321
Loss at iteration 200 : 0.8306735157966614
Loss at iteration 250 : 0.4735403060913086
Loss at iteration 300 : 0.28308868408203125
Loss at iteration 350 : 0.45917144417762756
Loss at iteration 400 : 0.6395215392112732
Mean training loss eporch  890 :  0.5989761385949738
Loss at iteration 50 : 0.45458984375
Loss at iteration 100 : 0.3998229503631592
Loss at iteration 150 : 0.9034553170204163
Loss at iteration 200 : 0.6168695092201233
Loss at iteration 250 : 0.4855630397796631
Loss at iteration 300 : 0.7563116550445557
Loss at iteration 350 : 0.25351834297180176
Loss at iteration 400 : 0.6326752305030823
Mean training loss eporch  891 :  0.5974288256379521
Loss at iteration 50 : 1.083970546722412
Loss at iteration 100 : 0.3368864953517914
Loss at iteration 150 : 0.5881531834602356
Loss at iteration 200 : 1.3083994388580322
Loss at iteration 250 : 0.5507433414459229
Loss at iteration 300 : 0.9427033066749573
Loss at iteration 350 : 0.3642767667770386
Loss at iteration 400 : 1.1351628303527832
Mean training loss eporch  892 :  0.5972550220687293
Loss at iteration 50 : 0.6080734729766846
Loss at iteration 100 : 0.4534747004508972
Loss at iteration 150 : 1.0780200958251953
Loss at iteration 200 : 0.3366740942001343
Loss at iteration 250 : 0.5216723680496216
Loss at iteration 300 : 0.5295270681381226
Loss at iteration 350 : 0.5131034851074219
Loss at iteration 400 : 0.9028984308242798
Mean training loss eporch  893 :  0.5973487294074399
Loss at iteration 50 : 0.43024855852127075
Loss at iteration 100 : 0.8150547742843628
Loss at iteration 150 : 0.7573652267456055
Loss at iteration 200 : 0.926374077796936
Loss at iteration 250 : 0.5984072685241699
Loss at iteration 300 : 0.8015595078468323
Loss at iteration 350 : 0.38679370284080505
Loss at iteration 400 : 0.31471630930900574
Mean training loss eporch  894 :  0.59719129805474
Loss at iteration 50 : 0.8801664710044861
Loss at iteration 100 : 0.4960887134075165
Loss at iteration 150 : 0.5247340202331543
Loss at iteration 200 : 0.581750214099884
Loss at iteration 250 : 0.7076700329780579
Loss at iteration 300 : 0.9837251305580139
Loss at iteration 350 : 0.8519262075424194
Loss at iteration 400 : 0.4001106023788452
Mean training loss eporch  895 :  0.598624401730005
Loss at iteration 50 : 0.5165615081787109
Loss at iteration 100 : 0.9162871241569519
Loss at iteration 150 : 0.6308916807174683
Loss at iteration 200 : 0.4347915053367615
Loss at iteration 250 : 0.7860845327377319
Loss at iteration 300 : 0.5464911460876465
Loss at iteration 350 : 0.29808807373046875
Loss at iteration 400 : 0.8876211643218994
Mean training loss eporch  896 :  0.5976286845891465
Loss at iteration 50 : 0.7229412198066711
Loss at iteration 100 : 0.40825510025024414
Loss at iteration 150 : 0.4057731032371521
Loss at iteration 200 : 0.4634319841861725
Loss at iteration 250 : 0.326832115650177
Loss at iteration 300 : 0.29426535964012146
Loss at iteration 350 : 0.5967216491699219
Loss at iteration 400 : 0.3842090368270874
Mean training loss eporch  897 :  0.5981646361906967
Loss at iteration 50 : 0.47420307993888855
Loss at iteration 100 : 0.5047801733016968
Loss at iteration 150 : 0.41915085911750793
Loss at iteration 200 : 0.4810035824775696
Loss at iteration 250 : 0.8102216124534607
Loss at iteration 300 : 1.022270679473877
Loss at iteration 350 : 0.7348705530166626
Loss at iteration 400 : 0.6297169923782349
Mean training loss eporch  898 :  0.5980561101984551
Loss at iteration 50 : 0.648610532283783
Loss at iteration 100 : 0.5366652011871338
Loss at iteration 150 : 0.7406755089759827
Loss at iteration 200 : 0.411706805229187
Loss at iteration 250 : 0.46722885966300964
Loss at iteration 300 : 0.9554564356803894
Loss at iteration 350 : 0.6138699054718018
Loss at iteration 400 : 0.5721904039382935
Mean training loss eporch  899 :  0.5984762466007284
Loss at iteration 50 : 0.2737833261489868
Loss at iteration 100 : 0.9875125288963318
Loss at iteration 150 : 0.7382763624191284
Loss at iteration 200 : 0.3526701033115387
Loss at iteration 250 : 0.28585025668144226
Loss at iteration 300 : 0.2856922745704651
Loss at iteration 350 : 0.6196908354759216
Loss at iteration 400 : 0.6683494448661804
Mean training loss eporch  900 :  0.5973585158786966
Loss at iteration 50 : 1.0825462341308594
Loss at iteration 100 : 0.8284329175949097
Loss at iteration 150 : 0.6814956068992615
Loss at iteration 200 : 0.27514901757240295
Loss at iteration 250 : 0.8439269065856934
Loss at iteration 300 : 0.847868800163269
Loss at iteration 350 : 0.6079556941986084
Loss at iteration 400 : 0.35641101002693176
Mean training loss eporch  901 :  0.6026552857772651
Loss at iteration 50 : 0.4536774158477783
Loss at iteration 100 : 0.6768272519111633
Loss at iteration 150 : 0.3565077781677246
Loss at iteration 200 : 0.6090617179870605
Loss at iteration 250 : 0.38021141290664673
Loss at iteration 300 : 0.7262304425239563
Loss at iteration 350 : 0.3817114233970642
Loss at iteration 400 : 0.3682594895362854
Mean training loss eporch  902 :  0.5985659234259161
Loss at iteration 50 : 0.5046441555023193
Loss at iteration 100 : 1.0369441509246826
Loss at iteration 150 : 0.5036990642547607
Loss at iteration 200 : 0.455083429813385
Loss at iteration 250 : 0.5662888288497925
Loss at iteration 300 : 0.9660557508468628
Loss at iteration 350 : 0.6387500762939453
Loss at iteration 400 : 0.3205305337905884
Mean training loss eporch  903 :  0.5981208313540493
Loss at iteration 50 : 0.3839457035064697
Loss at iteration 100 : 0.3593769073486328
Loss at iteration 150 : 0.7832704186439514
Loss at iteration 200 : 0.8283246755599976
Loss at iteration 250 : 0.9881972670555115
Loss at iteration 300 : 0.5714039206504822
Loss at iteration 350 : 0.2657429873943329
Loss at iteration 400 : 0.3528679609298706
Mean training loss eporch  904 :  0.597874147274569
Loss at iteration 50 : 0.5241425037384033
Loss at iteration 100 : 0.7110757827758789
Loss at iteration 150 : 0.6453635692596436
Loss at iteration 200 : 0.23996132612228394
Loss at iteration 250 : 1.2062740325927734
Loss at iteration 300 : 0.32326239347457886
Loss at iteration 350 : 0.8038413524627686
Loss at iteration 400 : 0.7323802709579468
Mean training loss eporch  905 :  0.5974610822417277
Loss at iteration 50 : 0.45695585012435913
Loss at iteration 100 : 0.6415359377861023
Loss at iteration 150 : 0.5508765578269958
Loss at iteration 200 : 0.794096827507019
Loss at iteration 250 : 1.0495537519454956
Loss at iteration 300 : 0.7796385288238525
Loss at iteration 350 : 0.4203490912914276
Loss at iteration 400 : 0.7608456611633301
Mean training loss eporch  906 :  0.5974301400807406
Loss at iteration 50 : 0.9164729714393616
Loss at iteration 100 : 0.5324037075042725
Loss at iteration 150 : 0.8454788327217102
Loss at iteration 200 : 0.6364167928695679
Loss at iteration 250 : 0.8852953314781189
Loss at iteration 300 : 1.0956318378448486
Loss at iteration 350 : 0.45802319049835205
Loss at iteration 400 : 0.7163106203079224
Mean training loss eporch  907 :  0.5976962902888054
Loss at iteration 50 : 0.6128286719322205
Loss at iteration 100 : 0.36644166707992554
Loss at iteration 150 : 0.3725304901599884
Loss at iteration 200 : 0.32846248149871826
Loss at iteration 250 : 0.38025742769241333
Loss at iteration 300 : 0.7364788055419922
Loss at iteration 350 : 0.6707324981689453
Loss at iteration 400 : 0.47818315029144287
Mean training loss eporch  908 :  0.5972280650161574
Loss at iteration 50 : 0.7953649759292603
Loss at iteration 100 : 1.2225936651229858
Loss at iteration 150 : 0.44560056924819946
Loss at iteration 200 : 0.6378898620605469
Loss at iteration 250 : 0.8291862607002258
Loss at iteration 300 : 0.6325891017913818
Loss at iteration 350 : 0.558842658996582
Loss at iteration 400 : 0.6612107753753662
Mean training loss eporch  909 :  0.598190193553142
Loss at iteration 50 : 0.5841613411903381
Loss at iteration 100 : 0.41472017765045166
Loss at iteration 150 : 0.338009238243103
Loss at iteration 200 : 0.6286653876304626
Loss at iteration 250 : 0.6411702632904053
Loss at iteration 300 : 0.7454893589019775
Loss at iteration 350 : 0.457557737827301
Loss at iteration 400 : 0.6821774244308472
Mean training loss eporch  910 :  0.5977819193772671
Loss at iteration 50 : 0.5215603709220886
Loss at iteration 100 : 0.21783801913261414
Loss at iteration 150 : 0.5870888233184814
Loss at iteration 200 : 0.5005581974983215
Loss at iteration 250 : 0.6444380879402161
Loss at iteration 300 : 0.5326581001281738
Loss at iteration 350 : 0.31647735834121704
Loss at iteration 400 : 0.4253924787044525
Mean training loss eporch  911 :  0.598040240929533
Loss at iteration 50 : 0.5222989320755005
Loss at iteration 100 : 0.4911985993385315
Loss at iteration 150 : 0.750462532043457
Loss at iteration 200 : 0.34952449798583984
Loss at iteration 250 : 1.0367982387542725
Loss at iteration 300 : 0.5073069930076599
Loss at iteration 350 : 0.6286759972572327
Loss at iteration 400 : 0.4128732681274414
Mean training loss eporch  912 :  0.5976420943312046
Loss at iteration 50 : 0.22592216730117798
Loss at iteration 100 : 0.48407837748527527
Loss at iteration 150 : 0.9959418773651123
Loss at iteration 200 : 0.8917503356933594
Loss at iteration 250 : 0.7169424295425415
Loss at iteration 300 : 0.21936437487602234
Loss at iteration 350 : 0.7402057647705078
Loss at iteration 400 : 0.6612769365310669
Mean training loss eporch  913 :  0.5978663038846623
Loss at iteration 50 : 0.3990638852119446
Loss at iteration 100 : 0.4084926247596741
Loss at iteration 150 : 0.5596975684165955
Loss at iteration 200 : 0.2574790120124817
Loss at iteration 250 : 0.5773584842681885
Loss at iteration 300 : 0.6204230785369873
Loss at iteration 350 : 0.44783681631088257
Loss at iteration 400 : 0.3546364903450012
Mean training loss eporch  914 :  0.5975072330423534
Loss at iteration 50 : 0.23203636705875397
Loss at iteration 100 : 0.7656491994857788
Loss at iteration 150 : 0.3754369020462036
Loss at iteration 200 : 0.7015703320503235
Loss at iteration 250 : 0.7036953568458557
Loss at iteration 300 : 0.4724137783050537
Loss at iteration 350 : 0.41064780950546265
Loss at iteration 400 : 0.3230051100254059
Mean training loss eporch  915 :  0.5978066652946408
Loss at iteration 50 : 0.45023447275161743
Loss at iteration 100 : 0.860851526260376
Loss at iteration 150 : 0.3877761960029602
Loss at iteration 200 : 1.1253024339675903
Loss at iteration 250 : 0.36778193712234497
Loss at iteration 300 : 0.7616549730300903
Loss at iteration 350 : 0.41122645139694214
Loss at iteration 400 : 0.8792513608932495
Mean training loss eporch  916 :  0.6019447905266232
Loss at iteration 50 : 0.7573135495185852
Loss at iteration 100 : 0.3336477279663086
Loss at iteration 150 : 0.7905323505401611
Loss at iteration 200 : 0.47341272234916687
Loss at iteration 250 : 0.362448513507843
Loss at iteration 300 : 0.6255300641059875
Loss at iteration 350 : 0.6229526996612549
Loss at iteration 400 : 0.7787045240402222
Mean training loss eporch  917 :  0.5982187973343738
Loss at iteration 50 : 0.6026560664176941
Loss at iteration 100 : 0.28247374296188354
Loss at iteration 150 : 0.3994376063346863
Loss at iteration 200 : 1.108166217803955
Loss at iteration 250 : 0.28077811002731323
Loss at iteration 300 : 0.5619646310806274
Loss at iteration 350 : 0.7269753813743591
Loss at iteration 400 : 0.8948317170143127
Mean training loss eporch  918 :  0.5978460195620499
Loss at iteration 50 : 0.8950568437576294
Loss at iteration 100 : 0.31795603036880493
Loss at iteration 150 : 0.35837310552597046
Loss at iteration 200 : 0.7216882705688477
Loss at iteration 250 : 0.5693480968475342
Loss at iteration 300 : 0.832503080368042
Loss at iteration 350 : 0.478516161441803
Loss at iteration 400 : 0.5164318084716797
Mean training loss eporch  919 :  0.5973342348374593
Loss at iteration 50 : 0.8232001066207886
Loss at iteration 100 : 0.609271764755249
Loss at iteration 150 : 0.1961650550365448
Loss at iteration 200 : 0.543115496635437
Loss at iteration 250 : 1.0190443992614746
Loss at iteration 300 : 0.878581166267395
Loss at iteration 350 : 0.48078110814094543
Loss at iteration 400 : 1.099172830581665
Mean training loss eporch  920 :  0.6027729101980214
Loss at iteration 50 : 0.6021852493286133
Loss at iteration 100 : 0.6743132472038269
Loss at iteration 150 : 0.2966218888759613
Loss at iteration 200 : 0.511506199836731
Loss at iteration 250 : 0.36726462841033936
Loss at iteration 300 : 0.2661442756652832
Loss at iteration 350 : 1.1114846467971802
Loss at iteration 400 : 0.48559245467185974
Mean training loss eporch  921 :  0.5985222116045887
Loss at iteration 50 : 0.3365592658519745
Loss at iteration 100 : 0.5083413124084473
Loss at iteration 150 : 0.38455861806869507
Loss at iteration 200 : 0.4941560924053192
Loss at iteration 250 : 1.0208653211593628
Loss at iteration 300 : 0.329529345035553
Loss at iteration 350 : 0.5410158634185791
Loss at iteration 400 : 0.22480815649032593
Mean training loss eporch  922 :  0.5990600526800605
Loss at iteration 50 : 0.8201441764831543
Loss at iteration 100 : 0.4430340826511383
Loss at iteration 150 : 0.421164870262146
Loss at iteration 200 : 0.614133358001709
Loss at iteration 250 : 0.660281240940094
Loss at iteration 300 : 0.7308716773986816
Loss at iteration 350 : 0.37412890791893005
Loss at iteration 400 : 0.6949490308761597
Mean training loss eporch  923 :  0.5980138306259575
Loss at iteration 50 : 0.5989022254943848
Loss at iteration 100 : 0.6793309450149536
Loss at iteration 150 : 1.0092873573303223
Loss at iteration 200 : 0.2614573836326599
Loss at iteration 250 : 0.7449368238449097
Loss at iteration 300 : 0.9494513273239136
Loss at iteration 350 : 0.301105797290802
Loss at iteration 400 : 0.7911521196365356
Mean training loss eporch  924 :  0.5976066455712767
Loss at iteration 50 : 0.4164063334465027
Loss at iteration 100 : 0.6444671154022217
Loss at iteration 150 : 0.3337308168411255
Loss at iteration 200 : 0.38926777243614197
Loss at iteration 250 : 1.436665654182434
Loss at iteration 300 : 0.35283708572387695
Loss at iteration 350 : 0.7975175976753235
Loss at iteration 400 : 0.48332470655441284
Mean training loss eporch  925 :  0.5974836187111423
Loss at iteration 50 : 0.6589910387992859
Loss at iteration 100 : 0.39849406480789185
Loss at iteration 150 : 0.4125773310661316
Loss at iteration 200 : 0.28883710503578186
Loss at iteration 250 : 1.0466346740722656
Loss at iteration 300 : 0.6396514177322388
Loss at iteration 350 : 0.5205778479576111
Loss at iteration 400 : 0.25691908597946167
Mean training loss eporch  926 :  0.5984149765674309
Loss at iteration 50 : 0.6534806489944458
Loss at iteration 100 : 0.350454181432724
Loss at iteration 150 : 0.7678149342536926
Loss at iteration 200 : 0.3956313729286194
Loss at iteration 250 : 0.8561520576477051
Loss at iteration 300 : 0.6582258343696594
Loss at iteration 350 : 0.48243188858032227
Loss at iteration 400 : 0.2644672393798828
Mean training loss eporch  927 :  0.5977739882175164
Loss at iteration 50 : 0.44479820132255554
Loss at iteration 100 : 0.726589024066925
Loss at iteration 150 : 0.5674372911453247
Loss at iteration 200 : 0.7005051374435425
Loss at iteration 250 : 0.39822259545326233
Loss at iteration 300 : 0.41989198327064514
Loss at iteration 350 : 0.2730715572834015
Loss at iteration 400 : 0.7112048268318176
Mean training loss eporch  928 :  0.5976620306816336
Loss at iteration 50 : 0.4037784934043884
Loss at iteration 100 : 1.1380860805511475
Loss at iteration 150 : 0.26852381229400635
Loss at iteration 200 : 0.6541398763656616
Loss at iteration 250 : 0.5474404096603394
Loss at iteration 300 : 0.41654378175735474
Loss at iteration 350 : 0.31165075302124023
Loss at iteration 400 : 0.7821741104125977
Mean training loss eporch  929 :  0.5971697687167223
Loss at iteration 50 : 0.4259360432624817
Loss at iteration 100 : 0.2817799150943756
Loss at iteration 150 : 0.48699498176574707
Loss at iteration 200 : 0.24802365899085999
Loss at iteration 250 : 0.5867969989776611
Loss at iteration 300 : 0.5074520111083984
Loss at iteration 350 : 0.5244690775871277
Loss at iteration 400 : 1.1700282096862793
Mean training loss eporch  930 :  0.5992818352486521
Loss at iteration 50 : 0.7922582626342773
Loss at iteration 100 : 1.0118610858917236
Loss at iteration 150 : 0.34474658966064453
Loss at iteration 200 : 0.3655558228492737
Loss at iteration 250 : 0.6710340976715088
Loss at iteration 300 : 0.29130789637565613
Loss at iteration 350 : 0.6816533803939819
Loss at iteration 400 : 0.6671606302261353
Mean training loss eporch  931 :  0.5979227908790914
Loss at iteration 50 : 0.6821520328521729
Loss at iteration 100 : 0.6243676543235779
Loss at iteration 150 : 0.9463694095611572
Loss at iteration 200 : 0.40596258640289307
Loss at iteration 250 : 0.8735722899436951
Loss at iteration 300 : 0.28480541706085205
Loss at iteration 350 : 0.6351941823959351
Loss at iteration 400 : 0.28607994318008423
Mean training loss eporch  932 :  0.5987376656567035
Loss at iteration 50 : 0.5475733876228333
Loss at iteration 100 : 0.3454459309577942
Loss at iteration 150 : 0.6409002542495728
Loss at iteration 200 : 0.26601141691207886
Loss at iteration 250 : 0.4935339689254761
Loss at iteration 300 : 0.3592018783092499
Loss at iteration 350 : 0.5009310245513916
Loss at iteration 400 : 0.808659017086029
Mean training loss eporch  933 :  0.5989023561301253
Loss at iteration 50 : 0.290585458278656
Loss at iteration 100 : 0.26538559794425964
Loss at iteration 150 : 1.5155274868011475
Loss at iteration 200 : 0.6519330143928528
Loss at iteration 250 : 0.35250186920166016
Loss at iteration 300 : 0.30523037910461426
Loss at iteration 350 : 0.8537667989730835
Loss at iteration 400 : 0.6486252546310425
Mean training loss eporch  934 :  0.5971581239938202
Loss at iteration 50 : 0.2729784846305847
Loss at iteration 100 : 0.6119601726531982
Loss at iteration 150 : 0.8510792255401611
Loss at iteration 200 : 0.653031587600708
Loss at iteration 250 : 0.3318468928337097
Loss at iteration 300 : 0.6206461191177368
Loss at iteration 350 : 0.7484243512153625
Loss at iteration 400 : 0.6127191781997681
Mean training loss eporch  935 :  0.5973689222015073
Loss at iteration 50 : 0.5200932025909424
Loss at iteration 100 : 0.3049379885196686
Loss at iteration 150 : 0.5792229771614075
Loss at iteration 200 : 0.7337659001350403
Loss at iteration 250 : 0.30289357900619507
Loss at iteration 300 : 0.3277992010116577
Loss at iteration 350 : 0.3829960525035858
Loss at iteration 400 : 0.6959288120269775
Mean training loss eporch  936 :  0.5982753199445827
Loss at iteration 50 : 0.232138991355896
Loss at iteration 100 : 0.5725375413894653
Loss at iteration 150 : 0.26960307359695435
Loss at iteration 200 : 0.9982562065124512
Loss at iteration 250 : 0.5278765559196472
Loss at iteration 300 : 0.6791513562202454
Loss at iteration 350 : 0.5221230387687683
Loss at iteration 400 : 1.2881519794464111
Mean training loss eporch  937 :  0.5995326253664868
Loss at iteration 50 : 0.46797099709510803
Loss at iteration 100 : 0.31706953048706055
Loss at iteration 150 : 0.4738934338092804
Loss at iteration 200 : 0.7149061560630798
Loss at iteration 250 : 0.37016668915748596
Loss at iteration 300 : 0.8889920711517334
Loss at iteration 350 : 0.9741812348365784
Loss at iteration 400 : 0.9658266305923462
Mean training loss eporch  938 :  0.5976644186762416
Loss at iteration 50 : 0.6643384695053101
Loss at iteration 100 : 0.33425313234329224
Loss at iteration 150 : 0.326127290725708
Loss at iteration 200 : 0.6487462520599365
Loss at iteration 250 : 0.9615477323532104
Loss at iteration 300 : 0.9149240255355835
Loss at iteration 350 : 0.29057541489601135
Loss at iteration 400 : 0.45551759004592896
Mean training loss eporch  939 :  0.5972250032785762
Loss at iteration 50 : 0.519051194190979
Loss at iteration 100 : 0.40896016359329224
Loss at iteration 150 : 0.7733988165855408
Loss at iteration 200 : 0.8487915992736816
Loss at iteration 250 : 0.6636785268783569
Loss at iteration 300 : 0.2717040181159973
Loss at iteration 350 : 0.3340570330619812
Loss at iteration 400 : 0.7596524953842163
Mean training loss eporch  940 :  0.5969768228685909
Loss at iteration 50 : 0.5061575174331665
Loss at iteration 100 : 0.7245803475379944
Loss at iteration 150 : 0.4112520217895508
Loss at iteration 200 : 0.4338441491127014
Loss at iteration 250 : 0.23294898867607117
Loss at iteration 300 : 0.5738986134529114
Loss at iteration 350 : 0.8100509643554688
Loss at iteration 400 : 0.20471450686454773
Mean training loss eporch  941 :  0.603104279046636
Loss at iteration 50 : 0.8142281174659729
Loss at iteration 100 : 0.68934565782547
Loss at iteration 150 : 0.3492993414402008
Loss at iteration 200 : 0.8453272581100464
Loss at iteration 250 : 0.37695106863975525
Loss at iteration 300 : 0.7653997540473938
Loss at iteration 350 : 0.5127959251403809
Loss at iteration 400 : 0.6735015511512756
Mean training loss eporch  942 :  0.6018024360251534
Loss at iteration 50 : 0.4208596646785736
Loss at iteration 100 : 0.2986939549446106
Loss at iteration 150 : 0.5728352069854736
Loss at iteration 200 : 0.40212732553482056
Loss at iteration 250 : 0.43741267919540405
Loss at iteration 300 : 0.671891987323761
Loss at iteration 350 : 0.3645115792751312
Loss at iteration 400 : 0.7636197805404663
Mean training loss eporch  943 :  0.5969179988308337
Loss at iteration 50 : 0.7640566825866699
Loss at iteration 100 : 1.0380017757415771
Loss at iteration 150 : 0.4392198324203491
Loss at iteration 200 : 0.382924884557724
Loss at iteration 250 : 0.4487403631210327
Loss at iteration 300 : 0.36294353008270264
Loss at iteration 350 : 0.6648801565170288
Loss at iteration 400 : 0.4644460082054138
Mean training loss eporch  944 :  0.5973140931450198
Loss at iteration 50 : 0.9506885409355164
Loss at iteration 100 : 0.7250353693962097
Loss at iteration 150 : 0.826837420463562
Loss at iteration 200 : 0.6346359848976135
Loss at iteration 250 : 0.3814457356929779
Loss at iteration 300 : 0.46703290939331055
Loss at iteration 350 : 0.4758487343788147
Loss at iteration 400 : 0.29717934131622314
Mean training loss eporch  945 :  0.5973604207215287
Loss at iteration 50 : 0.6430671215057373
Loss at iteration 100 : 0.3955532908439636
Loss at iteration 150 : 0.4739941954612732
Loss at iteration 200 : 0.6687486171722412
Loss at iteration 250 : 0.47051119804382324
Loss at iteration 300 : 0.6405535936355591
Loss at iteration 350 : 0.647880494594574
Loss at iteration 400 : 0.6990869045257568
Mean training loss eporch  946 :  0.5974938877494881
Loss at iteration 50 : 0.2181379795074463
Loss at iteration 100 : 0.5683932304382324
Loss at iteration 150 : 0.33700335025787354
Loss at iteration 200 : 0.3654831647872925
Loss at iteration 250 : 0.4277153015136719
Loss at iteration 300 : 0.6683692932128906
Loss at iteration 350 : 1.0758225917816162
Loss at iteration 400 : 0.4413827359676361
Mean training loss eporch  947 :  0.5966982136108816
Loss at iteration 50 : 0.43578290939331055
Loss at iteration 100 : 0.6345092058181763
Loss at iteration 150 : 0.43372678756713867
Loss at iteration 200 : 0.5860534906387329
Loss at iteration 250 : 1.3362641334533691
Loss at iteration 300 : 0.6000101566314697
Loss at iteration 350 : 0.5416019558906555
Loss at iteration 400 : 0.3998347222805023
Mean training loss eporch  948 :  0.6014284429729252
Loss at iteration 50 : 0.34412673115730286
Loss at iteration 100 : 0.5460463166236877
Loss at iteration 150 : 0.3475845158100128
Loss at iteration 200 : 0.342153936624527
Loss at iteration 250 : 0.5901411771774292
Loss at iteration 300 : 0.6878761053085327
Loss at iteration 350 : 0.926660418510437
Loss at iteration 400 : 0.3872619867324829
Mean training loss eporch  949 :  0.5972684708010455
Loss at iteration 50 : 0.6808115839958191
Loss at iteration 100 : 0.4441949725151062
Loss at iteration 150 : 0.9069841504096985
Loss at iteration 200 : 0.6107683181762695
Loss at iteration 250 : 0.6589106321334839
Loss at iteration 300 : 0.5912527441978455
Loss at iteration 350 : 1.1094900369644165
Loss at iteration 400 : 0.28348785638809204
Mean training loss eporch  950 :  0.5977297280267749
Loss at iteration 50 : 0.6573126316070557
Loss at iteration 100 : 0.5047683715820312
Loss at iteration 150 : 0.4182387590408325
Loss at iteration 200 : 0.8115122318267822
Loss at iteration 250 : 0.48991653323173523
Loss at iteration 300 : 0.36417263746261597
Loss at iteration 350 : 0.5760821104049683
Loss at iteration 400 : 0.29867276549339294
Mean training loss eporch  951 :  0.5972061102098948
Loss at iteration 50 : 0.3070419728755951
Loss at iteration 100 : 0.8605238199234009
Loss at iteration 150 : 0.5971199870109558
Loss at iteration 200 : 0.9874581098556519
Loss at iteration 250 : 0.29078105092048645
Loss at iteration 300 : 0.522850513458252
Loss at iteration 350 : 0.6655386686325073
Loss at iteration 400 : 0.7913900017738342
Mean training loss eporch  952 :  0.5972927603954157
Loss at iteration 50 : 0.3156328499317169
Loss at iteration 100 : 0.5210447311401367
Loss at iteration 150 : 0.42128443717956543
Loss at iteration 200 : 1.1221802234649658
Loss at iteration 250 : 0.47061049938201904
Loss at iteration 300 : 0.2982305884361267
Loss at iteration 350 : 0.2627418637275696
Loss at iteration 400 : 0.6602925062179565
Mean training loss eporch  953 :  0.5966252695284617
Loss at iteration 50 : 0.5110208988189697
Loss at iteration 100 : 0.43431511521339417
Loss at iteration 150 : 0.5255639553070068
Loss at iteration 200 : 0.8605583906173706
Loss at iteration 250 : 0.9433926343917847
Loss at iteration 300 : 0.6395020484924316
Loss at iteration 350 : 1.3357200622558594
Loss at iteration 400 : 0.3358616828918457
Mean training loss eporch  954 :  0.603091307351942
Loss at iteration 50 : 0.8105376958847046
Loss at iteration 100 : 0.43030864000320435
Loss at iteration 150 : 0.23734687268733978
Loss at iteration 200 : 0.658287763595581
Loss at iteration 250 : 0.5213577747344971
Loss at iteration 300 : 0.27290940284729004
Loss at iteration 350 : 0.28888314962387085
Loss at iteration 400 : 0.9619750380516052
Mean training loss eporch  955 :  0.5972230719076679
Loss at iteration 50 : 0.331508994102478
Loss at iteration 100 : 0.8012637495994568
Loss at iteration 150 : 0.592940628528595
Loss at iteration 200 : 0.7435598969459534
Loss at iteration 250 : 1.0436370372772217
Loss at iteration 300 : 0.4453159272670746
Loss at iteration 350 : 0.9034133553504944
Loss at iteration 400 : 0.6878995895385742
Mean training loss eporch  956 :  0.5970207902147631
Loss at iteration 50 : 0.9485757946968079
Loss at iteration 100 : 0.7340967655181885
Loss at iteration 150 : 0.8563768863677979
Loss at iteration 200 : 0.7756319046020508
Loss at iteration 250 : 0.29633012413978577
Loss at iteration 300 : 0.5157136917114258
Loss at iteration 350 : 0.8868905901908875
Loss at iteration 400 : 0.9741464853286743
Mean training loss eporch  957 :  0.5970852376366944
Loss at iteration 50 : 0.5433213114738464
Loss at iteration 100 : 0.4109065532684326
Loss at iteration 150 : 0.3338874578475952
Loss at iteration 200 : 0.2698782980442047
Loss at iteration 250 : 0.4703480303287506
Loss at iteration 300 : 0.6506727933883667
Loss at iteration 350 : 0.20876839756965637
Loss at iteration 400 : 0.5007979869842529
Mean training loss eporch  958 :  0.5973900467023722
Loss at iteration 50 : 0.8376168012619019
Loss at iteration 100 : 0.40759360790252686
Loss at iteration 150 : 0.6931531429290771
Loss at iteration 200 : 0.74134361743927
Loss at iteration 250 : 0.3261803984642029
Loss at iteration 300 : 0.5916429758071899
Loss at iteration 350 : 0.3908642530441284
Loss at iteration 400 : 0.5568063259124756
Mean training loss eporch  959 :  0.6032048320823721
Loss at iteration 50 : 0.7522144317626953
Loss at iteration 100 : 0.31926047801971436
Loss at iteration 150 : 0.4341539740562439
Loss at iteration 200 : 0.39048489928245544
Loss at iteration 250 : 0.9574751257896423
Loss at iteration 300 : 0.24827630817890167
Loss at iteration 350 : 0.49809563159942627
Loss at iteration 400 : 0.26097413897514343
Mean training loss eporch  960 :  0.6027125457944892
Loss at iteration 50 : 0.2517147362232208
Loss at iteration 100 : 0.3303872346878052
Loss at iteration 150 : 1.2679417133331299
Loss at iteration 200 : 0.3748728036880493
Loss at iteration 250 : 1.177921175956726
Loss at iteration 300 : 0.8675379157066345
Loss at iteration 350 : 0.4559595584869385
Loss at iteration 400 : 0.989478588104248
Mean training loss eporch  961 :  0.5975016291031923
Loss at iteration 50 : 0.5088049173355103
Loss at iteration 100 : 1.0456966161727905
Loss at iteration 150 : 0.4093995690345764
Loss at iteration 200 : 1.0953465700149536
Loss at iteration 250 : 0.7721078395843506
Loss at iteration 300 : 0.18914823234081268
Loss at iteration 350 : 0.38071316480636597
Loss at iteration 400 : 0.6451939344406128
Mean training loss eporch  962 :  0.5979987819232213
Loss at iteration 50 : 0.6488888263702393
Loss at iteration 100 : 0.7351391315460205
Loss at iteration 150 : 0.48848485946655273
Loss at iteration 200 : 0.7004209160804749
Loss at iteration 250 : 0.8417854309082031
Loss at iteration 300 : 0.34201765060424805
Loss at iteration 350 : 0.6707776784896851
Loss at iteration 400 : 0.44436144828796387
Mean training loss eporch  963 :  0.5972068895138967
Loss at iteration 50 : 0.3113473057746887
Loss at iteration 100 : 0.3366301655769348
Loss at iteration 150 : 0.3841018080711365
Loss at iteration 200 : 0.379503071308136
Loss at iteration 250 : 0.44028592109680176
Loss at iteration 300 : 0.6988552808761597
Loss at iteration 350 : 0.7565528154373169
Loss at iteration 400 : 0.408918559551239
Mean training loss eporch  964 :  0.5971309902488918
Loss at iteration 50 : 0.7401560544967651
Loss at iteration 100 : 0.6833004355430603
Loss at iteration 150 : 0.7000851035118103
Loss at iteration 200 : 0.688992977142334
Loss at iteration 250 : 0.5033761262893677
Loss at iteration 300 : 0.25241410732269287
Loss at iteration 350 : 0.377110093832016
Loss at iteration 400 : 0.2883637249469757
Mean training loss eporch  965 :  0.5971778519618671
Loss at iteration 50 : 0.24209144711494446
Loss at iteration 100 : 0.7196136116981506
Loss at iteration 150 : 0.26234763860702515
Loss at iteration 200 : 0.5015463829040527
Loss at iteration 250 : 0.514947772026062
Loss at iteration 300 : 0.7472597360610962
Loss at iteration 350 : 0.5416417121887207
Loss at iteration 400 : 0.9077296853065491
Mean training loss eporch  966 :  0.5973941670404956
Loss at iteration 50 : 0.45124301314353943
Loss at iteration 100 : 0.43625515699386597
Loss at iteration 150 : 0.4294470548629761
Loss at iteration 200 : 0.34579768776893616
Loss at iteration 250 : 0.5752058029174805
Loss at iteration 300 : 0.5790427327156067
Loss at iteration 350 : 0.7426179051399231
Loss at iteration 400 : 0.2517179250717163
Mean training loss eporch  967 :  0.5971218839020472
Loss at iteration 50 : 0.44247108697891235
Loss at iteration 100 : 0.6137809157371521
Loss at iteration 150 : 0.9232783913612366
Loss at iteration 200 : 0.802554726600647
Loss at iteration 250 : 0.49768152832984924
Loss at iteration 300 : 0.489797979593277
Loss at iteration 350 : 0.4126972258090973
Loss at iteration 400 : 0.7687067985534668
Mean training loss eporch  968 :  0.5970697728708186
Loss at iteration 50 : 0.7797859311103821
Loss at iteration 100 : 1.1412818431854248
Loss at iteration 150 : 0.5576531291007996
Loss at iteration 200 : 0.3358418643474579
Loss at iteration 250 : 0.511207103729248
Loss at iteration 300 : 0.6959127187728882
Loss at iteration 350 : 0.29248470067977905
Loss at iteration 400 : 0.3231375217437744
Mean training loss eporch  969 :  0.5982150159475514
Loss at iteration 50 : 0.8083182573318481
Loss at iteration 100 : 0.33379197120666504
Loss at iteration 150 : 1.0844467878341675
Loss at iteration 200 : 0.37975332140922546
Loss at iteration 250 : 0.7328523397445679
Loss at iteration 300 : 0.4275999665260315
Loss at iteration 350 : 0.40077871084213257
Loss at iteration 400 : 0.9242185354232788
Mean training loss eporch  970 :  0.5976327824752962
Loss at iteration 50 : 0.4341397285461426
Loss at iteration 100 : 0.5513731837272644
Loss at iteration 150 : 0.831261157989502
Loss at iteration 200 : 0.4293319880962372
Loss at iteration 250 : 0.2311021387577057
Loss at iteration 300 : 1.2944196462631226
Loss at iteration 350 : 0.972277045249939
Loss at iteration 400 : 0.4895181655883789
Mean training loss eporch  971 :  0.5969176296428714
Loss at iteration 50 : 0.6293649077415466
Loss at iteration 100 : 0.5766869783401489
Loss at iteration 150 : 0.4809712767601013
Loss at iteration 200 : 0.42251187562942505
Loss at iteration 250 : 0.5615321397781372
Loss at iteration 300 : 0.2924344539642334
Loss at iteration 350 : 0.8273777961730957
Loss at iteration 400 : 0.6471551656723022
Mean training loss eporch  972 :  0.5972958566295192
Loss at iteration 50 : 0.9643812775611877
Loss at iteration 100 : 0.5721828937530518
Loss at iteration 150 : 0.25790759921073914
Loss at iteration 200 : 0.787195086479187
Loss at iteration 250 : 0.82117760181427
Loss at iteration 300 : 0.2219315767288208
Loss at iteration 350 : 0.5044861435890198
Loss at iteration 400 : 0.5381192564964294
Mean training loss eporch  973 :  0.5983461794096793
Loss at iteration 50 : 0.5756475329399109
Loss at iteration 100 : 0.4606952965259552
Loss at iteration 150 : 0.7776251435279846
Loss at iteration 200 : 0.8380519151687622
Loss at iteration 250 : 1.0712308883666992
Loss at iteration 300 : 0.4485740661621094
Loss at iteration 350 : 0.6027415990829468
Loss at iteration 400 : 0.6697597503662109
Mean training loss eporch  974 :  0.5968016788630742
Loss at iteration 50 : 0.3347942531108856
Loss at iteration 100 : 0.36278969049453735
Loss at iteration 150 : 0.4746466875076294
Loss at iteration 200 : 0.6152873039245605
Loss at iteration 250 : 0.8888028860092163
Loss at iteration 300 : 0.6562772989273071
Loss at iteration 350 : 0.3291718363761902
Loss at iteration 400 : 0.41824185848236084
Mean training loss eporch  975 :  0.5979212759388401
Loss at iteration 50 : 0.44988059997558594
Loss at iteration 100 : 0.5123202800750732
Loss at iteration 150 : 0.4459298253059387
Loss at iteration 200 : 0.7840641736984253
Loss at iteration 250 : 0.42308947443962097
Loss at iteration 300 : 1.0133531093597412
Loss at iteration 350 : 0.7260856628417969
Loss at iteration 400 : 0.5708200931549072
Mean training loss eporch  976 :  0.5972874553815666
Loss at iteration 50 : 0.5001709461212158
Loss at iteration 100 : 0.663145899772644
Loss at iteration 150 : 0.6584763526916504
Loss at iteration 200 : 0.7166301012039185
Loss at iteration 250 : 0.6323838829994202
Loss at iteration 300 : 0.7074192762374878
Loss at iteration 350 : 0.33751094341278076
Loss at iteration 400 : 0.7611875534057617
Mean training loss eporch  977 :  0.5974355204022519
Loss at iteration 50 : 0.2665466368198395
Loss at iteration 100 : 0.543290376663208
Loss at iteration 150 : 0.5429553985595703
Loss at iteration 200 : 0.355688214302063
Loss at iteration 250 : 0.4550847113132477
Loss at iteration 300 : 0.314822793006897
Loss at iteration 350 : 0.7601338624954224
Loss at iteration 400 : 0.7816786766052246
Mean training loss eporch  978 :  0.5978357357227749
Loss at iteration 50 : 0.4341992735862732
Loss at iteration 100 : 0.645275354385376
Loss at iteration 150 : 0.38958460092544556
Loss at iteration 200 : 0.750485897064209
Loss at iteration 250 : 0.7477115392684937
Loss at iteration 300 : 0.2633701264858246
Loss at iteration 350 : 0.9880218505859375
Loss at iteration 400 : 0.3330806791782379
Mean training loss eporch  979 :  0.5970393898374832
Loss at iteration 50 : 1.1806004047393799
Loss at iteration 100 : 0.9545665383338928
Loss at iteration 150 : 1.261985421180725
Loss at iteration 200 : 0.7814429998397827
Loss at iteration 250 : 0.45916491746902466
Loss at iteration 300 : 0.7527170181274414
Loss at iteration 350 : 0.7329741716384888
Loss at iteration 400 : 0.6028108596801758
Mean training loss eporch  980 :  0.5974472313117019
Loss at iteration 50 : 0.6125493049621582
Loss at iteration 100 : 0.5025943517684937
Loss at iteration 150 : 0.5012583136558533
Loss at iteration 200 : 0.9295421242713928
Loss at iteration 250 : 0.759872555732727
Loss at iteration 300 : 0.7455825805664062
Loss at iteration 350 : 0.5720840692520142
Loss at iteration 400 : 0.28048378229141235
Mean training loss eporch  981 :  0.5978384342361993
Loss at iteration 50 : 0.8922790884971619
Loss at iteration 100 : 0.5534801483154297
Loss at iteration 150 : 0.5684543251991272
Loss at iteration 200 : 0.7811685800552368
Loss at iteration 250 : 0.4738251566886902
Loss at iteration 300 : 1.0426679849624634
Loss at iteration 350 : 0.5709802508354187
Loss at iteration 400 : 0.5180381536483765
Mean training loss eporch  982 :  0.5976464374183005
Loss at iteration 50 : 0.6632167100906372
Loss at iteration 100 : 0.7497138977050781
Loss at iteration 150 : 0.6071606278419495
Loss at iteration 200 : 0.7510784864425659
Loss at iteration 250 : 0.5110962986946106
Loss at iteration 300 : 0.674674391746521
Loss at iteration 350 : 0.4397629499435425
Loss at iteration 400 : 0.6480352878570557
Mean training loss eporch  983 :  0.5971137240796346
Loss at iteration 50 : 1.1541136503219604
Loss at iteration 100 : 0.6335750818252563
Loss at iteration 150 : 0.5049351453781128
Loss at iteration 200 : 0.32532793283462524
Loss at iteration 250 : 0.5926433801651001
Loss at iteration 300 : 1.0420832633972168
Loss at iteration 350 : 0.7933944463729858
Loss at iteration 400 : 0.40113168954849243
Mean training loss eporch  984 :  0.5972756080563293
Loss at iteration 50 : 1.0440661907196045
Loss at iteration 100 : 0.6333717703819275
Loss at iteration 150 : 0.5055800676345825
Loss at iteration 200 : 0.47895991802215576
Loss at iteration 250 : 1.2496371269226074
Loss at iteration 300 : 0.36328819394111633
Loss at iteration 350 : 0.43867045640945435
Loss at iteration 400 : 0.4046308994293213
Mean training loss eporch  985 :  0.597722533371951
Loss at iteration 50 : 0.41601282358169556
Loss at iteration 100 : 0.8064247965812683
Loss at iteration 150 : 0.5211054682731628
Loss at iteration 200 : 0.9199807643890381
Loss at iteration 250 : 0.7602711319923401
Loss at iteration 300 : 0.8771079778671265
Loss at iteration 350 : 0.7824474573135376
Loss at iteration 400 : 0.7218184471130371
Mean training loss eporch  986 :  0.5972722441024845
Loss at iteration 50 : 0.7057908177375793
Loss at iteration 100 : 0.4071946144104004
Loss at iteration 150 : 0.7204059362411499
Loss at iteration 200 : 0.8184112310409546
Loss at iteration 250 : 0.3079944849014282
Loss at iteration 300 : 1.0133061408996582
Loss at iteration 350 : 1.3391821384429932
Loss at iteration 400 : 0.44800373911857605
Mean training loss eporch  987 :  0.5973726802676782
Loss at iteration 50 : 0.9296910762786865
Loss at iteration 100 : 1.0224205255508423
Loss at iteration 150 : 0.5879942178726196
Loss at iteration 200 : 0.3200056552886963
Loss at iteration 250 : 0.6982995271682739
Loss at iteration 300 : 0.6398525834083557
Loss at iteration 350 : 0.30304837226867676
Loss at iteration 400 : 0.4032474160194397
Mean training loss eporch  988 :  0.5997932898757704
Loss at iteration 50 : 0.32498878240585327
Loss at iteration 100 : 0.4071398079395294
Loss at iteration 150 : 0.283582866191864
Loss at iteration 200 : 0.5458354949951172
Loss at iteration 250 : 0.5508986115455627
Loss at iteration 300 : 0.8944720029830933
Loss at iteration 350 : 0.6900887489318848
Loss at iteration 400 : 0.7313929200172424
Mean training loss eporch  989 :  0.5971532147628309
Loss at iteration 50 : 0.39425867795944214
Loss at iteration 100 : 0.48546260595321655
Loss at iteration 150 : 1.1069328784942627
Loss at iteration 200 : 0.7220488786697388
Loss at iteration 250 : 0.35447078943252563
Loss at iteration 300 : 0.38961899280548096
Loss at iteration 350 : 0.47982853651046753
Loss at iteration 400 : 0.6147950887680054
Mean training loss eporch  990 :  0.5968883399738859
Loss at iteration 50 : 0.5564623475074768
Loss at iteration 100 : 0.7764879465103149
Loss at iteration 150 : 1.4893882274627686
Loss at iteration 200 : 1.0728371143341064
Loss at iteration 250 : 1.0131436586380005
Loss at iteration 300 : 0.5506496429443359
Loss at iteration 350 : 0.854152500629425
Loss at iteration 400 : 0.6024401783943176
Mean training loss eporch  991 :  0.5979779274474345
Loss at iteration 50 : 0.4042772054672241
Loss at iteration 100 : 0.8948473930358887
Loss at iteration 150 : 0.26760977506637573
Loss at iteration 200 : 0.569640576839447
Loss at iteration 250 : 0.4930376708507538
Loss at iteration 300 : 0.3868175148963928
Loss at iteration 350 : 1.0087387561798096
Loss at iteration 400 : 1.1820318698883057
Mean training loss eporch  992 :  0.5977230119130537
Loss at iteration 50 : 0.9225900769233704
Loss at iteration 100 : 0.6684750318527222
Loss at iteration 150 : 0.4095640778541565
Loss at iteration 200 : 1.0356435775756836
Loss at iteration 250 : 0.7044302225112915
Loss at iteration 300 : 0.7113792896270752
Loss at iteration 350 : 0.8785399198532104
Loss at iteration 400 : 0.2424110770225525
Mean training loss eporch  993 :  0.6022887890448485
Loss at iteration 50 : 0.4789993166923523
Loss at iteration 100 : 0.41031479835510254
Loss at iteration 150 : 0.552578330039978
Loss at iteration 200 : 0.6448673009872437
Loss at iteration 250 : 0.747084379196167
Loss at iteration 300 : 0.363491415977478
Loss at iteration 350 : 0.2411973625421524
Loss at iteration 400 : 0.6744580268859863
Mean training loss eporch  994 :  0.5972888846234355
Loss at iteration 50 : 0.33381620049476624
Loss at iteration 100 : 0.3395848274230957
Loss at iteration 150 : 0.5755758285522461
Loss at iteration 200 : 0.5981247425079346
Loss at iteration 250 : 0.7591744661331177
Loss at iteration 300 : 0.46713754534721375
Loss at iteration 350 : 0.31844562292099
Loss at iteration 400 : 0.9555974006652832
Mean training loss eporch  995 :  0.6004457781467203
Loss at iteration 50 : 0.5504694581031799
Loss at iteration 100 : 0.9842791557312012
Loss at iteration 150 : 0.5140777826309204
Loss at iteration 200 : 1.512483835220337
Loss at iteration 250 : 0.6098603010177612
Loss at iteration 300 : 0.8774664402008057
Loss at iteration 350 : 0.6612720489501953
Loss at iteration 400 : 0.3202213644981384
Mean training loss eporch  996 :  0.5975482399688173
Loss at iteration 50 : 0.5691260695457458
Loss at iteration 100 : 0.5317673683166504
Loss at iteration 150 : 0.28645867109298706
Loss at iteration 200 : 0.6547962427139282
Loss at iteration 250 : 0.6899726390838623
Loss at iteration 300 : 0.6977047920227051
Loss at iteration 350 : 0.709692120552063
Loss at iteration 400 : 0.46739596128463745
Mean training loss eporch  997 :  0.5981788941629799
Loss at iteration 50 : 0.37529629468917847
Loss at iteration 100 : 0.368121474981308
Loss at iteration 150 : 0.6989319324493408
Loss at iteration 200 : 0.4085673987865448
Loss at iteration 250 : 0.3259168267250061
Loss at iteration 300 : 0.8220354914665222
Loss at iteration 350 : 0.5179585814476013
Loss at iteration 400 : 0.5105899572372437
Mean training loss eporch  998 :  0.6029274691848477
Loss at iteration 50 : 0.7369441986083984
Loss at iteration 100 : 0.42362332344055176
Loss at iteration 150 : 0.8257182836532593
Loss at iteration 200 : 0.52226722240448
Loss at iteration 250 : 0.2600261867046356
Loss at iteration 300 : 0.39901959896087646
Loss at iteration 350 : 0.605810284614563
Loss at iteration 400 : 0.2588033676147461
Mean training loss eporch  999 :  0.5978983239074459
Min training loss at epoch  954 :  0.5966252695284617
